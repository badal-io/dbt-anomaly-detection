

============================== 2023-02-06 17:48:46.371561 | f2a56d6a-a070-4799-9deb-9358f32a1300 ==============================
[0m17:48:46.371566 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:48:46.371751 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m17:48:46.371815 [debug] [MainThread]: Tracking: tracking
[0m17:48:46.380873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107faf490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107faf430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107faf520>]}
[0m17:48:46.600298 [debug] [MainThread]: Executing "git --help"
[0m17:48:46.606685 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m17:48:46.607184 [debug] [MainThread]: STDERR: "b''"
[0m17:48:46.607645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1093272b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109322c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109322f70>]}
[0m17:48:46.607983 [debug] [MainThread]: Flushing usage events


============================== 2023-02-06 17:56:38.068174 | 17d79ec4-d322-4301-8938-d95e4df7002f ==============================
[0m17:56:38.068180 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:56:38.068486 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m17:56:38.068568 [debug] [MainThread]: Tracking: tracking
[0m17:56:38.077087 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1106f53a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1106f0280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1106f00d0>]}
[0m17:56:38.289058 [debug] [MainThread]: Executing "git --help"
[0m17:56:38.295430 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m17:56:38.295843 [debug] [MainThread]: STDERR: "b''"
[0m17:56:38.310173 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m17:56:38.310682 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:56:39.855644 [debug] [MainThread]: On debug: select 1 as id
[0m17:56:40.639699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112330160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112330220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112330f40>]}
[0m17:56:40.641416 [debug] [MainThread]: Flushing usage events
[0m17:56:41.110169 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-06 17:57:35.445325 | ae2139ac-69a0-4761-a6ec-c62cf7dcc0ba ==============================
[0m17:57:35.445330 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:57:35.445527 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m17:57:35.445588 [debug] [MainThread]: Tracking: tracking
[0m17:57:35.453811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1084be250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1084b8160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1084b82b0>]}
[0m17:57:35.605171 [debug] [MainThread]: Executing "git --help"
[0m17:57:35.610169 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m17:57:35.610589 [debug] [MainThread]: STDERR: "b''"
[0m17:57:35.613398 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m17:57:35.613955 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:57:35.892059 [debug] [MainThread]: On debug: select 1 as id
[0m17:57:36.783903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1098123a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109812f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109812b80>]}
[0m17:57:36.784681 [debug] [MainThread]: Flushing usage events
[0m17:57:37.132406 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-06 17:57:55.379437 | 36ad5698-5f1a-4efa-bf3c-972274495c34 ==============================
[0m17:57:55.379443 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:57:55.379632 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m17:57:55.379693 [debug] [MainThread]: Tracking: tracking
[0m17:57:55.386971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043e3550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043dc1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043dc280>]}
[0m17:57:55.540016 [debug] [MainThread]: Executing "git --help"
[0m17:57:55.544788 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m17:57:55.545284 [debug] [MainThread]: STDERR: "b''"
[0m17:57:55.548128 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m17:57:55.548679 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:57:55.827664 [debug] [MainThread]: On debug: select 1 as id
[0m17:57:56.555254 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105739280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105739ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105739b80>]}
[0m17:57:56.555818 [debug] [MainThread]: Flushing usage events
[0m17:57:56.745128 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-06 18:02:23.324116 | 6b203dc9-8045-450f-bf52-0e9dee09ce1e ==============================
[0m18:02:23.324122 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:02:23.324433 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m18:02:23.324500 [debug] [MainThread]: Tracking: tracking
[0m18:02:23.333565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051300a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105130460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105130fd0>]}
[0m18:02:23.558337 [debug] [MainThread]: Executing "git --help"
[0m18:02:23.571043 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m18:02:23.571438 [debug] [MainThread]: STDERR: "b''"
[0m18:02:23.575107 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m18:02:23.575604 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:02:23.977407 [debug] [MainThread]: On debug: select 1 as id
[0m18:02:24.839592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106584f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106584400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106584250>]}
[0m18:02:24.841319 [debug] [MainThread]: Flushing usage events
[0m18:02:25.305497 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-06 18:07:05.541213 | 06a9d69a-015a-425b-bd8f-68a12264bd76 ==============================
[0m18:07:05.541219 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:07:05.541531 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m18:07:05.541598 [debug] [MainThread]: Tracking: tracking
[0m18:07:05.551316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10686c040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10686c3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10686cfa0>]}
[0m18:07:05.764016 [debug] [MainThread]: Executing "git --help"
[0m18:07:05.777584 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m18:07:05.777987 [debug] [MainThread]: STDERR: "b''"
[0m18:07:05.781685 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m18:07:05.782291 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:07:06.240433 [debug] [MainThread]: On debug: select 1 as id
[0m18:07:07.044397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bf0bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bf0880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bf0e80>]}
[0m18:07:07.045307 [debug] [MainThread]: Flushing usage events
[0m18:07:07.513970 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-06 18:15:35.956719 | da918c34-f0be-40d8-abd4-66bc25d74494 ==============================
[0m18:15:35.956724 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:15:35.957087 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m18:15:35.957165 [debug] [MainThread]: Tracking: tracking
[0m18:15:35.964465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104391f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043911c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104391fd0>]}
[0m18:15:36.185826 [debug] [MainThread]: Executing "git --help"
[0m18:15:36.199517 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m18:15:36.199988 [debug] [MainThread]: STDERR: "b''"
[0m18:15:36.203806 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m18:15:36.204388 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:15:36.519941 [debug] [MainThread]: On debug: select 1 as id
[0m18:15:37.252749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056ede20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056ed9a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056eda30>]}
[0m18:15:37.253865 [debug] [MainThread]: Flushing usage events
[0m18:15:37.709386 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-06 18:17:06.900655 | 7d735a53-38fc-4acc-a01d-a018f503ea92 ==============================
[0m18:17:06.900665 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:17:06.900853 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m18:17:06.900917 [debug] [MainThread]: Tracking: tracking
[0m18:17:06.908508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c07700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c07280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c073d0>]}
[0m18:17:07.064903 [debug] [MainThread]: Executing "git --help"
[0m18:17:07.069731 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m18:17:07.070131 [debug] [MainThread]: STDERR: "b''"
[0m18:17:07.073147 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m18:17:07.073714 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:17:07.379816 [debug] [MainThread]: On debug: select 1 as id
[0m18:17:08.156902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f69c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f69a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f69640>]}
[0m18:17:08.158585 [debug] [MainThread]: Flushing usage events
[0m18:17:08.679399 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-06 18:18:07.298456 | b9e8aa8b-dc7b-4e0e-97d5-8d71896584c8 ==============================
[0m18:18:07.298462 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:18:07.298758 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m18:18:07.298824 [debug] [MainThread]: Tracking: tracking
[0m18:18:07.310059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a77250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a70160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a702b0>]}
[0m18:18:07.523054 [debug] [MainThread]: Executing "git --help"
[0m18:18:07.535804 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m18:18:07.536280 [debug] [MainThread]: STDERR: "b''"
[0m18:18:07.540003 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m18:18:07.540615 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:18:07.951672 [debug] [MainThread]: On debug: select 1 as id
[0m18:18:08.754478 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106787c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a4b880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067b2160>]}
[0m18:18:08.755410 [debug] [MainThread]: Flushing usage events
[0m18:18:09.110942 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-06 18:26:10.646898 | 1a6e8ee6-a48f-4662-ba62-afec2ed5e883 ==============================
[0m18:26:10.646904 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:26:10.647234 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m18:26:10.647301 [debug] [MainThread]: Tracking: tracking
[0m18:26:10.657384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104214040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1042143d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104214fa0>]}
[0m18:26:10.876147 [debug] [MainThread]: Executing "git --help"
[0m18:26:10.881032 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m18:26:10.881536 [debug] [MainThread]: STDERR: "b''"
[0m18:26:10.885727 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m18:26:10.886512 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:26:11.299949 [debug] [MainThread]: On debug: select 1 as id
[0m18:26:12.152940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055a8c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055a8730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055a8310>]}
[0m18:26:12.153851 [debug] [MainThread]: Flushing usage events
[0m18:26:12.644207 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-06 18:27:12.606176 | 7d26255d-e638-4b13-a7c7-1305c693e421 ==============================
[0m18:27:12.606182 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:27:12.606354 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m18:27:12.606419 [debug] [MainThread]: Tracking: tracking
[0m18:27:12.614376 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104407700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104407280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1044073d0>]}
[0m18:27:12.840296 [debug] [MainThread]: Executing "git --help"
[0m18:27:12.852951 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m18:27:12.853497 [debug] [MainThread]: STDERR: "b''"
[0m18:27:12.857361 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m18:27:12.857863 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:27:13.301883 [debug] [MainThread]: On debug: select 1 as id
[0m18:27:14.046837 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043eb040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043eb2e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043eb790>]}
[0m18:27:14.047555 [debug] [MainThread]: Flushing usage events
[0m18:27:14.426781 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-06 19:09:33.493545 | dd71b91b-c096-4f83-b159-52b0eff49728 ==============================
[0m19:09:33.493552 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:09:33.493904 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m19:09:33.493971 [debug] [MainThread]: Tracking: tracking
[0m19:09:33.503301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a46700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a46280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a463d0>]}
[0m19:09:33.727605 [debug] [MainThread]: Executing "git --help"
[0m19:09:33.739702 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m19:09:33.740153 [debug] [MainThread]: STDERR: "b''"
[0m19:09:33.743683 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m19:09:33.744313 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:09:34.636699 [debug] [MainThread]: On debug: select 1 as id
[0m19:09:35.486306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a2a790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a2a1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105dc2460>]}
[0m19:09:35.487172 [debug] [MainThread]: Flushing usage events
[0m19:09:35.943378 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-06 19:20:16.461808 | 01f97def-eb41-4c0f-bc9a-8bc11037ff27 ==============================
[0m19:20:16.461814 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:20:16.462140 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m19:20:16.462209 [debug] [MainThread]: Tracking: tracking
[0m19:20:16.473320 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107cab490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107cab430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107cab520>]}
[0m19:20:16.691050 [debug] [MainThread]: Executing "git --help"
[0m19:20:16.700204 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m19:20:16.700775 [debug] [MainThread]: STDERR: "b''"
[0m19:20:16.704553 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m19:20:16.705268 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:20:17.121708 [debug] [MainThread]: On debug: select 1 as id
[0m19:20:17.881154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10904e970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10904e340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10904e9a0>]}
[0m19:20:17.882844 [debug] [MainThread]: Flushing usage events
[0m19:20:18.331996 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-06 19:23:54.488037 | f2ac38ba-e2eb-40c6-95a9-4f8365a85e52 ==============================
[0m19:23:54.488043 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:23:54.488360 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m19:23:54.488427 [debug] [MainThread]: Tracking: tracking
[0m19:23:54.497547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a37700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a37280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a373d0>]}
[0m19:23:54.728759 [debug] [MainThread]: Executing "git --help"
[0m19:23:54.733772 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m19:23:54.734245 [debug] [MainThread]: STDERR: "b''"
[0m19:23:54.738307 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m19:23:54.738958 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:23:55.178682 [debug] [MainThread]: On debug: select 1 as id
[0m19:23:55.977766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e598e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e59f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e59d00>]}
[0m19:23:55.978969 [debug] [MainThread]: Flushing usage events
[0m19:23:56.431626 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-06 19:26:26.006774 | d3b5f41e-d140-4bae-af3e-3c27b1ee5049 ==============================
[0m19:26:26.006803 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:26:26.007344 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:26:26.007434 [debug] [MainThread]: Tracking: tracking
[0m19:26:26.014702 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076f9f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076f9eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076f9b80>]}
[0m19:26:26.023240 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m19:26:26.023463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'd3b5f41e-d140-4bae-af3e-3c27b1ee5049', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107746580>]}
[0m19:26:26.037165 [debug] [MainThread]: Parsing macros/etc.sql
[0m19:26:26.038536 [debug] [MainThread]: Parsing macros/catalog.sql
[0m19:26:26.041906 [debug] [MainThread]: Parsing macros/adapters.sql
[0m19:26:26.053603 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m19:26:26.054824 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m19:26:26.056109 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m19:26:26.058218 [debug] [MainThread]: Parsing macros/materializations/copy.sql
[0m19:26:26.059443 [debug] [MainThread]: Parsing macros/materializations/incremental.sql
[0m19:26:26.066720 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m19:26:26.067509 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m19:26:26.067681 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m19:26:26.067955 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m19:26:26.068110 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m19:26:26.068370 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m19:26:26.068653 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m19:26:26.069083 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m19:26:26.069623 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m19:26:26.069845 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m19:26:26.070062 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m19:26:26.070293 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m19:26:26.070480 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m19:26:26.071118 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m19:26:26.071345 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m19:26:26.072581 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m19:26:26.074265 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m19:26:26.075265 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m19:26:26.075994 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m19:26:26.083625 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m19:26:26.089889 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m19:26:26.095618 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m19:26:26.097626 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m19:26:26.098409 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m19:26:26.099282 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m19:26:26.101329 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m19:26:26.109522 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m19:26:26.110214 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m19:26:26.114749 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m19:26:26.122115 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m19:26:26.124618 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m19:26:26.125867 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m19:26:26.128361 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m19:26:26.128922 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m19:26:26.130424 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m19:26:26.131417 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m19:26:26.134618 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m19:26:26.143223 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m19:26:26.143875 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m19:26:26.144949 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m19:26:26.145627 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m19:26:26.146018 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m19:26:26.146364 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m19:26:26.146658 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m19:26:26.147253 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m19:26:26.149233 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m19:26:26.153156 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m19:26:26.153513 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m19:26:26.154041 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m19:26:26.154451 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m19:26:26.154851 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m19:26:26.155392 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m19:26:26.155739 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m19:26:26.156179 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m19:26:26.156653 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m19:26:26.157756 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m19:26:26.158286 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m19:26:26.158754 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m19:26:26.159207 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m19:26:26.159654 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m19:26:26.160060 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m19:26:26.160520 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m19:26:26.160908 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m19:26:26.163498 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m19:26:26.163898 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m19:26:26.164686 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m19:26:26.165596 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m19:26:26.166039 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m19:26:26.167019 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m19:26:26.168160 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m19:26:26.175007 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m19:26:26.176285 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m19:26:26.182415 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m19:26:26.184357 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m19:26:26.187563 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m19:26:26.192031 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m19:26:26.311775 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.anomaly_detection

[0m19:26:26.314464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd3b5f41e-d140-4bae-af3e-3c27b1ee5049', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078780d0>]}
[0m19:26:26.317441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd3b5f41e-d140-4bae-af3e-3c27b1ee5049', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077463a0>]}
[0m19:26:26.317613 [info ] [MainThread]: Found 0 models, 0 tests, 0 snapshots, 0 analyses, 285 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m19:26:26.317812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd3b5f41e-d140-4bae-af3e-3c27b1ee5049', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10782daf0>]}
[0m19:26:26.318451 [info ] [MainThread]: 
[0m19:26:26.318575 [warn ] [MainThread]: [[33mWARNING[0m]: Nothing to do. Try checking your model configs and model specification args
[0m19:26:26.321599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107859d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107859b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107859dc0>]}
[0m19:26:26.321740 [debug] [MainThread]: Flushing usage events


============================== 2023-02-06 19:36:54.998897 | 7f0efdbb-792e-410f-a33a-4cd225a348e6 ==============================
[0m19:36:54.998902 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:36:54.999219 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m19:36:54.999287 [debug] [MainThread]: Tracking: tracking
[0m19:36:55.013241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ce3700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ce3280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ce33d0>]}
[0m19:36:55.244218 [debug] [MainThread]: Executing "git --help"
[0m19:36:55.259079 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m19:36:55.259531 [debug] [MainThread]: STDERR: "b''"
[0m19:36:55.262993 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m19:36:55.263752 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:36:55.755435 [debug] [MainThread]: On debug: select 1 as id
[0m19:36:56.657101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105cc61c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107036340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107036a90>]}
[0m19:36:56.657877 [debug] [MainThread]: Flushing usage events
[0m19:36:57.063041 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-06 19:38:14.438458 | 52bf277a-8fd8-49da-947e-e32993ce1737 ==============================
[0m19:38:14.438464 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:38:14.438659 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m19:38:14.438723 [debug] [MainThread]: Tracking: tracking
[0m19:38:14.445943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10610b250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106104160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061042b0>]}
[0m19:38:14.599584 [debug] [MainThread]: Executing "git --help"
[0m19:38:14.604669 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m19:38:14.605250 [debug] [MainThread]: STDERR: "b''"
[0m19:38:14.608574 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m19:38:14.609361 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:38:14.888600 [debug] [MainThread]: On debug: select 1 as id
[0m19:38:15.663699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107499a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074999a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107499580>]}
[0m19:38:15.665110 [debug] [MainThread]: Flushing usage events
[0m19:38:16.110589 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-06 19:49:37.551004 | 97c8dfda-a0b2-41d1-88b5-deec349264bf ==============================
[0m19:49:37.551010 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:49:37.551362 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m19:49:37.551428 [debug] [MainThread]: Tracking: tracking
[0m19:49:37.563399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1085a3400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1085a3190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10859c280>]}
[0m19:49:37.791715 [debug] [MainThread]: Executing "git --help"
[0m19:49:37.805631 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m19:49:37.806036 [debug] [MainThread]: STDERR: "b''"
[0m19:49:37.809730 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m19:49:37.810436 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:49:38.262836 [debug] [MainThread]: On debug: select 1 as id
[0m19:49:39.081531 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109930700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109930dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109930760>]}
[0m19:49:39.082381 [debug] [MainThread]: Flushing usage events
[0m19:49:39.547701 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-06 19:50:12.214810 | 5ffe3d4b-1a8b-4878-937b-6a475328acd5 ==============================
[0m19:50:12.214816 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:50:12.215003 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m19:50:12.215075 [debug] [MainThread]: Tracking: tracking
[0m19:50:12.223458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a77610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a77220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a77250>]}
[0m19:50:12.425632 [debug] [MainThread]: Executing "git --help"
[0m19:50:12.430984 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m19:50:12.431456 [debug] [MainThread]: STDERR: "b''"
[0m19:50:12.434600 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m19:50:12.435421 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:50:12.843360 [debug] [MainThread]: On debug: select 1 as id
[0m19:50:13.479335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107dd5ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107dd59a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107dd52e0>]}
[0m19:50:13.484514 [debug] [MainThread]: Flushing usage events
[0m19:50:13.891719 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-06 20:21:23.168317 | 6efbe037-20d5-4b07-9e8e-3b76e075e3ca ==============================
[0m20:21:23.168323 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:21:23.168604 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m20:21:23.185905 [debug] [MainThread]: Tracking: tracking
[0m20:21:23.194446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1088b7490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1088b7430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1088b7520>]}
[0m20:21:23.425368 [debug] [MainThread]: Executing "git --help"
[0m20:21:23.430517 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:21:23.430995 [debug] [MainThread]: STDERR: "b''"
[0m20:21:23.434895 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m20:21:23.435594 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:21:23.874688 [debug] [MainThread]: On debug: select 1 as id
[0m20:21:24.734617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109c456d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109c452e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109c45bb0>]}
[0m20:21:24.736271 [debug] [MainThread]: Flushing usage events
[0m20:21:25.233004 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-06 20:26:50.564711 | 28812a12-061b-4e60-8664-393cbceb8440 ==============================
[0m20:26:50.564717 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:26:50.564910 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m20:26:50.564974 [debug] [MainThread]: Tracking: tracking
[0m20:26:50.572665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106671eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066712e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106671fa0>]}
[0m20:26:50.724363 [debug] [MainThread]: Executing "git --help"
[0m20:26:50.729285 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:26:50.729715 [debug] [MainThread]: STDERR: "b''"
[0m20:26:50.732805 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m20:26:50.733550 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:26:51.047477 [debug] [MainThread]: On debug: select 1 as id
[0m20:26:51.910247 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1079cb790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1079cbbb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1079cba00>]}
[0m20:26:51.911795 [debug] [MainThread]: Flushing usage events
[0m20:26:52.406919 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-06 20:27:08.671015 | 0af16598-e64f-4086-950d-c9628c819b44 ==============================
[0m20:27:08.671021 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:27:08.671214 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m20:27:08.671289 [debug] [MainThread]: Tracking: tracking
[0m20:27:08.679060 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10448c0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10448c460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10448cf70>]}
[0m20:27:08.829279 [debug] [MainThread]: Executing "git --help"
[0m20:27:08.834349 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:27:08.834732 [debug] [MainThread]: STDERR: "b''"
[0m20:27:08.837657 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m20:27:08.838319 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:27:09.116798 [debug] [MainThread]: On debug: select 1 as id
[0m20:27:09.834375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057e7910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057e7c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057e77c0>]}
[0m20:27:09.835020 [debug] [MainThread]: Flushing usage events
[0m20:27:10.209433 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-06 20:27:29.021585 | ad52c1bc-77c7-47ea-9530-afee78197041 ==============================
[0m20:27:29.021590 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:27:29.021781 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m20:27:29.021846 [debug] [MainThread]: Tracking: tracking
[0m20:27:29.030670 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1045f00d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1045f0460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1045f0f70>]}
[0m20:27:29.181855 [debug] [MainThread]: Executing "git --help"
[0m20:27:29.187621 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:27:29.188175 [debug] [MainThread]: STDERR: "b''"
[0m20:27:29.191520 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m20:27:29.192355 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:27:29.467904 [debug] [MainThread]: On debug: select 1 as id
[0m20:27:30.233135 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b54a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b54af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b54c10>]}
[0m20:27:30.235692 [debug] [MainThread]: Flushing usage events
[0m20:27:30.597554 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-07 16:16:14.455718 | 2f43daca-fdea-4c72-a620-73e8d9c01726 ==============================
[0m16:16:14.455726 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:16:14.456070 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m16:16:14.456137 [debug] [MainThread]: Tracking: tracking
[0m16:16:14.468885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ef53a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ef0280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ef00d0>]}
[0m16:16:14.716016 [debug] [MainThread]: Executing "git --help"
[0m16:16:14.729612 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m16:16:14.730184 [debug] [MainThread]: STDERR: "b''"
[0m16:16:14.734014 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m16:16:14.736035 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:16:15.623847 [debug] [MainThread]: On debug: select 1 as id
[0m16:16:16.397561 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b4fa30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b4f850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b4fc10>]}
[0m16:16:16.398440 [debug] [MainThread]: Flushing usage events
[0m16:16:16.707163 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-07 16:19:12.130858 | c26ae60b-49b0-452b-9140-7e061ffd5dd2 ==============================
[0m16:19:12.130865 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:19:12.131293 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m16:19:12.131365 [debug] [MainThread]: Tracking: tracking
[0m16:19:12.138614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064b00d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064b0460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064b0f70>]}
[0m16:19:12.360566 [debug] [MainThread]: Executing "git --help"
[0m16:19:12.372369 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m16:19:12.372896 [debug] [MainThread]: STDERR: "b''"
[0m16:19:12.376433 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m16:19:12.377204 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:19:12.799795 [debug] [MainThread]: On debug: select 1 as id
[0m16:19:13.583619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107945f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107945bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107945940>]}
[0m16:19:13.584040 [debug] [MainThread]: Flushing usage events
[0m16:19:14.092691 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-07 16:20:37.939872 | d9c9ab73-e62d-4f37-8e09-e3fa42c40723 ==============================
[0m16:20:37.939878 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:20:37.940199 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m16:20:37.940265 [debug] [MainThread]: Tracking: tracking
[0m16:20:37.947747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107877490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107877430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107877520>]}
[0m16:20:38.169044 [debug] [MainThread]: Executing "git --help"
[0m16:20:38.184143 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m16:20:38.184619 [debug] [MainThread]: STDERR: "b''"
[0m16:20:38.188635 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m16:20:38.189613 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:20:38.634044 [debug] [MainThread]: On debug: select 1 as id
[0m16:20:40.186889 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110536850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110536880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110536af0>]}
[0m16:20:40.189645 [debug] [MainThread]: Flushing usage events
[0m16:20:40.661903 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-08 17:19:19.988666 | bea02763-aad5-4752-92e4-00caef072826 ==============================
[0m17:19:19.988672 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:19:19.988862 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m17:19:19.988926 [debug] [MainThread]: Tracking: tracking
[0m17:19:19.999020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103d5a100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103d54130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103d54310>]}
[0m17:19:20.243288 [debug] [MainThread]: Executing "git --help"
[0m17:19:20.248501 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m17:19:20.248915 [debug] [MainThread]: STDERR: "b''"
[0m17:19:20.258378 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m17:19:20.259107 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:19:21.697845 [debug] [MainThread]: On debug: select 1 as id
[0m17:19:23.188466 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048dd2e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048dd760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048dd8b0>]}
[0m17:19:23.189365 [debug] [MainThread]: Flushing usage events
[0m17:19:23.669792 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-08 17:19:27.532578 | 04a88c9d-fb89-49f2-9979-f13c7e663db9 ==============================
[0m17:19:27.532599 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:19:27.532975 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:19:27.533081 [debug] [MainThread]: Tracking: tracking
[0m17:19:27.542072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107703ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107703700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107703520>]}
[0m17:19:27.569166 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:19:27.569305 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:19:27.569403 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.anomaly_detection

[0m17:19:27.572295 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a88c9d-fb89-49f2-9979-f13c7e663db9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1080660d0>]}
[0m17:19:27.575767 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04a88c9d-fb89-49f2-9979-f13c7e663db9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077bf280>]}
[0m17:19:27.575985 [info ] [MainThread]: Found 0 models, 0 tests, 0 snapshots, 0 analyses, 285 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m17:19:27.576124 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04a88c9d-fb89-49f2-9979-f13c7e663db9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108038f10>]}
[0m17:19:27.576811 [info ] [MainThread]: 
[0m17:19:27.576957 [warn ] [MainThread]: [[33mWARNING[0m]: Nothing to do. Try checking your model configs and model specification args
[0m17:19:27.579883 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077bf0a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077bf160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077dac40>]}
[0m17:19:27.580013 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 15:55:38.349353 | 5de54678-b424-410e-aa82-2043e1767aee ==============================
[0m15:55:38.349390 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:55:38.350229 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m15:55:38.350358 [debug] [MainThread]: Tracking: tracking
[0m15:55:38.365952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10551e340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10551e670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10551ec10>]}
[0m15:55:38.391596 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:55:38.391754 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:55:38.391875 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.anomaly_detection

[0m15:55:38.394862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5de54678-b424-410e-aa82-2043e1767aee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057a60d0>]}
[0m15:55:38.398716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5de54678-b424-410e-aa82-2043e1767aee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10570a310>]}
[0m15:55:38.398963 [info ] [MainThread]: Found 0 models, 0 tests, 0 snapshots, 0 analyses, 285 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m15:55:38.399107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5de54678-b424-410e-aa82-2043e1767aee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105778fd0>]}
[0m15:55:38.399764 [info ] [MainThread]: 
[0m15:55:38.399886 [warn ] [MainThread]: [[33mWARNING[0m]: Nothing to do. Try checking your model configs and model specification args
[0m15:55:38.402530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10570a190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10570a0a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10570a040>]}
[0m15:55:38.402645 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 15:56:25.224802 | aef5c7b2-fd62-4ae2-95cc-81864800e140 ==============================
[0m15:56:25.224808 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:56:25.224986 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m15:56:25.225051 [debug] [MainThread]: Tracking: tracking
[0m15:56:25.232811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108097460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108097400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1080974f0>]}
[0m15:56:25.432167 [debug] [MainThread]: Executing "git --help"
[0m15:56:25.439842 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:56:25.440352 [debug] [MainThread]: STDERR: "b''"
[0m15:56:25.443402 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m15:56:25.444051 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:56:26.336989 [debug] [MainThread]: On debug: select 1 as id
[0m15:56:27.720173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105dc9550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109414d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1094125b0>]}
[0m15:56:27.721568 [debug] [MainThread]: Flushing usage events
[0m15:56:28.073883 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-09 15:59:57.067166 | e58924e2-91c4-4548-848a-aad8d3b922b3 ==============================
[0m15:59:57.068386 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:59:57.069765 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m15:59:57.069922 [debug] [MainThread]: Tracking: tracking
[0m15:59:57.079011 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1093cbee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1093cb730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1093cb520>]}
[0m15:59:57.106369 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:59:57.106547 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:59:57.106660 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.anomaly_detection

[0m15:59:57.109921 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e58924e2-91c4-4548-848a-aad8d3b922b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095260d0>]}
[0m15:59:57.113150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e58924e2-91c4-4548-848a-aad8d3b922b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109487250>]}
[0m15:59:57.113303 [info ] [MainThread]: Found 0 models, 0 tests, 0 snapshots, 0 analyses, 285 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m15:59:57.113435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e58924e2-91c4-4548-848a-aad8d3b922b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1094f8ee0>]}
[0m15:59:57.114077 [info ] [MainThread]: 
[0m15:59:57.114195 [warn ] [MainThread]: [[33mWARNING[0m]: Nothing to do. Try checking your model configs and model specification args
[0m15:59:57.116895 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109487190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1094870a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1094a1df0>]}
[0m15:59:57.117028 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 16:04:12.934891 | 6eb48810-afdb-4822-b7b5-546423ef4889 ==============================
[0m16:04:12.934933 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:04:12.935880 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m16:04:12.936015 [debug] [MainThread]: Tracking: tracking
[0m16:04:12.946418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a63ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a63700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a63520>]}
[0m16:04:12.970723 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:04:12.970861 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:04:12.970969 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.anomaly_detection

[0m16:04:12.973750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6eb48810-afdb-4822-b7b5-546423ef4889', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105bbf0d0>]}
[0m16:04:12.976667 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6eb48810-afdb-4822-b7b5-546423ef4889', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b22250>]}
[0m16:04:12.976802 [info ] [MainThread]: Found 0 models, 0 tests, 0 snapshots, 0 analyses, 285 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m16:04:12.976910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6eb48810-afdb-4822-b7b5-546423ef4889', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b92ee0>]}
[0m16:04:12.977172 [warn ] [MainThread]: No nodes selected!
[0m16:04:12.977315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b92ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b222b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b22040>]}
[0m16:04:12.977404 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 16:13:30.608186 | e272464a-d139-47c2-97cb-ec28f6044a5c ==============================
[0m16:13:30.608191 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:13:30.608525 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m16:13:30.608594 [debug] [MainThread]: Tracking: tracking
[0m16:13:30.616670 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081e0250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081e0610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081e0070>]}
[0m16:13:30.636641 [debug] [MainThread]: Executing "git --help"
[0m16:13:30.642148 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m16:13:30.642827 [debug] [MainThread]: STDERR: "b''"
[0m16:13:30.643188 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081eca00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081c9880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081c96a0>]}
[0m16:13:30.643415 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 16:15:59.488758 | 4dcf55d1-9027-4ac2-9a99-c1b96e738723 ==============================
[0m16:15:59.488766 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:15:59.489119 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m16:15:59.489185 [debug] [MainThread]: Tracking: tracking
[0m16:15:59.500678 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055b1ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055b12b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055b1fa0>]}
[0m16:15:59.520947 [debug] [MainThread]: Executing "git --help"
[0m16:15:59.526076 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m16:15:59.526496 [debug] [MainThread]: STDERR: "b''"
[0m16:15:59.526835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105598970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055ddb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055dd9a0>]}
[0m16:15:59.527169 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 16:17:12.428315 | 1dddc62e-71da-4e8c-ac4c-3775e9741acf ==============================
[0m16:17:12.428322 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:17:12.428525 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m16:17:12.428591 [debug] [MainThread]: Tracking: tracking
[0m16:17:12.436854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1117f7700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1117f7280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1117f73d0>]}
[0m16:17:12.654240 [debug] [MainThread]: Executing "git --help"
[0m16:17:12.659249 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m16:17:12.659660 [debug] [MainThread]: STDERR: "b''"
[0m16:17:12.663824 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m16:17:12.664637 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:17:13.107372 [debug] [MainThread]: On debug: select 1 as id
[0m16:17:14.525826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c716d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c71d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c71910>]}
[0m16:17:14.526517 [debug] [MainThread]: Flushing usage events
[0m16:17:14.878215 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-09 16:17:24.523316 | 0292bf9a-c660-46e7-8878-3b27677cde50 ==============================
[0m16:17:24.523361 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:17:24.523718 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m16:17:24.523803 [debug] [MainThread]: Tracking: tracking
[0m16:17:24.532867 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119bf04f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119bf0130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119bf0e80>]}
[0m16:17:24.558289 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:17:24.558439 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:17:24.558559 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.anomaly_detection

[0m16:17:24.561457 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0292bf9a-c660-46e7-8878-3b27677cde50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119d5e0d0>]}
[0m16:17:24.564451 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0292bf9a-c660-46e7-8878-3b27677cde50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119ccceb0>]}
[0m16:17:24.564592 [info ] [MainThread]: Found 0 models, 0 tests, 0 snapshots, 0 analyses, 285 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m16:17:24.564716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0292bf9a-c660-46e7-8878-3b27677cde50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119cccf10>]}
[0m16:17:24.565270 [info ] [MainThread]: 
[0m16:17:24.565394 [warn ] [MainThread]: [[33mWARNING[0m]: Nothing to do. Try checking your model configs and model specification args
[0m16:17:24.567952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119be0100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119be0ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119be0490>]}
[0m16:17:24.568068 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 16:50:46.367997 | 8f4218ff-3082-4f59-9497-7bd0f9c7eb72 ==============================
[0m16:50:46.368002 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:50:46.368344 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m16:50:46.368411 [debug] [MainThread]: Tracking: tracking
[0m16:50:46.379344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121576070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12156f220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12156f2b0>]}
[0m16:50:46.599661 [debug] [MainThread]: Executing "git --help"
[0m16:50:46.613137 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m16:50:46.613567 [debug] [MainThread]: STDERR: "b''"
[0m16:50:46.617634 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m16:50:46.618349 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:50:47.114109 [debug] [MainThread]: On debug: select 1 as id
[0m16:50:48.550882 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10669d520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1232a9610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1232a96a0>]}
[0m16:50:48.552345 [debug] [MainThread]: Flushing usage events
[0m16:50:48.961343 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-09 16:51:08.652087 | e55f3329-656a-48e1-84ab-7cb2110a6186 ==============================
[0m16:51:08.652110 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:51:08.652531 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['my_first_dbt_model'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m16:51:08.652616 [debug] [MainThread]: Tracking: tracking
[0m16:51:08.661843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105edeb20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105edef40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105edebb0>]}
[0m16:51:08.686058 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:51:08.686206 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:51:08.686305 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.anomaly_detection

[0m16:51:08.689253 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e55f3329-656a-48e1-84ab-7cb2110a6186', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106868fa0>]}
[0m16:51:08.692459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e55f3329-656a-48e1-84ab-7cb2110a6186', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fbf070>]}
[0m16:51:08.692635 [info ] [MainThread]: Found 0 models, 0 tests, 0 snapshots, 0 analyses, 285 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m16:51:08.692764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e55f3329-656a-48e1-84ab-7cb2110a6186', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106838d00>]}
[0m16:51:08.693001 [warn ] [MainThread]: The selection criterion 'my_first_dbt_model' does not match any nodes
[0m16:51:08.693433 [info ] [MainThread]: 
[0m16:51:08.693544 [warn ] [MainThread]: [[33mWARNING[0m]: Nothing to do. Try checking your model configs and model specification args
[0m16:51:08.696048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fd7c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fd7d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fd79d0>]}
[0m16:51:08.696157 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 16:51:24.057437 | 5767f579-5377-4536-a8c0-29761d37c7e5 ==============================
[0m16:51:24.057511 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:51:24.058178 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['my_first_dbt_model.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m16:51:24.058294 [debug] [MainThread]: Tracking: tracking
[0m16:51:24.066873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f5f100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f5fa30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f3f190>]}
[0m16:51:24.089851 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:51:24.090012 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:51:24.090109 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.anomaly_detection

[0m16:51:24.092985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5767f579-5377-4536-a8c0-29761d37c7e5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1080dea90>]}
[0m16:51:24.095907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5767f579-5377-4536-a8c0-29761d37c7e5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f5dee0>]}
[0m16:51:24.096041 [info ] [MainThread]: Found 0 models, 0 tests, 0 snapshots, 0 analyses, 285 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m16:51:24.096162 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5767f579-5377-4536-a8c0-29761d37c7e5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1080b1c40>]}
[0m16:51:24.096409 [warn ] [MainThread]: The selection criterion 'my_first_dbt_model.sql' does not match any nodes
[0m16:51:24.096839 [info ] [MainThread]: 
[0m16:51:24.096948 [warn ] [MainThread]: [[33mWARNING[0m]: Nothing to do. Try checking your model configs and model specification args
[0m16:51:24.100226 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108050b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108050e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1080509d0>]}
[0m16:51:24.100390 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 16:53:57.409920 | 64b8cfb2-5b42-49f3-b2bb-d4e03d196ec9 ==============================
[0m16:53:57.409925 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:53:57.410262 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m16:53:57.410327 [debug] [MainThread]: Tracking: tracking
[0m16:53:57.418159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111eb1070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111eb1460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111eb1f40>]}
[0m16:53:57.636633 [debug] [MainThread]: Executing "git --help"
[0m16:53:57.642656 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m16:53:57.643094 [debug] [MainThread]: STDERR: "b''"
[0m16:53:57.646784 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m16:53:57.647444 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:53:58.438399 [debug] [MainThread]: On debug: select 1 as id
[0m16:53:59.869198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111ebcdf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113b05e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113ad54c0>]}
[0m16:53:59.869977 [debug] [MainThread]: Flushing usage events
[0m16:54:00.354547 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-09 16:54:03.757717 | ab10dff5-e47e-43b6-bd66-a0ec535345f2 ==============================
[0m16:54:03.757749 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:54:03.758125 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m16:54:03.758213 [debug] [MainThread]: Tracking: tracking
[0m16:54:03.765203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107cf84c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107cf8100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107cf8880>]}
[0m16:54:03.792225 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 4 files added, 0 files changed.
[0m16:54:03.792446 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/my_second_dbt_model.sql
[0m16:54:03.792586 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/schema.yml
[0m16:54:03.792679 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/my_first_dbt_model.sql
[0m16:54:03.792773 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/sources.yml
[0m16:54:03.800881 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
[0m16:54:03.806716 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
[0m16:54:03.830161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ab10dff5-e47e-43b6-bd66-a0ec535345f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e77ee0>]}
[0m16:54:03.833580 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ab10dff5-e47e-43b6-bd66-a0ec535345f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107de7d30>]}
[0m16:54:03.833742 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 285 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m16:54:03.833876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ab10dff5-e47e-43b6-bd66-a0ec535345f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107de7e50>]}
[0m16:54:03.834640 [info ] [MainThread]: 
[0m16:54:03.834878 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m16:54:03.835274 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow"
[0m16:54:03.835399 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:54:04.942718 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow_dbt_anomaly_detection"
[0m16:54:04.943396 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:54:05.332815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ab10dff5-e47e-43b6-bd66-a0ec535345f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d0c580>]}
[0m16:54:05.334043 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:54:05.334494 [info ] [MainThread]: 
[0m16:54:05.342365 [debug] [Thread-1  ]: Began running node model.anomaly_detection.my_first_dbt_model
[0m16:54:05.342868 [info ] [Thread-1  ]: 1 of 2 START table model dbt_anomaly_detection.my_first_dbt_model .............. [RUN]
[0m16:54:05.343401 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.my_first_dbt_model"
[0m16:54:05.343577 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.my_first_dbt_model
[0m16:54:05.343821 [debug] [Thread-1  ]: Compiling model.anomaly_detection.my_first_dbt_model
[0m16:54:05.348369 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.my_first_dbt_model"
[0m16:54:05.349491 [debug] [Thread-1  ]: finished collecting timing info
[0m16:54:05.349651 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.my_first_dbt_model
[0m16:54:05.365753 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:54:05.654111 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.my_first_dbt_model"
[0m16:54:05.655498 [debug] [Thread-1  ]: On model.anomaly_detection.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.my_first_dbt_model"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`my_first_dbt_model`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



SELECT *
FROM `ld-snowplow`.`dbt_rhashemi`.`aggregations_main`

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
  
[0m16:54:07.916611 [debug] [Thread-1  ]: finished collecting timing info
[0m16:54:07.917022 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ab10dff5-e47e-43b6-bd66-a0ec535345f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107cf8700>]}
[0m16:54:07.917266 [info ] [Thread-1  ]: 1 of 2 OK created table model dbt_anomaly_detection.my_first_dbt_model ......... [[32mCREATE TABLE (500.0 rows, 29.4 KB processed)[0m in 2.57s]
[0m16:54:07.917538 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.my_first_dbt_model
[0m16:54:07.917968 [debug] [Thread-1  ]: Began running node model.anomaly_detection.my_second_dbt_model
[0m16:54:07.918333 [info ] [Thread-1  ]: 2 of 2 START view model dbt_anomaly_detection.my_second_dbt_model .............. [RUN]
[0m16:54:07.918630 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.my_second_dbt_model"
[0m16:54:07.918724 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.my_second_dbt_model
[0m16:54:07.918809 [debug] [Thread-1  ]: Compiling model.anomaly_detection.my_second_dbt_model
[0m16:54:07.920679 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.my_second_dbt_model"
[0m16:54:07.921114 [debug] [Thread-1  ]: finished collecting timing info
[0m16:54:07.921229 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.my_second_dbt_model
[0m16:54:07.934489 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.my_second_dbt_model"
[0m16:54:07.934936 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:54:07.935071 [debug] [Thread-1  ]: On model.anomaly_detection.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.my_second_dbt_model"} */


  create or replace view `ld-snowplow`.`dbt_anomaly_detection`.`my_second_dbt_model`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`my_first_dbt_model`
where id = 1;


[0m16:54:08.727046 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('Unrecognized name: id at [10:7]')
[0m16:54:10.354643 [debug] [Thread-1  ]: finished collecting timing info
[0m16:54:10.356393 [debug] [Thread-1  ]: Database Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)
  Unrecognized name: id at [10:7]
  compiled SQL at target/run/anomaly_detection/models/example/my_second_dbt_model.sql
[0m16:54:10.356883 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ab10dff5-e47e-43b6-bd66-a0ec535345f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110b32880>]}
[0m16:54:10.357389 [error] [Thread-1  ]: 2 of 2 ERROR creating view model dbt_anomaly_detection.my_second_dbt_model ..... [[31mERROR[0m in 2.44s]
[0m16:54:10.357915 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.my_second_dbt_model
[0m16:54:10.359305 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m16:54:10.359789 [info ] [MainThread]: 
[0m16:54:10.360056 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 6.52 seconds (6.52s).
[0m16:54:10.360263 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:54:10.360371 [debug] [MainThread]: Connection 'model.anomaly_detection.my_second_dbt_model' was properly closed.
[0m16:54:10.367166 [info ] [MainThread]: 
[0m16:54:10.367435 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:54:10.367780 [info ] [MainThread]: 
[0m16:54:10.368065 [error] [MainThread]: [33mDatabase Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)[0m
[0m16:54:10.368245 [error] [MainThread]:   Unrecognized name: id at [10:7]
[0m16:54:10.368408 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/my_second_dbt_model.sql
[0m16:54:10.368574 [info ] [MainThread]: 
[0m16:54:10.368746 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
[0m16:54:10.369046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107cea2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ce8cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110b31220>]}
[0m16:54:10.369293 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 16:54:56.848702 | b16e79f5-b304-4e49-803d-cf4c33261a19 ==============================
[0m16:54:56.848708 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:54:56.848907 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m16:54:56.848972 [debug] [MainThread]: Tracking: tracking
[0m16:54:56.857657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048700a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104870460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104870fd0>]}
[0m16:54:57.014508 [debug] [MainThread]: Executing "git --help"
[0m16:54:57.021453 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m16:54:57.021889 [debug] [MainThread]: STDERR: "b''"
[0m16:54:57.025472 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m16:54:57.026115 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:54:57.411053 [debug] [MainThread]: On debug: select 1 as id
[0m16:54:58.789380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105efa5b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105efa3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105efa730>]}
[0m16:54:58.790278 [debug] [MainThread]: Flushing usage events
[0m16:54:59.155333 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-09 16:55:02.912104 | 086eb373-1123-45d7-a810-7e1433caccfd ==============================
[0m16:55:02.912152 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:55:02.912560 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m16:55:02.912646 [debug] [MainThread]: Tracking: tracking
[0m16:55:02.919285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113fa13a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113fa1190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113fa10a0>]}
[0m16:55:02.945562 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 0 files added, 0 files changed.
[0m16:55:02.945753 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/my_second_dbt_model.sql
[0m16:55:02.950163 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'my_second_dbt_model' in the 'models' section of file 'models/example/schema.yml'
[0m16:55:02.961301 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.anomaly_detection.unique_my_second_dbt_model_id.57a0f8c493' (models/example/schema.yml) depends on a node named 'my_second_dbt_model' which was not found
[0m16:55:02.961457 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.anomaly_detection.not_null_my_second_dbt_model_id.151b76d778' (models/example/schema.yml) depends on a node named 'my_second_dbt_model' which was not found
[0m16:55:02.966216 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '086eb373-1123-45d7-a810-7e1433caccfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1141d10d0>]}
[0m16:55:02.970143 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '086eb373-1123-45d7-a810-7e1433caccfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11412edf0>]}
[0m16:55:02.970335 [info ] [MainThread]: Found 1 model, 2 tests, 0 snapshots, 0 analyses, 285 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m16:55:02.970482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '086eb373-1123-45d7-a810-7e1433caccfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114108ee0>]}
[0m16:55:02.971252 [info ] [MainThread]: 
[0m16:55:02.971518 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m16:55:02.971914 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow"
[0m16:55:02.972044 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:55:04.219093 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow_dbt_anomaly_detection"
[0m16:55:04.219842 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:55:04.714393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '086eb373-1123-45d7-a810-7e1433caccfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11412e100>]}
[0m16:55:04.715575 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:55:04.716011 [info ] [MainThread]: 
[0m16:55:04.722987 [debug] [Thread-1  ]: Began running node model.anomaly_detection.my_first_dbt_model
[0m16:55:04.723455 [info ] [Thread-1  ]: 1 of 1 START table model dbt_anomaly_detection.my_first_dbt_model .............. [RUN]
[0m16:55:04.724021 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.my_first_dbt_model"
[0m16:55:04.724206 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.my_first_dbt_model
[0m16:55:04.724488 [debug] [Thread-1  ]: Compiling model.anomaly_detection.my_first_dbt_model
[0m16:55:04.729032 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.my_first_dbt_model"
[0m16:55:04.729797 [debug] [Thread-1  ]: finished collecting timing info
[0m16:55:04.729977 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.my_first_dbt_model
[0m16:55:04.743511 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:55:05.217601 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.my_first_dbt_model"
[0m16:55:05.218612 [debug] [Thread-1  ]: On model.anomaly_detection.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.my_first_dbt_model"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`my_first_dbt_model`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



SELECT *
FROM `ld-snowplow`.`dbt_rhashemi`.`aggregations_main`

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
  
[0m16:55:07.583737 [debug] [Thread-1  ]: finished collecting timing info
[0m16:55:07.584075 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '086eb373-1123-45d7-a810-7e1433caccfd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113fa1280>]}
[0m16:55:07.584288 [info ] [Thread-1  ]: 1 of 1 OK created table model dbt_anomaly_detection.my_first_dbt_model ......... [[32mCREATE TABLE (500.0 rows, 29.4 KB processed)[0m in 2.86s]
[0m16:55:07.584530 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.my_first_dbt_model
[0m16:55:07.585207 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m16:55:07.585407 [info ] [MainThread]: 
[0m16:55:07.585543 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.61 seconds (4.61s).
[0m16:55:07.585660 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:55:07.585720 [debug] [MainThread]: Connection 'model.anomaly_detection.my_first_dbt_model' was properly closed.
[0m16:55:07.589402 [info ] [MainThread]: 
[0m16:55:07.589590 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:55:07.589742 [info ] [MainThread]: 
[0m16:55:07.590014 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m16:55:07.590214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114120430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114120850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11412ef10>]}
[0m16:55:07.590356 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 16:56:12.065743 | 7e33e97c-f4b6-4ad8-97a9-38864dd31cc0 ==============================
[0m16:56:12.065750 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:56:12.066073 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m16:56:12.066138 [debug] [MainThread]: Tracking: tracking
[0m16:56:12.073682 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10877a610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10877a220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10877a250>]}
[0m16:56:12.283219 [debug] [MainThread]: Executing "git --help"
[0m16:56:12.296663 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m16:56:12.297087 [debug] [MainThread]: STDERR: "b''"
[0m16:56:12.300873 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m16:56:12.301559 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:56:12.705087 [debug] [MainThread]: On debug: select 1 as id
[0m16:56:14.131302 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10877a310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109af8100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10875e760>]}
[0m16:56:14.132914 [debug] [MainThread]: Flushing usage events
[0m16:56:14.600705 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-09 16:56:19.964709 | fc046b31-5da2-4033-809b-f77037f50707 ==============================
[0m16:56:19.964738 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:56:19.965131 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m16:56:19.965231 [debug] [MainThread]: Tracking: tracking
[0m16:56:19.972976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fa2c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fa2d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fa2730>]}
[0m16:56:20.000024 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:56:20.000310 [debug] [MainThread]: Partial parsing: update schema file: anomaly_detection://models/example/schema.yml
[0m16:56:20.007130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fc046b31-5da2-4033-809b-f77037f50707', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10614c0d0>]}
[0m16:56:20.010288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fc046b31-5da2-4033-809b-f77037f50707', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fa2520>]}
[0m16:56:20.010461 [info ] [MainThread]: Found 1 model, 2 tests, 0 snapshots, 0 analyses, 285 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m16:56:20.010601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fc046b31-5da2-4033-809b-f77037f50707', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fb03d0>]}
[0m16:56:20.011336 [info ] [MainThread]: 
[0m16:56:20.011628 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m16:56:20.012028 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow"
[0m16:56:20.012177 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:56:21.061875 [debug] [ThreadPool]: Acquiring new bigquery connection "create_ld-snowplow_dbt_anomaly_detection"
[0m16:56:21.062481 [debug] [ThreadPool]: Acquiring new bigquery connection "create_ld-snowplow_dbt_anomaly_detection"
[0m16:56:21.062644 [debug] [ThreadPool]: BigQuery adapter: Creating schema "ld-snowplow.dbt_anomaly_detection".
[0m16:56:21.062798 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:56:21.839633 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow_dbt_anomaly_detection"
[0m16:56:21.840254 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:56:22.209246 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fc046b31-5da2-4033-809b-f77037f50707', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10612c040>]}
[0m16:56:22.211141 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:56:22.211750 [info ] [MainThread]: 
[0m16:56:22.220812 [debug] [Thread-1  ]: Began running node model.anomaly_detection.my_first_dbt_model
[0m16:56:22.221369 [info ] [Thread-1  ]: 1 of 1 START table model dbt_anomaly_detection.my_first_dbt_model .............. [RUN]
[0m16:56:22.222031 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.my_first_dbt_model"
[0m16:56:22.222221 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.my_first_dbt_model
[0m16:56:22.222703 [debug] [Thread-1  ]: Compiling model.anomaly_detection.my_first_dbt_model
[0m16:56:22.227769 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.my_first_dbt_model"
[0m16:56:22.228446 [debug] [Thread-1  ]: finished collecting timing info
[0m16:56:22.228602 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.my_first_dbt_model
[0m16:56:22.255634 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.my_first_dbt_model"
[0m16:56:22.256054 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:56:22.256199 [debug] [Thread-1  ]: On model.anomaly_detection.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.my_first_dbt_model"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`my_first_dbt_model`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



SELECT *
FROM `ld-snowplow`.`dbt_rhashemi`.`aggregations_main`

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
  
[0m16:56:24.863184 [debug] [Thread-1  ]: finished collecting timing info
[0m16:56:24.863849 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc046b31-5da2-4033-809b-f77037f50707', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10611c790>]}
[0m16:56:24.864208 [info ] [Thread-1  ]: 1 of 1 OK created table model dbt_anomaly_detection.my_first_dbt_model ......... [[32mCREATE TABLE (500.0 rows, 29.4 KB processed)[0m in 2.64s]
[0m16:56:24.864604 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.my_first_dbt_model
[0m16:56:24.865732 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m16:56:24.866107 [info ] [MainThread]: 
[0m16:56:24.866325 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.85 seconds (4.85s).
[0m16:56:24.866521 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:56:24.866616 [debug] [MainThread]: Connection 'model.anomaly_detection.my_first_dbt_model' was properly closed.
[0m16:56:24.872558 [info ] [MainThread]: 
[0m16:56:24.872770 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:56:24.872958 [info ] [MainThread]: 
[0m16:56:24.873101 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m16:56:24.873463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f9e3a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10612cfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10612cca0>]}
[0m16:56:24.873701 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 17:21:45.404641 | bf5574ed-3cd2-418e-935d-603750e720cb ==============================
[0m17:21:45.404647 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:21:45.404976 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m17:21:45.405043 [debug] [MainThread]: Tracking: tracking
[0m17:21:45.415165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105db6460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105db6400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105db64f0>]}
[0m17:21:45.635171 [debug] [MainThread]: Executing "git --help"
[0m17:21:45.640572 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m17:21:45.640990 [debug] [MainThread]: STDERR: "b''"
[0m17:21:45.645035 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m17:21:45.645763 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:21:46.442993 [debug] [MainThread]: On debug: select 1 as id
[0m17:21:47.970621 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105db6ca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a45370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a45a30>]}
[0m17:21:47.972122 [debug] [MainThread]: Flushing usage events
[0m17:21:48.431917 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-09 17:22:00.271003 | 062828f7-de68-4bcd-869c-dd515e1684f6 ==============================
[0m17:22:00.271030 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:22:00.271399 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['dbt_first_model'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:22:00.271495 [debug] [MainThread]: Tracking: tracking
[0m17:22:00.278575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f49a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f498e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f49910>]}
[0m17:22:00.279924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f49910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f498e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f497c0>]}
[0m17:22:00.280046 [debug] [MainThread]: Flushing usage events
[0m17:22:00.645102 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found 2 package(s) specified in packages.yml, but only 0 package(s) installed in dbt_packages. Run "dbt deps" to install package dependencies.
[0m17:22:00.647542 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 216, in get_full_manifest
    projects = config.load_dependencies()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/runtime.py", line 327, in load_dependencies
    raise_compiler_error(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error
  dbt found 2 package(s) specified in packages.yml, but only 0 package(s) installed in dbt_packages. Run "dbt deps" to install package dependencies.



============================== 2023-02-09 17:22:11.132220 | 908b361e-1b62-462d-9cda-1c5895b8bc3e ==============================
[0m17:22:11.132226 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:22:11.132428 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'deps', 'rpc_method': 'deps', 'indirect_selection': 'eager'}
[0m17:22:11.132493 [debug] [MainThread]: Tracking: tracking
[0m17:22:11.139024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10504f5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050719d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105071760>]}
[0m17:22:11.141182 [debug] [MainThread]: Set downloads directory='/var/folders/2b/s3pxyycs63s1dl1mn2p9mnq40000gn/T/dbt-downloads-qmcl2lyr'
[0m17:22:11.141585 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m17:22:11.484254 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m17:22:11.485081 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/kristeligt-dagblad/dbt_ml.json
[0m17:22:11.765796 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/kristeligt-dagblad/dbt_ml.json 200
[0m17:22:11.769456 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m17:22:12.037201 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m17:22:12.088120 [info ] [MainThread]: Installing kristeligt-dagblad/dbt_ml
[0m17:22:12.441256 [info ] [MainThread]:   Installed from version 0.5.2
[0m17:22:12.441696 [info ] [MainThread]:   Up to date!
[0m17:22:12.442084 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '908b361e-1b62-462d-9cda-1c5895b8bc3e', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050893d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105089670>]}
[0m17:22:12.442426 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m17:22:12.946003 [info ] [MainThread]:   Installed from version 0.8.6
[0m17:22:12.946200 [info ] [MainThread]:   Updated version available: 1.0.0
[0m17:22:12.946356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '908b361e-1b62-462d-9cda-1c5895b8bc3e', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050d28e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050d27c0>]}
[0m17:22:12.946508 [info ] [MainThread]: 
[0m17:22:12.946666 [info ] [MainThread]: Updates available for packages: ['dbt-labs/dbt_utils']                 
Update your versions in packages.yml, then run dbt deps
[0m17:22:12.947373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10503ac40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050717c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10504f5e0>]}
[0m17:22:12.947503 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 17:22:55.724894 | 69863980-de92-453b-a307-eb1dd24d7908 ==============================
[0m17:22:55.724900 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:22:55.725281 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'deps', 'rpc_method': 'deps', 'indirect_selection': 'eager'}
[0m17:22:55.725357 [debug] [MainThread]: Tracking: tracking
[0m17:22:55.733308 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a5c370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a5c160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a5c610>]}
[0m17:22:55.734669 [debug] [MainThread]: Set downloads directory='/var/folders/2b/s3pxyycs63s1dl1mn2p9mnq40000gn/T/dbt-downloads-_p9iy97u'
[0m17:22:55.735062 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m17:22:56.047898 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m17:22:56.048622 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/kristeligt-dagblad/dbt_ml.json
[0m17:22:56.169634 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/kristeligt-dagblad/dbt_ml.json 200
[0m17:22:56.172857 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m17:22:56.449980 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m17:22:56.497233 [info ] [MainThread]: Installing kristeligt-dagblad/dbt_ml
[0m17:22:57.051638 [info ] [MainThread]:   Installed from version 0.5.2
[0m17:22:57.052156 [info ] [MainThread]:   Up to date!
[0m17:22:57.052533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '69863980-de92-453b-a307-eb1dd24d7908', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112aed490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112aed4c0>]}
[0m17:22:57.052855 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m17:22:57.554139 [info ] [MainThread]:   Installed from version 1.0.0
[0m17:22:57.554345 [info ] [MainThread]:   Up to date!
[0m17:22:57.554506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '69863980-de92-453b-a307-eb1dd24d7908', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a49730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a49940>]}
[0m17:22:57.555038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1128f7d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a49c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a49eb0>]}
[0m17:22:57.555182 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 17:23:05.814226 | c35c9e42-d46f-462e-b5c1-80e99ea24454 ==============================
[0m17:23:05.814249 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:23:05.814800 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['dbt_first_model'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:23:05.814893 [debug] [MainThread]: Tracking: tracking
[0m17:23:05.822345 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057c5160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057c5760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057c5880>]}
[0m17:23:05.832954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105813970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105813100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058134c0>]}
[0m17:23:05.833148 [debug] [MainThread]: Flushing usage events
[0m17:23:06.310955 [error] [MainThread]: Encountered an error:
Runtime Error
  Failed to read package: Runtime Error
    This version of dbt is not supported with the 'dbt_utils' package.
      Installed version of dbt: =1.2.1
      Required version of dbt for 'dbt_utils': ['>=1.3.0', '<2.0.0']
    Check for a different version of the 'dbt_utils' package, or run dbt again with --no-version-check
    
  
  Error encountered in /Users/rana/bi-dbt-anomaly-detection/dbt_packages/dbt_utils/dbt_project.yml

Error encountered in /Users/rana/bi-dbt-anomaly-detection/dbt_packages/dbt_utils
[0m17:23:06.313319 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/runtime.py", line 354, in load_projects
    project = self.new_project(str(path))
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/runtime.py", line 140, in new_project
    project = Project.from_project_root(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/project.py", line 653, in from_project_root
    return partial.render(renderer)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/project.py", line 286, in render
    return self.create_project(rendered)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/project.py", line 316, in create_project
    dbt_version = _get_required_version(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/project.py", line 231, in _get_required_version
    validate_version(dbt_version, project_dict["name"])
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/project.py", line 208, in validate_version
    raise DbtProjectError(msg)
dbt.exceptions.DbtProjectError: Runtime Error
  This version of dbt is not supported with the 'dbt_utils' package.
    Installed version of dbt: =1.2.1
    Required version of dbt for 'dbt_utils': ['>=1.3.0', '<2.0.0']
  Check for a different version of the 'dbt_utils' package, or run dbt again with --no-version-check
  

Error encountered in /Users/rana/bi-dbt-anomaly-detection/dbt_packages/dbt_utils/dbt_project.yml

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 216, in get_full_manifest
    projects = config.load_dependencies()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/runtime.py", line 335, in load_dependencies
    for project_name, project in self.load_projects(project_paths):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/runtime.py", line 356, in load_projects
    raise DbtProjectError(
dbt.exceptions.DbtProjectError: Runtime Error
  Failed to read package: Runtime Error
    This version of dbt is not supported with the 'dbt_utils' package.
      Installed version of dbt: =1.2.1
      Required version of dbt for 'dbt_utils': ['>=1.3.0', '<2.0.0']
    Check for a different version of the 'dbt_utils' package, or run dbt again with --no-version-check
    
  
  Error encountered in /Users/rana/bi-dbt-anomaly-detection/dbt_packages/dbt_utils/dbt_project.yml

Error encountered in /Users/rana/bi-dbt-anomaly-detection/dbt_packages/dbt_utils



============================== 2023-02-09 17:23:44.057676 | b615d3da-a252-43f7-869e-c04d98212ccd ==============================
[0m17:23:44.057695 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:23:44.058066 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['dbt_first_model'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:23:44.058186 [debug] [MainThread]: Tracking: tracking
[0m17:23:44.066291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10978dbb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10978d730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10978dac0>]}
[0m17:23:44.075618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10978d7f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10977a340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1097e2ca0>]}
[0m17:23:44.075801 [debug] [MainThread]: Flushing usage events
[0m17:23:44.451756 [error] [MainThread]: Encountered an error:
Runtime Error
  Failed to read package: Runtime Error
    This version of dbt is not supported with the 'dbt_utils' package.
      Installed version of dbt: =1.2.1
      Required version of dbt for 'dbt_utils': ['>=1.3.0', '<2.0.0']
    Check for a different version of the 'dbt_utils' package, or run dbt again with --no-version-check
    
  
  Error encountered in /Users/rana/bi-dbt-anomaly-detection/dbt_packages/dbt_utils/dbt_project.yml

Error encountered in /Users/rana/bi-dbt-anomaly-detection/dbt_packages/dbt_utils
[0m17:23:44.454062 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/runtime.py", line 354, in load_projects
    project = self.new_project(str(path))
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/runtime.py", line 140, in new_project
    project = Project.from_project_root(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/project.py", line 653, in from_project_root
    return partial.render(renderer)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/project.py", line 286, in render
    return self.create_project(rendered)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/project.py", line 316, in create_project
    dbt_version = _get_required_version(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/project.py", line 231, in _get_required_version
    validate_version(dbt_version, project_dict["name"])
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/project.py", line 208, in validate_version
    raise DbtProjectError(msg)
dbt.exceptions.DbtProjectError: Runtime Error
  This version of dbt is not supported with the 'dbt_utils' package.
    Installed version of dbt: =1.2.1
    Required version of dbt for 'dbt_utils': ['>=1.3.0', '<2.0.0']
  Check for a different version of the 'dbt_utils' package, or run dbt again with --no-version-check
  

Error encountered in /Users/rana/bi-dbt-anomaly-detection/dbt_packages/dbt_utils/dbt_project.yml

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 216, in get_full_manifest
    projects = config.load_dependencies()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/runtime.py", line 335, in load_dependencies
    for project_name, project in self.load_projects(project_paths):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/runtime.py", line 356, in load_projects
    raise DbtProjectError(
dbt.exceptions.DbtProjectError: Runtime Error
  Failed to read package: Runtime Error
    This version of dbt is not supported with the 'dbt_utils' package.
      Installed version of dbt: =1.2.1
      Required version of dbt for 'dbt_utils': ['>=1.3.0', '<2.0.0']
    Check for a different version of the 'dbt_utils' package, or run dbt again with --no-version-check
    
  
  Error encountered in /Users/rana/bi-dbt-anomaly-detection/dbt_packages/dbt_utils/dbt_project.yml

Error encountered in /Users/rana/bi-dbt-anomaly-detection/dbt_packages/dbt_utils



============================== 2023-02-09 17:25:21.240198 | ba7db401-342b-48fc-be05-c0d9744d5ce1 ==============================
[0m17:25:21.240204 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:25:21.240412 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'deps', 'rpc_method': 'deps', 'indirect_selection': 'eager'}
[0m17:25:21.240481 [debug] [MainThread]: Tracking: tracking
[0m17:25:21.248137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106916a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106916bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106916be0>]}
[0m17:25:21.250039 [debug] [MainThread]: Set downloads directory='/var/folders/2b/s3pxyycs63s1dl1mn2p9mnq40000gn/T/dbt-downloads-q35xwjzd'
[0m17:25:21.250519 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m17:25:21.582986 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m17:25:21.583698 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/kristeligt-dagblad/dbt_ml.json
[0m17:25:21.851285 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/kristeligt-dagblad/dbt_ml.json 200
[0m17:25:21.856178 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m17:25:22.144660 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m17:25:22.194623 [info ] [MainThread]: Installing kristeligt-dagblad/dbt_ml
[0m17:25:22.584258 [info ] [MainThread]:   Installed from version 0.5.2
[0m17:25:22.584708 [info ] [MainThread]:   Up to date!
[0m17:25:22.585074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'ba7db401-342b-48fc-be05-c0d9744d5ce1', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069164f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069164c0>]}
[0m17:25:22.585370 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m17:25:23.040005 [info ] [MainThread]:   Installed from version 1.0.0
[0m17:25:23.040228 [info ] [MainThread]:   Up to date!
[0m17:25:23.040404 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'ba7db401-342b-48fc-be05-c0d9744d5ce1', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106964910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106964070>]}
[0m17:25:23.041042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106836790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068b72e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106916310>]}
[0m17:25:23.041181 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 17:25:46.259581 | 45b2c188-e476-4694-b44c-8d456ac2f934 ==============================
[0m17:25:46.259586 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:25:46.259778 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m17:25:46.259843 [debug] [MainThread]: Tracking: tracking
[0m17:25:46.269181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110cb6460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110cb6400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110cb64f0>]}
[0m17:25:46.445785 [debug] [MainThread]: Executing "git --help"
[0m17:25:46.451723 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m17:25:46.452232 [debug] [MainThread]: STDERR: "b''"
[0m17:25:46.456833 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m17:25:46.457596 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:25:46.838817 [debug] [MainThread]: On debug: select 1 as id
[0m17:25:48.130613 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064ad550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a45cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a458e0>]}
[0m17:25:48.131477 [debug] [MainThread]: Flushing usage events
[0m17:25:48.492966 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-09 17:25:52.659119 | b5cbbad3-83e3-4c88-ad58-6f0af4712778 ==============================
[0m17:25:52.659159 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:25:52.659569 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:25:52.659657 [debug] [MainThread]: Tracking: tracking
[0m17:25:52.668689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108626d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108626490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108626130>]}
[0m17:25:52.679784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10862b2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10862b910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10868edc0>]}
[0m17:25:52.679970 [debug] [MainThread]: Flushing usage events
[0m17:25:53.038658 [error] [MainThread]: Encountered an error:
Runtime Error
  Failed to read package: Runtime Error
    This version of dbt is not supported with the 'dbt_utils' package.
      Installed version of dbt: =1.2.1
      Required version of dbt for 'dbt_utils': ['>=1.3.0', '<2.0.0']
    Check for a different version of the 'dbt_utils' package, or run dbt again with --no-version-check
    
  
  Error encountered in /Users/rana/bi-dbt-anomaly-detection/dbt_packages/dbt_utils/dbt_project.yml

Error encountered in /Users/rana/bi-dbt-anomaly-detection/dbt_packages/dbt_utils
[0m17:25:53.040739 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/runtime.py", line 354, in load_projects
    project = self.new_project(str(path))
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/runtime.py", line 140, in new_project
    project = Project.from_project_root(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/project.py", line 653, in from_project_root
    return partial.render(renderer)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/project.py", line 286, in render
    return self.create_project(rendered)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/project.py", line 316, in create_project
    dbt_version = _get_required_version(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/project.py", line 231, in _get_required_version
    validate_version(dbt_version, project_dict["name"])
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/project.py", line 208, in validate_version
    raise DbtProjectError(msg)
dbt.exceptions.DbtProjectError: Runtime Error
  This version of dbt is not supported with the 'dbt_utils' package.
    Installed version of dbt: =1.2.1
    Required version of dbt for 'dbt_utils': ['>=1.3.0', '<2.0.0']
  Check for a different version of the 'dbt_utils' package, or run dbt again with --no-version-check
  

Error encountered in /Users/rana/bi-dbt-anomaly-detection/dbt_packages/dbt_utils/dbt_project.yml

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 216, in get_full_manifest
    projects = config.load_dependencies()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/runtime.py", line 335, in load_dependencies
    for project_name, project in self.load_projects(project_paths):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/runtime.py", line 356, in load_projects
    raise DbtProjectError(
dbt.exceptions.DbtProjectError: Runtime Error
  Failed to read package: Runtime Error
    This version of dbt is not supported with the 'dbt_utils' package.
      Installed version of dbt: =1.2.1
      Required version of dbt for 'dbt_utils': ['>=1.3.0', '<2.0.0']
    Check for a different version of the 'dbt_utils' package, or run dbt again with --no-version-check
    
  
  Error encountered in /Users/rana/bi-dbt-anomaly-detection/dbt_packages/dbt_utils/dbt_project.yml

Error encountered in /Users/rana/bi-dbt-anomaly-detection/dbt_packages/dbt_utils



============================== 2023-02-09 17:27:01.873368 | 19e3e8b3-4ee3-4a12-a4ca-4106846ca4f7 ==============================
[0m17:27:01.873419 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:27:01.873845 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:27:01.873951 [debug] [MainThread]: Tracking: tracking
[0m17:27:01.881590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109e60b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109e60370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109e60280>]}
[0m17:27:01.909619 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:27:01.909761 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:27:01.912880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '19e3e8b3-4ee3-4a12-a4ca-4106846ca4f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ff70d0>]}
[0m17:27:01.916783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '19e3e8b3-4ee3-4a12-a4ca-4106846ca4f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f50970>]}
[0m17:27:01.916922 [info ] [MainThread]: Found 1 model, 2 tests, 0 snapshots, 0 analyses, 285 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m17:27:01.917049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '19e3e8b3-4ee3-4a12-a4ca-4106846ca4f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f509d0>]}
[0m17:27:01.917768 [info ] [MainThread]: 
[0m17:27:01.917998 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m17:27:01.918409 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow"
[0m17:27:01.918557 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:27:03.125082 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow_dbt_anomaly_detection"
[0m17:27:03.125524 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:27:03.514539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '19e3e8b3-4ee3-4a12-a4ca-4106846ca4f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109e62580>]}
[0m17:27:03.515909 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:27:03.516345 [info ] [MainThread]: 
[0m17:27:03.523661 [debug] [Thread-1  ]: Began running node model.anomaly_detection.my_first_dbt_model
[0m17:27:03.524138 [info ] [Thread-1  ]: 1 of 1 START table model dbt_anomaly_detection.my_first_dbt_model .............. [RUN]
[0m17:27:03.524696 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.my_first_dbt_model"
[0m17:27:03.524869 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.my_first_dbt_model
[0m17:27:03.525192 [debug] [Thread-1  ]: Compiling model.anomaly_detection.my_first_dbt_model
[0m17:27:03.530035 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.my_first_dbt_model"
[0m17:27:03.531241 [debug] [Thread-1  ]: finished collecting timing info
[0m17:27:03.531451 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.my_first_dbt_model
[0m17:27:03.544550 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:27:03.990613 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.my_first_dbt_model"
[0m17:27:03.991582 [debug] [Thread-1  ]: On model.anomaly_detection.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.my_first_dbt_model"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`my_first_dbt_model`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



SELECT *
FROM `ld-snowplow`.`dbt_rhashemi`.`aggregations_main`

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
  
[0m17:27:06.334643 [debug] [Thread-1  ]: finished collecting timing info
[0m17:27:06.335309 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19e3e8b3-4ee3-4a12-a4ca-4106846ca4f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a8d0c10>]}
[0m17:27:06.335683 [info ] [Thread-1  ]: 1 of 1 OK created table model dbt_anomaly_detection.my_first_dbt_model ......... [[32mCREATE TABLE (500.0 rows, 29.4 KB processed)[0m in 2.81s]
[0m17:27:06.336092 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.my_first_dbt_model
[0m17:27:06.337313 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m17:27:06.337686 [info ] [MainThread]: 
[0m17:27:06.337911 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.42 seconds (4.42s).
[0m17:27:06.338104 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:27:06.338198 [debug] [MainThread]: Connection 'model.anomaly_detection.my_first_dbt_model' was properly closed.
[0m17:27:06.350135 [info ] [MainThread]: 
[0m17:27:06.350358 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:27:06.350518 [info ] [MainThread]: 
[0m17:27:06.350658 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m17:27:06.350880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046244f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a8d0220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a8d0160>]}
[0m17:27:06.351028 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 17:27:32.116799 | 8eecc6d2-c2dc-4d35-9419-18c94f7d960b ==============================
[0m17:27:32.116807 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:27:32.117004 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'deps', 'rpc_method': 'deps', 'indirect_selection': 'eager'}
[0m17:27:32.117069 [debug] [MainThread]: Tracking: tracking
[0m17:27:32.125363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046ae970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046aea90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046aeac0>]}
[0m17:27:32.126818 [debug] [MainThread]: Set downloads directory='/var/folders/2b/s3pxyycs63s1dl1mn2p9mnq40000gn/T/dbt-downloads-493e4bpc'
[0m17:27:32.127205 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m17:27:33.275505 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m17:27:33.276357 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/kristeligt-dagblad/dbt_ml.json
[0m17:27:33.560081 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/kristeligt-dagblad/dbt_ml.json 200
[0m17:27:33.563639 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m17:27:33.875451 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m17:27:33.925722 [info ] [MainThread]: Installing kristeligt-dagblad/dbt_ml
[0m17:27:34.317926 [info ] [MainThread]:   Installed from version 0.5.2
[0m17:27:34.318369 [info ] [MainThread]:   Up to date!
[0m17:27:34.318764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '8eecc6d2-c2dc-4d35-9419-18c94f7d960b', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046ae1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046aefa0>]}
[0m17:27:34.319073 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m17:27:34.775819 [info ] [MainThread]:   Installed from version 1.0.0
[0m17:27:34.776033 [info ] [MainThread]:   Up to date!
[0m17:27:34.776210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '8eecc6d2-c2dc-4d35-9419-18c94f7d960b', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046f3ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104623be0>]}
[0m17:27:34.776797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046ae2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104648340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046f3e80>]}
[0m17:27:34.776929 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 17:27:49.859360 | 4e4361d8-0aa9-4600-979d-2ed9a7b99d15 ==============================
[0m17:27:49.859407 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:27:49.859783 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:27:49.859886 [debug] [MainThread]: Tracking: tracking
[0m17:27:49.867880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112220a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112220130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112220100>]}
[0m17:27:49.877054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11220cdf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11220c070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11226e250>]}
[0m17:27:49.877219 [debug] [MainThread]: Flushing usage events
[0m17:27:50.233935 [error] [MainThread]: Encountered an error:
Runtime Error
  Failed to read package: Runtime Error
    This version of dbt is not supported with the 'dbt_utils' package.
      Installed version of dbt: =1.2.1
      Required version of dbt for 'dbt_utils': ['>=1.3.0', '<2.0.0']
    Check for a different version of the 'dbt_utils' package, or run dbt again with --no-version-check
    
  
  Error encountered in /Users/rana/bi-dbt-anomaly-detection/dbt_packages/dbt_utils/dbt_project.yml

Error encountered in /Users/rana/bi-dbt-anomaly-detection/dbt_packages/dbt_utils
[0m17:27:50.236910 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/runtime.py", line 354, in load_projects
    project = self.new_project(str(path))
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/runtime.py", line 140, in new_project
    project = Project.from_project_root(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/project.py", line 653, in from_project_root
    return partial.render(renderer)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/project.py", line 286, in render
    return self.create_project(rendered)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/project.py", line 316, in create_project
    dbt_version = _get_required_version(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/project.py", line 231, in _get_required_version
    validate_version(dbt_version, project_dict["name"])
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/project.py", line 208, in validate_version
    raise DbtProjectError(msg)
dbt.exceptions.DbtProjectError: Runtime Error
  This version of dbt is not supported with the 'dbt_utils' package.
    Installed version of dbt: =1.2.1
    Required version of dbt for 'dbt_utils': ['>=1.3.0', '<2.0.0']
  Check for a different version of the 'dbt_utils' package, or run dbt again with --no-version-check
  

Error encountered in /Users/rana/bi-dbt-anomaly-detection/dbt_packages/dbt_utils/dbt_project.yml

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 216, in get_full_manifest
    projects = config.load_dependencies()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/runtime.py", line 335, in load_dependencies
    for project_name, project in self.load_projects(project_paths):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/config/runtime.py", line 356, in load_projects
    raise DbtProjectError(
dbt.exceptions.DbtProjectError: Runtime Error
  Failed to read package: Runtime Error
    This version of dbt is not supported with the 'dbt_utils' package.
      Installed version of dbt: =1.2.1
      Required version of dbt for 'dbt_utils': ['>=1.3.0', '<2.0.0']
    Check for a different version of the 'dbt_utils' package, or run dbt again with --no-version-check
    
  
  Error encountered in /Users/rana/bi-dbt-anomaly-detection/dbt_packages/dbt_utils/dbt_project.yml

Error encountered in /Users/rana/bi-dbt-anomaly-detection/dbt_packages/dbt_utils



============================== 2023-02-09 17:28:30.623208 | c37117a4-3dfa-4496-b2fa-4c40ad9ccb50 ==============================
[0m17:28:30.623214 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:28:30.623410 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'deps', 'rpc_method': 'deps', 'indirect_selection': 'eager'}
[0m17:28:30.623477 [debug] [MainThread]: Tracking: tracking
[0m17:28:30.632131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c369d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103bdee80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103bde520>]}
[0m17:28:30.633342 [debug] [MainThread]: Set downloads directory='/var/folders/2b/s3pxyycs63s1dl1mn2p9mnq40000gn/T/dbt-downloads-e7e_6lwm'
[0m17:28:30.633732 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m17:28:31.004730 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m17:28:31.005543 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/kristeligt-dagblad/dbt_ml.json
[0m17:28:31.285003 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/kristeligt-dagblad/dbt_ml.json 200
[0m17:28:31.289158 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m17:28:31.558152 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m17:28:31.610181 [info ] [MainThread]: Installing kristeligt-dagblad/dbt_ml
[0m17:28:31.906482 [info ] [MainThread]:   Installed from version 0.5.2
[0m17:28:31.906942 [info ] [MainThread]:   Up to date!
[0m17:28:31.907319 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'c37117a4-3dfa-4496-b2fa-4c40ad9ccb50', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c36c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103bde8e0>]}
[0m17:28:31.907645 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m17:28:32.399083 [info ] [MainThread]:   Installed from version 0.8.6
[0m17:28:32.399286 [info ] [MainThread]:   Updated version available: 1.0.0
[0m17:28:32.399441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'c37117a4-3dfa-4496-b2fa-4c40ad9ccb50', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c84820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c847c0>]}
[0m17:28:32.399585 [info ] [MainThread]: 
[0m17:28:32.399703 [info ] [MainThread]: Updates available for packages: ['dbt-labs/dbt_utils']                 
Update your versions in packages.yml, then run dbt deps
[0m17:28:32.400154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103b8fdc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103bd7130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103bd7370>]}
[0m17:28:32.400271 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 17:28:37.744462 | 359e886c-899d-489f-9182-63fade666cde ==============================
[0m17:28:37.744492 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:28:37.744846 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:28:37.744941 [debug] [MainThread]: Tracking: tracking
[0m17:28:37.752426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d05910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d057f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d05820>]}
[0m17:28:37.771677 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
[0m17:28:37.771905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '359e886c-899d-489f-9182-63fade666cde', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ce3cd0>]}
[0m17:28:37.792422 [debug] [MainThread]: Parsing macros/etc.sql
[0m17:28:37.793668 [debug] [MainThread]: Parsing macros/catalog.sql
[0m17:28:37.796940 [debug] [MainThread]: Parsing macros/adapters.sql
[0m17:28:37.808264 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m17:28:37.809494 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m17:28:37.810745 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m17:28:37.812792 [debug] [MainThread]: Parsing macros/materializations/copy.sql
[0m17:28:37.814033 [debug] [MainThread]: Parsing macros/materializations/incremental.sql
[0m17:28:37.821252 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m17:28:37.822141 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m17:28:37.822354 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m17:28:37.822667 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m17:28:37.822829 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m17:28:37.823080 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m17:28:37.823360 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m17:28:37.823795 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m17:28:37.824342 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m17:28:37.824569 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m17:28:37.824798 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m17:28:37.825053 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m17:28:37.825240 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m17:28:37.825850 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m17:28:37.826092 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m17:28:37.827322 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m17:28:37.829015 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m17:28:37.830005 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m17:28:37.830740 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m17:28:37.838327 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m17:28:37.844614 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m17:28:37.850367 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m17:28:37.852365 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m17:28:37.853148 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m17:28:37.853926 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m17:28:37.855886 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m17:28:37.863965 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m17:28:37.864635 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m17:28:37.868929 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m17:28:37.876301 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m17:28:37.878793 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m17:28:37.880037 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m17:28:37.882518 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m17:28:37.883073 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m17:28:37.884575 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m17:28:37.885557 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m17:28:37.888749 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m17:28:37.897556 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m17:28:37.898198 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m17:28:37.899275 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m17:28:37.899952 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m17:28:37.900340 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m17:28:37.900683 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m17:28:37.900978 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m17:28:37.901571 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m17:28:37.903520 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m17:28:37.907404 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m17:28:37.907759 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m17:28:37.908292 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m17:28:37.908702 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m17:28:37.909101 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m17:28:37.909641 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m17:28:37.909987 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m17:28:37.910433 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m17:28:37.910902 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m17:28:37.912034 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m17:28:37.912580 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m17:28:37.913051 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m17:28:37.913509 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m17:28:37.913951 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m17:28:37.914351 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m17:28:37.914819 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m17:28:37.915208 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m17:28:37.917830 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m17:28:37.918227 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m17:28:37.919007 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m17:28:37.919921 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m17:28:37.920367 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m17:28:37.921345 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m17:28:37.922486 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m17:28:37.929216 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m17:28:37.930485 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m17:28:37.936568 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m17:28:37.938525 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m17:28:37.941720 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m17:28:37.946173 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m17:28:37.947491 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
[0m17:28:37.947933 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
[0m17:28:37.948467 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
[0m17:28:37.948882 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
[0m17:28:37.951398 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
[0m17:28:37.951886 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_array_to_string.sql
[0m17:28:37.953055 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
[0m17:28:37.953592 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
[0m17:28:37.954823 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
[0m17:28:37.955255 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
[0m17:28:37.956022 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
[0m17:28:37.957065 [debug] [MainThread]: Parsing macros/cross_db_utils/listagg.sql
[0m17:28:37.961178 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
[0m17:28:37.965822 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
[0m17:28:37.966611 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
[0m17:28:37.967195 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
[0m17:28:37.967794 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
[0m17:28:37.968488 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
[0m17:28:37.969017 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
[0m17:28:37.969676 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
[0m17:28:37.970245 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
[0m17:28:37.971672 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
[0m17:28:37.974129 [debug] [MainThread]: Parsing macros/cross_db_utils/array_concat.sql
[0m17:28:37.974923 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
[0m17:28:37.975643 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
[0m17:28:37.977168 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
[0m17:28:37.979934 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
[0m17:28:37.980561 [debug] [MainThread]: Parsing macros/cross_db_utils/array_construct.sql
[0m17:28:37.981733 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
[0m17:28:37.982645 [debug] [MainThread]: Parsing macros/cross_db_utils/array_append.sql
[0m17:28:37.983541 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
[0m17:28:37.995268 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
[0m17:28:37.996145 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
[0m17:28:37.997297 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
[0m17:28:37.997974 [debug] [MainThread]: Parsing macros/generic_tests/fewer_rows_than.sql
[0m17:28:37.998756 [debug] [MainThread]: Parsing macros/generic_tests/equal_rowcount.sql
[0m17:28:37.999467 [debug] [MainThread]: Parsing macros/generic_tests/relationships_where.sql
[0m17:28:38.000466 [debug] [MainThread]: Parsing macros/generic_tests/recency.sql
[0m17:28:38.001221 [debug] [MainThread]: Parsing macros/generic_tests/not_constant.sql
[0m17:28:38.001745 [debug] [MainThread]: Parsing macros/generic_tests/accepted_range.sql
[0m17:28:38.002881 [debug] [MainThread]: Parsing macros/generic_tests/not_accepted_values.sql
[0m17:28:38.003816 [debug] [MainThread]: Parsing macros/generic_tests/test_unique_where.sql
[0m17:28:38.004428 [debug] [MainThread]: Parsing macros/generic_tests/at_least_one.sql
[0m17:28:38.004967 [debug] [MainThread]: Parsing macros/generic_tests/unique_combination_of_columns.sql
[0m17:28:38.006238 [debug] [MainThread]: Parsing macros/generic_tests/cardinality_equality.sql
[0m17:28:38.007149 [debug] [MainThread]: Parsing macros/generic_tests/expression_is_true.sql
[0m17:28:38.007935 [debug] [MainThread]: Parsing macros/generic_tests/not_null_proportion.sql
[0m17:28:38.008892 [debug] [MainThread]: Parsing macros/generic_tests/sequential_values.sql
[0m17:28:38.010219 [debug] [MainThread]: Parsing macros/generic_tests/test_not_null_where.sql
[0m17:28:38.010833 [debug] [MainThread]: Parsing macros/generic_tests/equality.sql
[0m17:28:38.012443 [debug] [MainThread]: Parsing macros/generic_tests/mutually_exclusive_ranges.sql
[0m17:28:38.016801 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
[0m17:28:38.017275 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
[0m17:28:38.017791 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
[0m17:28:38.018256 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
[0m17:28:38.018761 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
[0m17:28:38.020613 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
[0m17:28:38.021369 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
[0m17:28:38.022897 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
[0m17:28:38.024797 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
[0m17:28:38.026336 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
[0m17:28:38.027063 [debug] [MainThread]: Parsing macros/sql/star.sql
[0m17:28:38.028730 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
[0m17:28:38.032444 [debug] [MainThread]: Parsing macros/sql/union.sql
[0m17:28:38.037817 [debug] [MainThread]: Parsing macros/sql/groupby.sql
[0m17:28:38.038396 [debug] [MainThread]: Parsing macros/sql/deduplicate.sql
[0m17:28:38.041646 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
[0m17:28:38.043142 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
[0m17:28:38.043832 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
[0m17:28:38.044529 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
[0m17:28:38.047507 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
[0m17:28:38.050081 [debug] [MainThread]: Parsing macros/sql/pivot.sql
[0m17:28:38.051953 [debug] [MainThread]: Parsing macros/sql/get_filtered_columns_in_relation.sql
[0m17:28:38.053166 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
[0m17:28:38.054165 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
[0m17:28:38.054891 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
[0m17:28:38.058913 [debug] [MainThread]: Parsing macros/predict.sql
[0m17:28:38.059300 [debug] [MainThread]: Parsing macros/recommend.sql
[0m17:28:38.059667 [debug] [MainThread]: Parsing macros/detect_anomalies.sql
[0m17:28:38.059963 [debug] [MainThread]: Parsing macros/hparam.sql
[0m17:28:38.060393 [debug] [MainThread]: Parsing macros/materializations/model.sql
[0m17:28:38.065979 [debug] [MainThread]: Parsing macros/hooks/model_audit.sql
[0m17:28:38.295485 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
[0m17:28:38.345324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '359e886c-899d-489f-9182-63fade666cde', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068f20d0>]}
[0m17:28:38.350241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '359e886c-899d-489f-9182-63fade666cde', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068173d0>]}
[0m17:28:38.350374 [info ] [MainThread]: Found 1 model, 2 tests, 0 snapshots, 0 analyses, 540 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m17:28:38.350491 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '359e886c-899d-489f-9182-63fade666cde', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d05310>]}
[0m17:28:38.351027 [info ] [MainThread]: 
[0m17:28:38.351262 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m17:28:38.351597 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow"
[0m17:28:38.351702 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:28:39.651968 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow_dbt_anomaly_detection"
[0m17:28:39.652245 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:28:40.023124 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '359e886c-899d-489f-9182-63fade666cde', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068f25b0>]}
[0m17:28:40.024448 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:28:40.024949 [info ] [MainThread]: 
[0m17:28:40.032194 [debug] [Thread-1  ]: Began running node model.anomaly_detection.my_first_dbt_model
[0m17:28:40.032705 [info ] [Thread-1  ]: 1 of 1 START table model dbt_anomaly_detection.my_first_dbt_model .............. [RUN]
[0m17:28:40.033283 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.my_first_dbt_model"
[0m17:28:40.033466 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.my_first_dbt_model
[0m17:28:40.033740 [debug] [Thread-1  ]: Compiling model.anomaly_detection.my_first_dbt_model
[0m17:28:40.039159 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.my_first_dbt_model"
[0m17:28:40.039917 [debug] [Thread-1  ]: finished collecting timing info
[0m17:28:40.040090 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.my_first_dbt_model
[0m17:28:40.054007 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:28:40.520313 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.my_first_dbt_model"
[0m17:28:40.521467 [debug] [Thread-1  ]: On model.anomaly_detection.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.my_first_dbt_model"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`my_first_dbt_model`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



SELECT *
FROM `ld-snowplow`.`dbt_rhashemi`.`aggregations_main`

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
  
[0m17:28:42.877615 [debug] [Thread-1  ]: finished collecting timing info
[0m17:28:42.878356 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '359e886c-899d-489f-9182-63fade666cde', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068fdf40>]}
[0m17:28:42.878760 [info ] [Thread-1  ]: 1 of 1 OK created table model dbt_anomaly_detection.my_first_dbt_model ......... [[32mCREATE TABLE (500.0 rows, 29.4 KB processed)[0m in 2.85s]
[0m17:28:42.879191 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.my_first_dbt_model
[0m17:28:42.880489 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m17:28:42.881036 [info ] [MainThread]: 
[0m17:28:42.881280 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.53 seconds (4.53s).
[0m17:28:42.881460 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:28:42.881553 [debug] [MainThread]: Connection 'model.anomaly_detection.my_first_dbt_model' was properly closed.
[0m17:28:42.895973 [info ] [MainThread]: 
[0m17:28:42.896220 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:28:42.896390 [info ] [MainThread]: 
[0m17:28:42.896521 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m17:28:42.896733 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1021b0550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a50be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a50f40>]}
[0m17:28:42.896889 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 17:28:56.185455 | da24d81c-c5e6-4b54-b62e-4ab96e6d219c ==============================
[0m17:28:56.185460 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:28:56.185673 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m17:28:56.185740 [debug] [MainThread]: Tracking: tracking
[0m17:28:56.192840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108d700d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108d70460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108d70f70>]}
[0m17:28:56.345861 [debug] [MainThread]: Executing "git --help"
[0m17:28:56.351017 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m17:28:56.351559 [debug] [MainThread]: STDERR: "b''"
[0m17:28:56.354791 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m17:28:56.355359 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:28:56.641966 [debug] [MainThread]: On debug: select 1 as id
[0m17:28:57.999159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e59520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2c4310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2c4160>]}
[0m17:28:58.000757 [debug] [MainThread]: Flushing usage events
[0m17:28:58.188464 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-09 17:31:20.697857 | aab09bde-fba5-496d-9323-a31cbbc01ca6 ==============================
[0m17:31:20.697883 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:31:20.698284 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:31:20.698375 [debug] [MainThread]: Tracking: tracking
[0m17:31:20.706796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106447d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106447c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106447c40>]}
[0m17:31:20.747769 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:31:20.747914 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:31:20.751421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'aab09bde-fba5-496d-9323-a31cbbc01ca6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10669e0d0>]}
[0m17:31:20.756737 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'aab09bde-fba5-496d-9323-a31cbbc01ca6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065b16a0>]}
[0m17:31:20.756885 [info ] [MainThread]: Found 1 model, 2 tests, 0 snapshots, 0 analyses, 540 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m17:31:20.757043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aab09bde-fba5-496d-9323-a31cbbc01ca6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106431160>]}
[0m17:31:20.757778 [info ] [MainThread]: 
[0m17:31:20.758065 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m17:31:20.758426 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow"
[0m17:31:20.758513 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:31:21.850143 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow_dbt_anomaly_detection"
[0m17:31:21.850841 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:31:22.225276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aab09bde-fba5-496d-9323-a31cbbc01ca6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106447bb0>]}
[0m17:31:22.225809 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:31:22.226030 [info ] [MainThread]: 
[0m17:31:22.230003 [debug] [Thread-1  ]: Began running node model.anomaly_detection.my_first_dbt_model
[0m17:31:22.230224 [info ] [Thread-1  ]: 1 of 1 START table model dbt_anomaly_detection.my_first_dbt_model .............. [RUN]
[0m17:31:22.230552 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.my_first_dbt_model"
[0m17:31:22.230675 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.my_first_dbt_model
[0m17:31:22.230817 [debug] [Thread-1  ]: Compiling model.anomaly_detection.my_first_dbt_model
[0m17:31:22.233991 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.my_first_dbt_model"
[0m17:31:22.234368 [debug] [Thread-1  ]: finished collecting timing info
[0m17:31:22.234474 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.my_first_dbt_model
[0m17:31:22.245932 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:31:22.688829 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.my_first_dbt_model"
[0m17:31:22.689385 [debug] [Thread-1  ]: On model.anomaly_detection.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.my_first_dbt_model"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`my_first_dbt_model`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



SELECT *
FROM `ld-snowplow`.`dbt_rhashemi`.`aggregations_main`

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
  
[0m17:31:25.007067 [debug] [Thread-1  ]: finished collecting timing info
[0m17:31:25.007834 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aab09bde-fba5-496d-9323-a31cbbc01ca6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10681a310>]}
[0m17:31:25.008194 [info ] [Thread-1  ]: 1 of 1 OK created table model dbt_anomaly_detection.my_first_dbt_model ......... [[32mCREATE TABLE (500.0 rows, 29.4 KB processed)[0m in 2.78s]
[0m17:31:25.008595 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.my_first_dbt_model
[0m17:31:25.009952 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m17:31:25.010326 [info ] [MainThread]: 
[0m17:31:25.010551 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.25 seconds (4.25s).
[0m17:31:25.010732 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:31:25.010827 [debug] [MainThread]: Connection 'model.anomaly_detection.my_first_dbt_model' was properly closed.
[0m17:31:25.019568 [info ] [MainThread]: 
[0m17:31:25.019818 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:31:25.020009 [info ] [MainThread]: 
[0m17:31:25.020156 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m17:31:25.020395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10681a310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106664eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065b1340>]}
[0m17:31:25.020563 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 17:31:30.574524 | f54cb906-8b4e-4a2a-8b71-f54a25ea9371 ==============================
[0m17:31:30.574530 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:31:30.574724 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m17:31:30.574833 [debug] [MainThread]: Tracking: tracking
[0m17:31:30.582067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055b7490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055b7430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055b7520>]}
[0m17:31:30.736392 [debug] [MainThread]: Executing "git --help"
[0m17:31:30.741330 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m17:31:30.741807 [debug] [MainThread]: STDERR: "b''"
[0m17:31:30.744662 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m17:31:30.745264 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:31:31.028864 [debug] [MainThread]: On debug: select 1 as id
[0m17:31:32.204718 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069fc160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069fcbb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069fc9a0>]}
[0m17:31:32.206701 [debug] [MainThread]: Flushing usage events
[0m17:31:32.564805 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-09 21:21:21.560989 | 3d371a47-68e6-47f6-b541-0a57776a0cad ==============================
[0m21:21:21.560995 [info ] [MainThread]: Running with dbt=1.2.1
[0m21:21:21.561327 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m21:21:21.561394 [debug] [MainThread]: Tracking: tracking
[0m21:21:21.577322 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10899f700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10899f280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10899f3d0>]}
[0m21:21:21.799928 [debug] [MainThread]: Executing "git --help"
[0m21:21:21.816593 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m21:21:21.817031 [debug] [MainThread]: STDERR: "b''"
[0m21:21:21.820485 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m21:21:21.820998 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:21:22.685668 [debug] [MainThread]: On debug: select 1 as id
[0m21:21:23.877177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109d19070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109cef760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ceff70>]}
[0m21:21:23.878036 [debug] [MainThread]: Flushing usage events
[0m21:21:24.339107 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-09 21:21:29.644636 | cd6debbc-a6d1-4a33-8466-74effe742ac4 ==============================
[0m21:21:29.644666 [info ] [MainThread]: Running with dbt=1.2.1
[0m21:21:29.645083 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:21:29.645213 [debug] [MainThread]: Tracking: tracking
[0m21:21:29.652175 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d06eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d06b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d06bb0>]}
[0m21:21:29.722516 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 97 files added, 1 files changed.
[0m21:21:29.722732 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_model_features_dbt.yml
[0m21:21:29.722836 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_long.yml
[0m21:21:29.722931 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/anomaly_detection_source.yml
[0m21:21:29.723010 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_events_with_anomalies.sql
[0m21:21:29.723092 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_nonrecent_events.yml
[0m21:21:29.723164 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/reference_derived.sql
[0m21:21:29.723245 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_models.yml
[0m21:21:29.723331 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff.yml
[0m21:21:29.723400 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ref_including_LoBs.sql
[0m21:21:29.723469 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_models.sql
[0m21:21:29.723537 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/count_cutoff.sql
[0m21:21:29.723614 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_12hr.yml
[0m21:21:29.723684 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_short.sql
[0m21:21:29.723770 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies.sql
[0m21:21:29.723837 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features.sql
[0m21:21:29.723915 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_8hr.yml
[0m21:21:29.723994 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_short.yml
[0m21:21:29.724065 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ml_detect.sql
[0m21:21:29.724142 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies_results.yml
[0m21:21:29.724224 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/count_cutoff.yml
[0m21:21:29.724300 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_4hr.yml
[0m21:21:29.724390 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_short.sql
[0m21:21:29.724461 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ref_distinct_tuples.sql
[0m21:21:29.724528 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_12hr.sql
[0m21:21:29.724596 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_24hr.sql
[0m21:21:29.724662 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_long.sql
[0m21:21:29.724737 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_short.yml
[0m21:21:29.724813 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_12hr.yml
[0m21:21:29.724879 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_all_models.sql
[0m21:21:29.724954 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_24hr.yml
[0m21:21:29.725021 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features_null_filtered.sql
[0m21:21:29.725096 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ref_distinct_tuples.yml
[0m21:21:29.725170 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_long.yml
[0m21:21:29.725244 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_all_models.yml
[0m21:21:29.725310 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/dev_ml_model_constraints.sql
[0m21:21:29.725376 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD.sql
[0m21:21:29.725450 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies.yml
[0m21:21:29.725516 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_model_features_dbt.sql
[0m21:21:29.725581 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_8hr.sql
[0m21:21:29.725657 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD_results.yml
[0m21:21:29.725723 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies_results.sql
[0m21:21:29.725798 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_24hr.yml
[0m21:21:29.725866 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_model_features_dbt.sql
[0m21:21:29.725944 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_24hr.yml
[0m21:21:29.726013 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies.sql
[0m21:21:29.726080 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_8hr.sql
[0m21:21:29.726158 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ml_detect.yml
[0m21:21:29.726224 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_24hr.sql
[0m21:21:29.726301 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_short.yml
[0m21:21:29.726368 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_short.sql
[0m21:21:29.726434 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies_results.sql
[0m21:21:29.726501 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_long.sql
[0m21:21:29.726568 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_long.sql
[0m21:21:29.726645 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events.yml
[0m21:21:29.726713 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD.sql
[0m21:21:29.726788 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/dev_ml_model_constraints.yml
[0m21:21:29.726863 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features_null_filtered.yml
[0m21:21:29.726937 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_model_features_dbt.yml
[0m21:21:29.727011 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD_results.yml
[0m21:21:29.727085 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD.yml
[0m21:21:29.727159 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ref_including_LoBs.yml
[0m21:21:29.727225 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_nonrecent_events.sql
[0m21:21:29.727314 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD.yml
[0m21:21:29.727383 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD_results.sql
[0m21:21:29.727458 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_long.yml
[0m21:21:29.727534 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_short.yml
[0m21:21:29.727601 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_24hr.sql
[0m21:21:29.727676 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_8hr.yml
[0m21:21:29.727743 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_alerts_fired_automation_dbt.sql
[0m21:21:29.727810 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_12hr.sql
[0m21:21:29.727877 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_12hr.sql
[0m21:21:29.727954 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_4hr.yml
[0m21:21:29.728031 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_phase.yml
[0m21:21:29.728099 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events.sql
[0m21:21:29.728167 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff.sql
[0m21:21:29.728241 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features.yml
[0m21:21:29.728315 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_events_with_anomalies.yml
[0m21:21:29.728380 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_long.sql
[0m21:21:29.728455 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_8hr.yml
[0m21:21:29.728529 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_12hr.yml
[0m21:21:29.728618 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies_results.yml
[0m21:21:29.728686 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_4hr.sql
[0m21:21:29.728752 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_phase.sql
[0m21:21:29.728827 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_4hr.yml
[0m21:21:29.728894 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD_results.sql
[0m21:21:29.728968 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/reference_derived.yml
[0m21:21:29.729032 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_4hr.sql
[0m21:21:29.729097 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ml_detect_tweaked.sql
[0m21:21:29.729162 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_8hr.sql
[0m21:21:29.729228 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_short.sql
[0m21:21:29.729302 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived.yml
[0m21:21:29.729375 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_long.yml
[0m21:21:29.729440 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived.sql
[0m21:21:29.729512 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ml_detect_tweaked.yml
[0m21:21:29.729577 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_4hr.sql
[0m21:21:29.729650 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_alerts_fired_automation_dbt.yml
[0m21:21:29.729722 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies.yml
[0m21:21:29.729873 [debug] [MainThread]: Partial parsing: deleted source source.anomaly_detection.derived_web_3month.aggregations_main
[0m21:21:29.729940 [debug] [MainThread]: Partial parsing: update schema file: anomaly_detection://models/example/sources.yml
[0m21:21:29.738550 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_events_with_anomalies.sql
[0m21:21:29.744155 [debug] [MainThread]: 1603: static parser failed on example/reference_derived.sql
[0m21:21:29.747661 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/reference_derived.sql
[0m21:21:29.748205 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_ref_including_LoBs.sql
[0m21:21:29.749500 [debug] [MainThread]: 1699: static parser successfully parsed example/all_models.sql
[0m21:21:29.750955 [debug] [MainThread]: 1699: static parser successfully parsed example/count_cutoff.sql
[0m21:21:29.752307 [debug] [MainThread]: 1699: static parser successfully parsed example/all_agg_derived_cutoff_short.sql
[0m21:21:29.753560 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies.sql
[0m21:21:29.754910 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_features.sql
[0m21:21:29.756268 [debug] [MainThread]: 1603: static parser failed on example/derived_ml_detect.sql
[0m21:21:29.762505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f558e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11023fd60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11023ffa0>]}
[0m21:21:29.762655 [debug] [MainThread]: Flushing usage events
[0m21:21:30.116485 [error] [MainThread]: Encountered an error:
'NoneType' object is not iterable
[0m21:21:30.119630 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 349, in load
    self.parse_project(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 473, in parse_project
    parser.parse_file(block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 392, in parse_file
    self.parse_node(file_block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 366, in parse_node
    self.render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/models.py", line 147, in render_update
    super().render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 341, in render_update
    context = self.render_with_context(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 238, in render_with_context
    get_rendered(parsed_node.raw_sql, context, parsed_node, capture_macros=True)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 571, in get_rendered
    return render_template(template, ctx, node)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 526, in render_template
    return template.render(ctx)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 1090, in render
    self.environment.handle_exception()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 832, in handle_exception
    reraise(*rewrite_traceback_stack(source=source))
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/_compat.py", line 28, in reraise
    raise value.with_traceback(tb)
  File "<template>", line 20, in top-level template code
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 403, in __init__
    self._iterator = self._to_iterator(iterable)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 411, in _to_iterator
    return iter(iterable)
TypeError: 'NoneType' object is not iterable



============================== 2023-02-09 21:23:42.250840 | f722883b-a1ad-4779-b8ef-915c38a577d4 ==============================
[0m21:23:42.250877 [info ] [MainThread]: Running with dbt=1.2.1
[0m21:23:42.251401 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:23:42.251498 [debug] [MainThread]: Tracking: tracking
[0m21:23:42.259707 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10906fe20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10906ff10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10906fee0>]}
[0m21:23:42.324390 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 96 files added, 1 files changed.
[0m21:23:42.324619 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies.sql
[0m21:23:42.324731 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff.yml
[0m21:23:42.324811 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_long.sql
[0m21:23:42.324887 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ref_including_LoBs.sql
[0m21:23:42.324958 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_4hr.sql
[0m21:23:42.325036 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD.yml
[0m21:23:42.325106 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features_null_filtered.sql
[0m21:23:42.325184 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies_results.yml
[0m21:23:42.325260 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_all_models.yml
[0m21:23:42.325333 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_nonrecent_events.sql
[0m21:23:42.325402 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_all_models.sql
[0m21:23:42.325479 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_model_features_dbt.yml
[0m21:23:42.325556 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ref_distinct_tuples.yml
[0m21:23:42.325624 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ml_detect.sql
[0m21:23:42.325701 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/reference_derived.yml
[0m21:23:42.325779 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_short.yml
[0m21:23:42.325856 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/dev_ml_model_constraints.yml
[0m21:23:42.325923 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_24hr.sql
[0m21:23:42.325999 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_short.yml
[0m21:23:42.326074 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_events_with_anomalies.yml
[0m21:23:42.326142 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD.sql
[0m21:23:42.326211 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_short.sql
[0m21:23:42.326281 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events.sql
[0m21:23:42.326382 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features.yml
[0m21:23:42.326463 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_12hr.yml
[0m21:23:42.326540 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_long.yml
[0m21:23:42.326618 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/count_cutoff.yml
[0m21:23:42.326695 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_models.yml
[0m21:23:42.326771 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events.yml
[0m21:23:42.326838 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD.sql
[0m21:23:42.326913 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_8hr.yml
[0m21:23:42.326980 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_4hr.sql
[0m21:23:42.327054 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_short.yml
[0m21:23:42.327120 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ref_distinct_tuples.sql
[0m21:23:42.327187 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_8hr.sql
[0m21:23:42.327253 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived.sql
[0m21:23:42.327329 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_long.sql
[0m21:23:42.327396 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/reference_derived.sql
[0m21:23:42.327465 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features.sql
[0m21:23:42.327540 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_4hr.yml
[0m21:23:42.327614 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_8hr.yml
[0m21:23:42.327680 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/dev_ml_model_constraints.sql
[0m21:23:42.327757 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_8hr.yml
[0m21:23:42.327823 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff.sql
[0m21:23:42.327897 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_12hr.yml
[0m21:23:42.327962 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_short.sql
[0m21:23:42.328036 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_4hr.yml
[0m21:23:42.328102 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_12hr.sql
[0m21:23:42.328168 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies_results.sql
[0m21:23:42.328234 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_short.sql
[0m21:23:42.328308 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_long.yml
[0m21:23:42.328374 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_4hr.sql
[0m21:23:42.328447 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ml_detect_tweaked.yml
[0m21:23:42.328520 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_12hr.sql
[0m21:23:42.328586 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_24hr.sql
[0m21:23:42.328652 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_24hr.sql
[0m21:23:42.328717 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies.sql
[0m21:23:42.328791 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD.yml
[0m21:23:42.328873 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_alerts_fired_automation_dbt.yml
[0m21:23:42.328947 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ref_including_LoBs.yml
[0m21:23:42.329014 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ml_detect_tweaked.sql
[0m21:23:42.329089 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_4hr.yml
[0m21:23:42.329155 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_12hr.sql
[0m21:23:42.329221 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_model_features_dbt.sql
[0m21:23:42.329296 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_24hr.yml
[0m21:23:42.329380 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/count_cutoff.sql
[0m21:23:42.329455 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_long.yml
[0m21:23:42.329531 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies.yml
[0m21:23:42.329597 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_events_with_anomalies.sql
[0m21:23:42.329665 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD_results.sql
[0m21:23:42.329739 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived.yml
[0m21:23:42.329814 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_12hr.yml
[0m21:23:42.329888 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD_results.yml
[0m21:23:42.329954 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_8hr.sql
[0m21:23:42.330028 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies_results.yml
[0m21:23:42.330094 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_8hr.sql
[0m21:23:42.330160 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_short.sql
[0m21:23:42.330235 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_short.yml
[0m21:23:42.330309 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_nonrecent_events.yml
[0m21:23:42.330375 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_phase.sql
[0m21:23:42.330448 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_model_features_dbt.yml
[0m21:23:42.330513 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_alerts_fired_automation_dbt.sql
[0m21:23:42.330586 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_24hr.yml
[0m21:23:42.330659 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_phase.yml
[0m21:23:42.330725 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_long.sql
[0m21:23:42.330791 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD_results.sql
[0m21:23:42.330857 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies_results.sql
[0m21:23:42.330931 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies.yml
[0m21:23:42.331005 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ml_detect.yml
[0m21:23:42.331071 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_model_features_dbt.sql
[0m21:23:42.331145 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_long.yml
[0m21:23:42.331211 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_models.sql
[0m21:23:42.331286 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD_results.yml
[0m21:23:42.331360 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_24hr.yml
[0m21:23:42.331426 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_long.sql
[0m21:23:42.331499 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features_null_filtered.yml
[0m21:23:42.331661 [debug] [MainThread]: Partial parsing: deleted source source.anomaly_detection.derived_web_3month.aggregations_main
[0m21:23:42.331725 [debug] [MainThread]: Partial parsing: update schema file: anomaly_detection://models/example/sources.yml
[0m21:23:42.340440 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_min_anomalies.sql
[0m21:23:42.346150 [debug] [MainThread]: 1699: static parser successfully parsed example/all_agg_derived_cutoff_long.sql
[0m21:23:42.347338 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_ref_including_LoBs.sql
[0m21:23:42.348608 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_4hr.sql
[0m21:23:42.353570 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_4hr.sql
[0m21:23:42.354165 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_features_null_filtered.sql
[0m21:23:42.355386 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_nonrecent_events.sql
[0m21:23:42.356932 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_all_models.sql
[0m21:23:42.358487 [debug] [MainThread]: 1603: static parser failed on example/derived_ml_detect.sql
[0m21:23:42.364696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092befa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109399f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1093991f0>]}
[0m21:23:42.364850 [debug] [MainThread]: Flushing usage events
[0m21:23:42.876938 [error] [MainThread]: Encountered an error:
'NoneType' object is not iterable
[0m21:23:42.881023 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 349, in load
    self.parse_project(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 473, in parse_project
    parser.parse_file(block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 392, in parse_file
    self.parse_node(file_block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 366, in parse_node
    self.render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/models.py", line 147, in render_update
    super().render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 341, in render_update
    context = self.render_with_context(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 238, in render_with_context
    get_rendered(parsed_node.raw_sql, context, parsed_node, capture_macros=True)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 571, in get_rendered
    return render_template(template, ctx, node)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 526, in render_template
    return template.render(ctx)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 1090, in render
    self.environment.handle_exception()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 832, in handle_exception
    reraise(*rewrite_traceback_stack(source=source))
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/_compat.py", line 28, in reraise
    raise value.with_traceback(tb)
  File "<template>", line 20, in top-level template code
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 403, in __init__
    self._iterator = self._to_iterator(iterable)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 411, in _to_iterator
    return iter(iterable)
TypeError: 'NoneType' object is not iterable



============================== 2023-02-09 21:23:52.402183 | a43b4de7-b676-4330-b45c-6e187afb1b66 ==============================
[0m21:23:52.402216 [info ] [MainThread]: Running with dbt=1.2.1
[0m21:23:52.402575 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'parse_only': False, 'which': 'compile', 'rpc_method': 'compile', 'indirect_selection': 'eager'}
[0m21:23:52.402669 [debug] [MainThread]: Tracking: tracking
[0m21:23:52.411857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1052cbb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1052cb760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1052cb730>]}
[0m21:23:52.475828 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 96 files added, 1 files changed.
[0m21:23:52.476044 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_24hr.sql
[0m21:23:52.476164 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_24hr.yml
[0m21:23:52.476257 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies_results.yml
[0m21:23:52.476342 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies.sql
[0m21:23:52.476419 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_models.sql
[0m21:23:52.476499 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_short.yml
[0m21:23:52.476578 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_12hr.yml
[0m21:23:52.476658 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_short.yml
[0m21:23:52.476742 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features.yml
[0m21:23:52.476820 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_long.yml
[0m21:23:52.476897 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD_results.yml
[0m21:23:52.476966 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_8hr.sql
[0m21:23:52.477044 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD_results.yml
[0m21:23:52.477120 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies.yml
[0m21:23:52.477197 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_model_features_dbt.yml
[0m21:23:52.477273 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_4hr.yml
[0m21:23:52.477342 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_short.sql
[0m21:23:52.477411 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/count_cutoff.sql
[0m21:23:52.477488 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_12hr.yml
[0m21:23:52.477557 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_8hr.sql
[0m21:23:52.477625 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_phase.sql
[0m21:23:52.477693 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_24hr.sql
[0m21:23:52.477797 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_events_with_anomalies.yml
[0m21:23:52.477870 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_nonrecent_events.sql
[0m21:23:52.477939 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies_results.sql
[0m21:23:52.478016 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_nonrecent_events.yml
[0m21:23:52.478098 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_phase.yml
[0m21:23:52.478175 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ref_including_LoBs.yml
[0m21:23:52.478251 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ml_detect.yml
[0m21:23:52.478318 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_alerts_fired_automation_dbt.sql
[0m21:23:52.478386 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_24hr.sql
[0m21:23:52.478452 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_12hr.sql
[0m21:23:52.478527 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD.yml
[0m21:23:52.478594 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD.sql
[0m21:23:52.478669 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ref_distinct_tuples.yml
[0m21:23:52.478736 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD_results.sql
[0m21:23:52.478803 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_12hr.sql
[0m21:23:52.478877 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/dev_ml_model_constraints.yml
[0m21:23:52.478944 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_all_models.sql
[0m21:23:52.479010 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/dev_ml_model_constraints.sql
[0m21:23:52.479084 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_24hr.yml
[0m21:23:52.479151 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived.sql
[0m21:23:52.479216 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_long.sql
[0m21:23:52.479290 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_long.yml
[0m21:23:52.479356 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_4hr.sql
[0m21:23:52.479433 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_all_models.yml
[0m21:23:52.479509 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_12hr.yml
[0m21:23:52.479585 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ml_detect_tweaked.yml
[0m21:23:52.479659 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_4hr.yml
[0m21:23:52.479725 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_long.sql
[0m21:23:52.479791 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ref_including_LoBs.sql
[0m21:23:52.479866 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events.yml
[0m21:23:52.479940 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD.yml
[0m21:23:52.480008 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_4hr.sql
[0m21:23:52.480083 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/count_cutoff.yml
[0m21:23:52.480152 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff.sql
[0m21:23:52.480218 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_model_features_dbt.sql
[0m21:23:52.480284 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events.sql
[0m21:23:52.480349 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/reference_derived.sql
[0m21:23:52.480415 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD_results.sql
[0m21:23:52.480489 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_4hr.yml
[0m21:23:52.480564 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies_results.yml
[0m21:23:52.480630 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_model_features_dbt.sql
[0m21:23:52.480696 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies.sql
[0m21:23:52.480776 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ml_detect.sql
[0m21:23:52.480845 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_long.sql
[0m21:23:52.480911 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies_results.sql
[0m21:23:52.480979 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features_null_filtered.sql
[0m21:23:52.481061 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived.yml
[0m21:23:52.481138 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_models.yml
[0m21:23:52.481205 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_short.sql
[0m21:23:52.481280 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_24hr.yml
[0m21:23:52.481355 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff.yml
[0m21:23:52.481429 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_8hr.yml
[0m21:23:52.481496 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD.sql
[0m21:23:52.481569 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_8hr.yml
[0m21:23:52.481634 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ref_distinct_tuples.sql
[0m21:23:52.481700 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features.sql
[0m21:23:52.481774 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_short.yml
[0m21:23:52.481848 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_8hr.yml
[0m21:23:52.481914 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_long.sql
[0m21:23:52.481988 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies.yml
[0m21:23:52.482055 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_short.sql
[0m21:23:52.482131 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_short.yml
[0m21:23:52.482197 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_12hr.sql
[0m21:23:52.482263 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_short.sql
[0m21:23:52.482328 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_8hr.sql
[0m21:23:52.482394 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_events_with_anomalies.sql
[0m21:23:52.482469 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_long.yml
[0m21:23:52.482544 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_model_features_dbt.yml
[0m21:23:52.482610 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_4hr.sql
[0m21:23:52.482677 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ml_detect_tweaked.sql
[0m21:23:52.482751 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/reference_derived.yml
[0m21:23:52.482828 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_long.yml
[0m21:23:52.482902 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_alerts_fired_automation_dbt.yml
[0m21:23:52.482977 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features_null_filtered.yml
[0m21:23:52.483123 [debug] [MainThread]: Partial parsing: deleted source source.anomaly_detection.derived_web_3month.aggregations_main
[0m21:23:52.483188 [debug] [MainThread]: Partial parsing: update schema file: anomaly_detection://models/example/sources.yml
[0m21:23:52.490061 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_24hr.sql
[0m21:23:52.499178 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_24hr.sql
[0m21:23:52.499895 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_min_anomalies.sql
[0m21:23:52.501149 [debug] [MainThread]: 1699: static parser successfully parsed example/all_models.sql
[0m21:23:52.502291 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_8hr.sql
[0m21:23:52.506888 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_8hr.sql
[0m21:23:52.507447 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_short.sql
[0m21:23:52.508566 [debug] [MainThread]: 1699: static parser successfully parsed example/count_cutoff.sql
[0m21:23:52.510123 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_8hr.sql
[0m21:23:52.515003 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_8hr.sql
[0m21:23:52.515559 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_phase.sql
[0m21:23:52.516962 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_24hr.sql
[0m21:23:52.521688 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_24hr.sql
[0m21:23:52.522234 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_nonrecent_events.sql
[0m21:23:52.523357 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies_results.sql
[0m21:23:52.524644 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_alerts_fired_automation_dbt.sql
[0m21:23:52.525926 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_24hr.sql
[0m21:23:52.530594 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_24hr.sql
[0m21:23:52.531138 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_12hr.sql
[0m21:23:52.536000 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_12hr.sql
[0m21:23:52.536602 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_max_RMSD.sql
[0m21:23:52.537908 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_max_RMSD_results.sql
[0m21:23:52.539026 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_12hr.sql
[0m21:23:52.543535 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_12hr.sql
[0m21:23:52.544115 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_all_models.sql
[0m21:23:52.545400 [debug] [MainThread]: 1699: static parser successfully parsed example/dev_ml_model_constraints.sql
[0m21:23:52.547847 [debug] [MainThread]: 1603: static parser failed on example/all_agg_derived.sql
[0m21:23:52.550781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105518280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055eaf70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055ea1f0>]}
[0m21:23:52.550923 [debug] [MainThread]: Flushing usage events
[0m21:23:52.925691 [error] [MainThread]: Encountered an error:
'NoneType' object is not iterable
[0m21:23:52.928048 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 349, in load
    self.parse_project(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 473, in parse_project
    parser.parse_file(block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 392, in parse_file
    self.parse_node(file_block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 366, in parse_node
    self.render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/models.py", line 147, in render_update
    super().render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 341, in render_update
    context = self.render_with_context(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 238, in render_with_context
    get_rendered(parsed_node.raw_sql, context, parsed_node, capture_macros=True)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 571, in get_rendered
    return render_template(template, ctx, node)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 526, in render_template
    return template.render(ctx)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 1090, in render
    self.environment.handle_exception()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 832, in handle_exception
    reraise(*rewrite_traceback_stack(source=source))
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/_compat.py", line 28, in reraise
    raise value.with_traceback(tb)
  File "<template>", line 3, in top-level template code
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 403, in __init__
    self._iterator = self._to_iterator(iterable)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 411, in _to_iterator
    return iter(iterable)
TypeError: 'NoneType' object is not iterable



============================== 2023-02-09 21:25:09.769813 | 006c55ed-c97e-4cb0-b68c-ea2c80b718b9 ==============================
[0m21:25:09.769851 [info ] [MainThread]: Running with dbt=1.2.1
[0m21:25:09.770378 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:25:09.770484 [debug] [MainThread]: Tracking: tracking
[0m21:25:09.777567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f85ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f856d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f856a0>]}
[0m21:25:09.842898 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 96 files added, 1 files changed.
[0m21:25:09.843167 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived.yml
[0m21:25:09.843274 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies_results.yml
[0m21:25:09.843357 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_24hr.sql
[0m21:25:09.843437 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_8hr.sql
[0m21:25:09.843511 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_models.sql
[0m21:25:09.843582 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features_null_filtered.sql
[0m21:25:09.843653 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived.sql
[0m21:25:09.843722 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_long.sql
[0m21:25:09.843793 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_short.sql
[0m21:25:09.843864 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/reference_derived.sql
[0m21:25:09.843943 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies_results.yml
[0m21:25:09.844022 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies.yml
[0m21:25:09.844099 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_events_with_anomalies.yml
[0m21:25:09.844168 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_model_features_dbt.sql
[0m21:25:09.844236 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_24hr.sql
[0m21:25:09.844304 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_4hr.sql
[0m21:25:09.844380 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_short.yml
[0m21:25:09.844448 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_short.sql
[0m21:25:09.844515 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_events_with_anomalies.sql
[0m21:25:09.844582 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_8hr.sql
[0m21:25:09.844648 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_12hr.sql
[0m21:25:09.844715 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_model_features_dbt.sql
[0m21:25:09.844796 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/dev_ml_model_constraints.yml
[0m21:25:09.844865 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_8hr.sql
[0m21:25:09.844941 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_long.yml
[0m21:25:09.845041 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_models.yml
[0m21:25:09.845111 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_all_models.sql
[0m21:25:09.845188 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features.yml
[0m21:25:09.845265 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_24hr.yml
[0m21:25:09.845332 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features.sql
[0m21:25:09.845399 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/dev_ml_model_constraints.sql
[0m21:25:09.845475 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_4hr.yml
[0m21:25:09.845551 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_8hr.yml
[0m21:25:09.845626 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_nonrecent_events.yml
[0m21:25:09.845693 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies.sql
[0m21:25:09.845769 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_long.yml
[0m21:25:09.845844 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_short.yml
[0m21:25:09.845912 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_12hr.sql
[0m21:25:09.845978 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_long.sql
[0m21:25:09.846044 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD.sql
[0m21:25:09.846119 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_model_features_dbt.yml
[0m21:25:09.846187 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff.sql
[0m21:25:09.846253 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_4hr.sql
[0m21:25:09.846333 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD.yml
[0m21:25:09.846409 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_24hr.yml
[0m21:25:09.846483 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_short.yml
[0m21:25:09.846550 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_24hr.sql
[0m21:25:09.846624 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff.yml
[0m21:25:09.846699 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_phase.yml
[0m21:25:09.846766 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_long.sql
[0m21:25:09.846832 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ref_distinct_tuples.sql
[0m21:25:09.846897 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_phase.sql
[0m21:25:09.846963 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_short.sql
[0m21:25:09.847037 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_24hr.yml
[0m21:25:09.847112 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD_results.yml
[0m21:25:09.847186 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features_null_filtered.yml
[0m21:25:09.847252 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD.sql
[0m21:25:09.847326 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events.yml
[0m21:25:09.847401 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ref_distinct_tuples.yml
[0m21:25:09.847467 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD_results.sql
[0m21:25:09.847542 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_short.yml
[0m21:25:09.847609 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ml_detect_tweaked.sql
[0m21:25:09.847676 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies.sql
[0m21:25:09.847743 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ref_including_LoBs.sql
[0m21:25:09.847810 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies_results.sql
[0m21:25:09.847884 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_long.yml
[0m21:25:09.847966 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_long.sql
[0m21:25:09.848046 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_4hr.yml
[0m21:25:09.848113 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_short.sql
[0m21:25:09.848189 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_12hr.yml
[0m21:25:09.848264 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_12hr.yml
[0m21:25:09.848332 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD_results.sql
[0m21:25:09.848406 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/reference_derived.yml
[0m21:25:09.848481 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_8hr.yml
[0m21:25:09.848556 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_long.yml
[0m21:25:09.848631 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/count_cutoff.yml
[0m21:25:09.848705 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_alerts_fired_automation_dbt.yml
[0m21:25:09.848771 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ml_detect.sql
[0m21:25:09.848846 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ml_detect.yml
[0m21:25:09.848920 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD_results.yml
[0m21:25:09.848986 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/count_cutoff.sql
[0m21:25:09.849060 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_4hr.yml
[0m21:25:09.849125 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_12hr.sql
[0m21:25:09.849200 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD.yml
[0m21:25:09.849275 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ref_including_LoBs.yml
[0m21:25:09.849341 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_alerts_fired_automation_dbt.sql
[0m21:25:09.849408 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events.sql
[0m21:25:09.849482 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ml_detect_tweaked.yml
[0m21:25:09.849548 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies_results.sql
[0m21:25:09.849622 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_all_models.yml
[0m21:25:09.849696 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies.yml
[0m21:25:09.849770 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_model_features_dbt.yml
[0m21:25:09.849846 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_8hr.yml
[0m21:25:09.849915 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_nonrecent_events.sql
[0m21:25:09.849988 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_12hr.yml
[0m21:25:09.850054 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_4hr.sql
[0m21:25:09.850218 [debug] [MainThread]: Partial parsing: deleted source source.anomaly_detection.derived_web_3month.aggregations_main
[0m21:25:09.850283 [debug] [MainThread]: Partial parsing: update schema file: anomaly_detection://models/example/sources.yml
[0m21:25:09.858545 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_24hr.sql
[0m21:25:09.867787 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_24hr.sql
[0m21:25:09.868367 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_8hr.sql
[0m21:25:09.872840 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_8hr.sql
[0m21:25:09.873819 [debug] [MainThread]: 1699: static parser successfully parsed example/all_models.sql
[0m21:25:09.875050 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_features_null_filtered.sql
[0m21:25:09.876145 [debug] [MainThread]: 1603: static parser failed on example/all_agg_derived.sql
[0m21:25:09.878862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061d41f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062a3d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062aff70>]}
[0m21:25:09.879025 [debug] [MainThread]: Flushing usage events
[0m21:25:10.321665 [error] [MainThread]: Encountered an error:
'NoneType' object is not iterable
[0m21:25:10.324266 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 349, in load
    self.parse_project(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 473, in parse_project
    parser.parse_file(block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 392, in parse_file
    self.parse_node(file_block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 366, in parse_node
    self.render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/models.py", line 147, in render_update
    super().render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 341, in render_update
    context = self.render_with_context(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 238, in render_with_context
    get_rendered(parsed_node.raw_sql, context, parsed_node, capture_macros=True)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 571, in get_rendered
    return render_template(template, ctx, node)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 526, in render_template
    return template.render(ctx)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 1090, in render
    self.environment.handle_exception()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 832, in handle_exception
    reraise(*rewrite_traceback_stack(source=source))
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/_compat.py", line 28, in reraise
    raise value.with_traceback(tb)
  File "<template>", line 3, in top-level template code
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 403, in __init__
    self._iterator = self._to_iterator(iterable)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 411, in _to_iterator
    return iter(iterable)
TypeError: 'NoneType' object is not iterable



============================== 2023-02-09 21:25:19.264136 | b7f4b9bc-053e-4880-bff8-ed87aa5e347f ==============================
[0m21:25:19.264169 [info ] [MainThread]: Running with dbt=1.2.1
[0m21:25:19.264530 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'parse_only': False, 'which': 'compile', 'rpc_method': 'compile', 'indirect_selection': 'eager'}
[0m21:25:19.264626 [debug] [MainThread]: Tracking: tracking
[0m21:25:19.272132 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112785b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112785730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112785700>]}
[0m21:25:19.326269 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 96 files added, 1 files changed.
[0m21:25:19.326484 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies_results.yml
[0m21:25:19.326592 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies_results.yml
[0m21:25:19.326683 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_short.yml
[0m21:25:19.326765 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_model_features_dbt.yml
[0m21:25:19.326845 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_4hr.sql
[0m21:25:19.326932 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_long.yml
[0m21:25:19.327010 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_24hr.yml
[0m21:25:19.327088 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_8hr.yml
[0m21:25:19.327166 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies.yml
[0m21:25:19.327244 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_nonrecent_events.yml
[0m21:25:19.327323 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_12hr.yml
[0m21:25:19.327392 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived.sql
[0m21:25:19.327461 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features.sql
[0m21:25:19.327538 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD_results.yml
[0m21:25:19.327614 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_4hr.yml
[0m21:25:19.327682 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD_results.sql
[0m21:25:19.327757 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD.yml
[0m21:25:19.327833 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_short.yml
[0m21:25:19.327908 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_24hr.yml
[0m21:25:19.327985 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived.yml
[0m21:25:19.328085 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features_null_filtered.yml
[0m21:25:19.328169 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_24hr.yml
[0m21:25:19.328241 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_events_with_anomalies.sql
[0m21:25:19.328311 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features_null_filtered.sql
[0m21:25:19.328391 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_phase.yml
[0m21:25:19.328459 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_model_features_dbt.sql
[0m21:25:19.328527 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_all_models.sql
[0m21:25:19.328595 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ref_including_LoBs.sql
[0m21:25:19.328663 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/dev_ml_model_constraints.sql
[0m21:25:19.328733 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_12hr.sql
[0m21:25:19.328803 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff.sql
[0m21:25:19.328868 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_models.sql
[0m21:25:19.328935 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_nonrecent_events.sql
[0m21:25:19.329001 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_12hr.sql
[0m21:25:19.329078 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_8hr.yml
[0m21:25:19.329154 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_model_features_dbt.yml
[0m21:25:19.329231 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_alerts_fired_automation_dbt.yml
[0m21:25:19.329306 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ml_detect_tweaked.yml
[0m21:25:19.329382 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_all_models.yml
[0m21:25:19.329458 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_4hr.yml
[0m21:25:19.329525 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_phase.sql
[0m21:25:19.329593 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_8hr.sql
[0m21:25:19.329663 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ml_detect.sql
[0m21:25:19.329734 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_long.sql
[0m21:25:19.329810 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ref_distinct_tuples.yml
[0m21:25:19.329886 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_models.yml
[0m21:25:19.329962 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies.yml
[0m21:25:19.330037 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_long.yml
[0m21:25:19.330114 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_8hr.sql
[0m21:25:19.330194 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_12hr.yml
[0m21:25:19.330265 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies_results.sql
[0m21:25:19.330341 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/dev_ml_model_constraints.yml
[0m21:25:19.330415 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD_results.yml
[0m21:25:19.330489 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/count_cutoff.yml
[0m21:25:19.330556 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_24hr.sql
[0m21:25:19.330631 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ml_detect.yml
[0m21:25:19.330698 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_long.sql
[0m21:25:19.330765 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD.sql
[0m21:25:19.330841 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ref_including_LoBs.yml
[0m21:25:19.330916 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events.yml
[0m21:25:19.330982 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD.sql
[0m21:25:19.331048 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_short.sql
[0m21:25:19.331128 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_long.sql
[0m21:25:19.331204 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_8hr.yml
[0m21:25:19.331275 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_model_features_dbt.sql
[0m21:25:19.331350 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features.yml
[0m21:25:19.331416 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_4hr.sql
[0m21:25:19.331492 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_long.yml
[0m21:25:19.331581 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff.yml
[0m21:25:19.331651 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events.sql
[0m21:25:19.331737 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD.yml
[0m21:25:19.331812 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_12hr.yml
[0m21:25:19.331888 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_short.sql
[0m21:25:19.331955 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_long.sql
[0m21:25:19.332035 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_short.sql
[0m21:25:19.332101 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ref_distinct_tuples.sql
[0m21:25:19.332176 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_short.yml
[0m21:25:19.332242 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_4hr.sql
[0m21:25:19.332310 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies.sql
[0m21:25:19.332386 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_long.yml
[0m21:25:19.332453 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_24hr.sql
[0m21:25:19.332526 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/count_cutoff.sql
[0m21:25:19.332596 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_12hr.sql
[0m21:25:19.332662 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies.sql
[0m21:25:19.332731 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies_results.sql
[0m21:25:19.332798 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_short.sql
[0m21:25:19.332866 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/reference_derived.sql
[0m21:25:19.332932 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD_results.sql
[0m21:25:19.332999 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_24hr.sql
[0m21:25:19.333064 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_alerts_fired_automation_dbt.sql
[0m21:25:19.333131 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_8hr.sql
[0m21:25:19.333206 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_short.yml
[0m21:25:19.333283 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/reference_derived.yml
[0m21:25:19.333350 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ml_detect_tweaked.sql
[0m21:25:19.333425 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_4hr.yml
[0m21:25:19.333500 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_events_with_anomalies.yml
[0m21:25:19.333654 [debug] [MainThread]: Partial parsing: deleted source source.anomaly_detection.derived_web_3month.aggregations_main
[0m21:25:19.333719 [debug] [MainThread]: Partial parsing: update schema file: anomaly_detection://models/example/sources.yml
[0m21:25:19.340531 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_4hr.sql
[0m21:25:19.349731 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_4hr.sql
[0m21:25:19.350300 [debug] [MainThread]: 1603: static parser failed on example/all_agg_derived.sql
[0m21:25:19.352648 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112bdc3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c86f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c934f0>]}
[0m21:25:19.352773 [debug] [MainThread]: Flushing usage events
[0m21:25:19.702692 [error] [MainThread]: Encountered an error:
'NoneType' object is not iterable
[0m21:25:19.706315 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 349, in load
    self.parse_project(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 473, in parse_project
    parser.parse_file(block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 392, in parse_file
    self.parse_node(file_block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 366, in parse_node
    self.render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/models.py", line 147, in render_update
    super().render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 341, in render_update
    context = self.render_with_context(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 238, in render_with_context
    get_rendered(parsed_node.raw_sql, context, parsed_node, capture_macros=True)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 571, in get_rendered
    return render_template(template, ctx, node)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 526, in render_template
    return template.render(ctx)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 1090, in render
    self.environment.handle_exception()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 832, in handle_exception
    reraise(*rewrite_traceback_stack(source=source))
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/_compat.py", line 28, in reraise
    raise value.with_traceback(tb)
  File "<template>", line 3, in top-level template code
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 403, in __init__
    self._iterator = self._to_iterator(iterable)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 411, in _to_iterator
    return iter(iterable)
TypeError: 'NoneType' object is not iterable



============================== 2023-02-09 21:25:56.523218 | c62d8a3c-6096-416d-a7e0-d2a1d5d6ebc7 ==============================
[0m21:25:56.523260 [info ] [MainThread]: Running with dbt=1.2.1
[0m21:25:56.523802 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'parse_only': False, 'which': 'compile', 'rpc_method': 'compile', 'indirect_selection': 'eager'}
[0m21:25:56.523902 [debug] [MainThread]: Tracking: tracking
[0m21:25:56.531545 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f18ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f186d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f186a0>]}
[0m21:25:56.596790 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 96 files added, 1 files changed.
[0m21:25:56.597010 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_short.yml
[0m21:25:56.597109 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_alerts_fired_automation_dbt.sql
[0m21:25:56.597191 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_short.sql
[0m21:25:56.597275 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/reference_derived.yml
[0m21:25:56.597355 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies_results.yml
[0m21:25:56.597425 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/count_cutoff.sql
[0m21:25:56.597495 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_long.sql
[0m21:25:56.597563 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_models.sql
[0m21:25:56.597631 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/dev_ml_model_constraints.sql
[0m21:25:56.597708 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_long.yml
[0m21:25:56.597785 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features.yml
[0m21:25:56.597861 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ml_detect.yml
[0m21:25:56.597930 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_short.sql
[0m21:25:56.598006 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_model_features_dbt.yml
[0m21:25:56.598072 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ml_detect.sql
[0m21:25:56.598147 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_8hr.yml
[0m21:25:56.598214 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies_results.sql
[0m21:25:56.598291 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_model_features_dbt.yml
[0m21:25:56.598366 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features_null_filtered.yml
[0m21:25:56.598442 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_12hr.yml
[0m21:25:56.598516 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_24hr.yml
[0m21:25:56.598582 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD.sql
[0m21:25:56.598659 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ref_including_LoBs.yml
[0m21:25:56.598759 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived.yml
[0m21:25:56.598838 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_4hr.yml
[0m21:25:56.598906 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_4hr.sql
[0m21:25:56.598983 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD_results.yml
[0m21:25:56.599058 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events.yml
[0m21:25:56.599133 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_short.yml
[0m21:25:56.599208 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_models.yml
[0m21:25:56.599275 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ml_detect_tweaked.sql
[0m21:25:56.599342 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_8hr.sql
[0m21:25:56.599408 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_model_features_dbt.sql
[0m21:25:56.599483 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies.yml
[0m21:25:56.599550 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_model_features_dbt.sql
[0m21:25:56.599616 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features_null_filtered.sql
[0m21:25:56.599683 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_long.sql
[0m21:25:56.599759 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD.yml
[0m21:25:56.599830 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_24hr.sql
[0m21:25:56.599906 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies.yml
[0m21:25:56.599974 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_12hr.sql
[0m21:25:56.600051 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_8hr.yml
[0m21:25:56.600119 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_phase.sql
[0m21:25:56.600195 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_events_with_anomalies.yml
[0m21:25:56.600265 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_events_with_anomalies.sql
[0m21:25:56.600339 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_8hr.yml
[0m21:25:56.600415 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_12hr.yml
[0m21:25:56.600491 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_all_models.yml
[0m21:25:56.600564 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/dev_ml_model_constraints.yml
[0m21:25:56.600629 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_long.sql
[0m21:25:56.600702 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD.yml
[0m21:25:56.600768 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features.sql
[0m21:25:56.600842 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ref_distinct_tuples.yml
[0m21:25:56.600908 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/reference_derived.sql
[0m21:25:56.600974 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_short.sql
[0m21:25:56.601039 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_4hr.sql
[0m21:25:56.601104 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_long.sql
[0m21:25:56.601169 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_4hr.sql
[0m21:25:56.601242 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies_results.yml
[0m21:25:56.601318 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_12hr.yml
[0m21:25:56.601397 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_long.yml
[0m21:25:56.601471 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff.yml
[0m21:25:56.601537 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_24hr.sql
[0m21:25:56.601603 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ref_distinct_tuples.sql
[0m21:25:56.601681 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events.sql
[0m21:25:56.601748 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_8hr.sql
[0m21:25:56.601814 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_12hr.sql
[0m21:25:56.601881 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies.sql
[0m21:25:56.601956 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_short.yml
[0m21:25:56.602024 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies.sql
[0m21:25:56.602090 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD_results.sql
[0m21:25:56.602159 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_nonrecent_events.sql
[0m21:25:56.602224 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived.sql
[0m21:25:56.602298 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_24hr.yml
[0m21:25:56.602372 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_4hr.yml
[0m21:25:56.602438 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ref_including_LoBs.sql
[0m21:25:56.602504 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_8hr.sql
[0m21:25:56.602578 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_24hr.yml
[0m21:25:56.602644 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff.sql
[0m21:25:56.602718 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_nonrecent_events.yml
[0m21:25:56.602783 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD_results.sql
[0m21:25:56.602856 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_alerts_fired_automation_dbt.yml
[0m21:25:56.602922 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_all_models.sql
[0m21:25:56.602995 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_long.yml
[0m21:25:56.603069 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD_results.yml
[0m21:25:56.603135 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies_results.sql
[0m21:25:56.603208 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_short.yml
[0m21:25:56.603282 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_long.yml
[0m21:25:56.603348 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_24hr.sql
[0m21:25:56.603413 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_short.sql
[0m21:25:56.603478 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD.sql
[0m21:25:56.603551 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/count_cutoff.yml
[0m21:25:56.603616 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_12hr.sql
[0m21:25:56.603690 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_phase.yml
[0m21:25:56.603765 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ml_detect_tweaked.yml
[0m21:25:56.603839 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_4hr.yml
[0m21:25:56.603954 [debug] [MainThread]: Partial parsing: deleted source source.anomaly_detection.derived_web_3month.aggregations_main
[0m21:25:56.604020 [debug] [MainThread]: Partial parsing: update schema file: anomaly_detection://models/example/sources.yml
[0m21:25:56.604083 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/my_first_dbt_model.sql
[0m21:25:56.612878 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_alerts_fired_automation_dbt.sql
[0m21:25:56.618690 [debug] [MainThread]: 1699: static parser successfully parsed example/all_agg_derived_cutoff_short.sql
[0m21:25:56.619904 [debug] [MainThread]: 1699: static parser successfully parsed example/count_cutoff.sql
[0m21:25:56.621115 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_long.sql
[0m21:25:56.622345 [debug] [MainThread]: 1699: static parser successfully parsed example/all_models.sql
[0m21:25:56.623560 [debug] [MainThread]: 1699: static parser successfully parsed example/dev_ml_model_constraints.sql
[0m21:25:56.625029 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_bounds_short.sql
[0m21:25:56.626647 [debug] [MainThread]: 1603: static parser failed on example/derived_ml_detect.sql
[0m21:25:56.633450 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061683d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106231940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106231b80>]}
[0m21:25:56.633606 [debug] [MainThread]: Flushing usage events
[0m21:25:56.804377 [error] [MainThread]: Encountered an error:
'NoneType' object is not iterable
[0m21:25:56.805597 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 349, in load
    self.parse_project(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 473, in parse_project
    parser.parse_file(block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 392, in parse_file
    self.parse_node(file_block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 366, in parse_node
    self.render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/models.py", line 147, in render_update
    super().render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 341, in render_update
    context = self.render_with_context(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 238, in render_with_context
    get_rendered(parsed_node.raw_sql, context, parsed_node, capture_macros=True)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 571, in get_rendered
    return render_template(template, ctx, node)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 526, in render_template
    return template.render(ctx)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 1090, in render
    self.environment.handle_exception()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 832, in handle_exception
    reraise(*rewrite_traceback_stack(source=source))
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/_compat.py", line 28, in reraise
    raise value.with_traceback(tb)
  File "<template>", line 20, in top-level template code
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 403, in __init__
    self._iterator = self._to_iterator(iterable)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 411, in _to_iterator
    return iter(iterable)
TypeError: 'NoneType' object is not iterable



============================== 2023-02-09 21:26:02.585134 | fcb15b5f-3b7e-4fdc-a6f7-343d1f1e2d7f ==============================
[0m21:26:02.585163 [info ] [MainThread]: Running with dbt=1.2.1
[0m21:26:02.585597 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:26:02.585681 [debug] [MainThread]: Tracking: tracking
[0m21:26:02.593349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10609dd00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10609dd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10609ddc0>]}
[0m21:26:02.666769 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 96 files added, 1 files changed.
[0m21:26:02.667089 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_long.yml
[0m21:26:02.667206 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies_results.yml
[0m21:26:02.667299 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ml_detect.yml
[0m21:26:02.667383 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features.yml
[0m21:26:02.667466 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_4hr.yml
[0m21:26:02.667548 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_24hr.yml
[0m21:26:02.667628 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_long.sql
[0m21:26:02.667711 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies.yml
[0m21:26:02.667793 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies.yml
[0m21:26:02.667875 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_4hr.yml
[0m21:26:02.667949 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_alerts_fired_automation_dbt.sql
[0m21:26:02.668029 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ref_distinct_tuples.yml
[0m21:26:02.668098 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_all_models.sql
[0m21:26:02.668167 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_nonrecent_events.sql
[0m21:26:02.668247 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_12hr.yml
[0m21:26:02.668325 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_alerts_fired_automation_dbt.yml
[0m21:26:02.668402 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD.yml
[0m21:26:02.668479 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff.yml
[0m21:26:02.668557 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD_results.yml
[0m21:26:02.668625 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_model_features_dbt.sql
[0m21:26:02.668693 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD_results.sql
[0m21:26:02.668764 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ref_including_LoBs.sql
[0m21:26:02.668867 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD_results.sql
[0m21:26:02.668939 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_short.sql
[0m21:26:02.669018 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_events_with_anomalies.yml
[0m21:26:02.669092 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_short.sql
[0m21:26:02.669169 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_long.yml
[0m21:26:02.669252 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD_results.yml
[0m21:26:02.669330 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_model_features_dbt.yml
[0m21:26:02.669399 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_long.sql
[0m21:26:02.669475 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_8hr.yml
[0m21:26:02.669552 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_12hr.yml
[0m21:26:02.669621 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_24hr.sql
[0m21:26:02.669691 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived.sql
[0m21:26:02.669772 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_phase.yml
[0m21:26:02.669842 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_short.sql
[0m21:26:02.669911 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/reference_derived.sql
[0m21:26:02.670006 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events.yml
[0m21:26:02.670074 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_24hr.sql
[0m21:26:02.670141 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ml_detect.sql
[0m21:26:02.670224 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_model_features_dbt.sql
[0m21:26:02.670293 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD.sql
[0m21:26:02.670360 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/dev_ml_model_constraints.sql
[0m21:26:02.670426 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_12hr.sql
[0m21:26:02.670511 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_short.yml
[0m21:26:02.670587 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/reference_derived.yml
[0m21:26:02.670654 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_long.sql
[0m21:26:02.670732 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/dev_ml_model_constraints.yml
[0m21:26:02.670800 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD.sql
[0m21:26:02.670867 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_models.sql
[0m21:26:02.670933 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_4hr.sql
[0m21:26:02.670999 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_short.sql
[0m21:26:02.671065 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ref_distinct_tuples.sql
[0m21:26:02.671134 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ml_detect_tweaked.sql
[0m21:26:02.671209 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_24hr.yml
[0m21:26:02.671287 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_8hr.yml
[0m21:26:02.671369 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived.yml
[0m21:26:02.671443 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features.sql
[0m21:26:02.671521 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ref_including_LoBs.yml
[0m21:26:02.671600 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_short.yml
[0m21:26:02.671670 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_phase.sql
[0m21:26:02.671749 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ml_detect_tweaked.yml
[0m21:26:02.671826 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies_results.yml
[0m21:26:02.671895 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_24hr.sql
[0m21:26:02.671991 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_short.yml
[0m21:26:02.672061 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies_results.sql
[0m21:26:02.672140 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_models.yml
[0m21:26:02.672216 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_8hr.yml
[0m21:26:02.672290 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_12hr.yml
[0m21:26:02.672356 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/count_cutoff.sql
[0m21:26:02.672422 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_4hr.sql
[0m21:26:02.672490 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_12hr.sql
[0m21:26:02.672568 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_4hr.yml
[0m21:26:02.672636 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_4hr.sql
[0m21:26:02.672702 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff.sql
[0m21:26:02.672770 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_8hr.sql
[0m21:26:02.672836 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_events_with_anomalies.sql
[0m21:26:02.672915 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_all_models.yml
[0m21:26:02.672990 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/count_cutoff.yml
[0m21:26:02.673065 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_model_features_dbt.yml
[0m21:26:02.673139 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_24hr.yml
[0m21:26:02.673205 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_8hr.sql
[0m21:26:02.673271 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies.sql
[0m21:26:02.673337 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies_results.sql
[0m21:26:02.673404 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features_null_filtered.sql
[0m21:26:02.673478 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_nonrecent_events.yml
[0m21:26:02.673545 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_8hr.sql
[0m21:26:02.673621 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features_null_filtered.yml
[0m21:26:02.673697 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD.yml
[0m21:26:02.673764 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_long.sql
[0m21:26:02.673831 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_12hr.sql
[0m21:26:02.673899 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies.sql
[0m21:26:02.673976 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_long.yml
[0m21:26:02.674044 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events.sql
[0m21:26:02.674121 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_long.yml
[0m21:26:02.674199 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_short.yml
[0m21:26:02.674328 [debug] [MainThread]: Partial parsing: deleted source source.anomaly_detection.derived_web_3month.aggregations_main
[0m21:26:02.674400 [debug] [MainThread]: Partial parsing: update schema file: anomaly_detection://models/example/sources.yml
[0m21:26:02.674474 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/my_first_dbt_model.sql
[0m21:26:02.682267 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_bounds_long.sql
[0m21:26:02.688344 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_alerts_fired_automation_dbt.sql
[0m21:26:02.689560 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_all_models.sql
[0m21:26:02.690649 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_nonrecent_events.sql
[0m21:26:02.692264 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_model_features_dbt.sql
[0m21:26:02.697818 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_RMSD_results.sql
[0m21:26:02.699125 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_ref_including_LoBs.sql
[0m21:26:02.700337 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_max_RMSD_results.sql
[0m21:26:02.701423 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_short.sql
[0m21:26:02.702953 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_quartiles_short.sql
[0m21:26:02.704093 [debug] [MainThread]: 1699: static parser successfully parsed example/all_agg_derived_cutoff_long.sql
[0m21:26:02.705491 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_24hr.sql
[0m21:26:02.710834 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_24hr.sql
[0m21:26:02.711428 [debug] [MainThread]: 1603: static parser failed on example/all_agg_derived.sql
[0m21:26:02.714970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106314250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1063ddf70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1063dd1f0>]}
[0m21:26:02.715200 [debug] [MainThread]: Flushing usage events
[0m21:26:03.061974 [error] [MainThread]: Encountered an error:
'NoneType' object is not iterable
[0m21:26:03.063300 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 349, in load
    self.parse_project(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 473, in parse_project
    parser.parse_file(block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 392, in parse_file
    self.parse_node(file_block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 366, in parse_node
    self.render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/models.py", line 147, in render_update
    super().render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 341, in render_update
    context = self.render_with_context(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 238, in render_with_context
    get_rendered(parsed_node.raw_sql, context, parsed_node, capture_macros=True)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 571, in get_rendered
    return render_template(template, ctx, node)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 526, in render_template
    return template.render(ctx)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 1090, in render
    self.environment.handle_exception()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 832, in handle_exception
    reraise(*rewrite_traceback_stack(source=source))
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/_compat.py", line 28, in reraise
    raise value.with_traceback(tb)
  File "<template>", line 3, in top-level template code
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 403, in __init__
    self._iterator = self._to_iterator(iterable)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 411, in _to_iterator
    return iter(iterable)
TypeError: 'NoneType' object is not iterable



============================== 2023-02-09 21:34:46.338328 | 04483eca-ab44-4a33-ae80-fce8d0f7b5de ==============================
[0m21:34:46.338346 [info ] [MainThread]: Running with dbt=1.2.1
[0m21:34:46.339214 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'parse_only': False, 'which': 'compile', 'rpc_method': 'compile', 'indirect_selection': 'eager'}
[0m21:34:46.339318 [debug] [MainThread]: Tracking: tracking
[0m21:34:46.347784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059212e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105921d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105921a60>]}
[0m21:34:46.421458 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 96 files added, 1 files changed.
[0m21:34:46.421752 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD.yml
[0m21:34:46.421858 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_24hr.sql
[0m21:34:46.421962 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD.yml
[0m21:34:46.422041 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies_results.sql
[0m21:34:46.422114 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_12hr.sql
[0m21:34:46.422183 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_4hr.sql
[0m21:34:46.422252 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff.sql
[0m21:34:46.422329 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_alerts_fired_automation_dbt.yml
[0m21:34:46.422407 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/count_cutoff.yml
[0m21:34:46.422483 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies.yml
[0m21:34:46.422552 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_24hr.sql
[0m21:34:46.422628 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_8hr.yml
[0m21:34:46.422697 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_short.sql
[0m21:34:46.422764 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/dev_ml_model_constraints.sql
[0m21:34:46.422830 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies.sql
[0m21:34:46.422906 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ref_distinct_tuples.yml
[0m21:34:46.422980 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ml_detect.yml
[0m21:34:46.423046 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_4hr.sql
[0m21:34:46.423120 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_12hr.yml
[0m21:34:46.423196 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD_results.yml
[0m21:34:46.423270 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_24hr.yml
[0m21:34:46.423344 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/reference_derived.yml
[0m21:34:46.423414 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_12hr.sql
[0m21:34:46.423517 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_short.yml
[0m21:34:46.423586 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_all_models.sql
[0m21:34:46.423652 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/count_cutoff.sql
[0m21:34:46.423727 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived.yml
[0m21:34:46.423801 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_models.yml
[0m21:34:46.423867 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events.sql
[0m21:34:46.423932 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD.sql
[0m21:34:46.423996 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ref_including_LoBs.sql
[0m21:34:46.424061 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ref_distinct_tuples.sql
[0m21:34:46.424134 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_short.yml
[0m21:34:46.424209 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_events_with_anomalies.yml
[0m21:34:46.424275 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_8hr.sql
[0m21:34:46.424349 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_long.yml
[0m21:34:46.424423 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events.yml
[0m21:34:46.424489 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_long.sql
[0m21:34:46.424562 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ml_detect_tweaked.yml
[0m21:34:46.424627 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_long.sql
[0m21:34:46.424691 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD_results.sql
[0m21:34:46.424755 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_4hr.sql
[0m21:34:46.424829 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features_null_filtered.yml
[0m21:34:46.424905 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_long.yml
[0m21:34:46.424970 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_24hr.sql
[0m21:34:46.425034 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_model_features_dbt.sql
[0m21:34:46.425098 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_8hr.sql
[0m21:34:46.425173 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_model_features_dbt.yml
[0m21:34:46.425238 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies_results.sql
[0m21:34:46.425311 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ref_including_LoBs.yml
[0m21:34:46.425377 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived.sql
[0m21:34:46.425441 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_short.sql
[0m21:34:46.425506 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ml_detect.sql
[0m21:34:46.425570 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features_null_filtered.sql
[0m21:34:46.425635 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_8hr.sql
[0m21:34:46.425708 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features.yml
[0m21:34:46.425782 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD_results.yml
[0m21:34:46.425848 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_models.sql
[0m21:34:46.425922 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_12hr.yml
[0m21:34:46.425992 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD_results.sql
[0m21:34:46.426070 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_short.yml
[0m21:34:46.426137 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ml_detect_tweaked.sql
[0m21:34:46.426216 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_8hr.yml
[0m21:34:46.426294 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies_results.yml
[0m21:34:46.426373 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_4hr.yml
[0m21:34:46.426443 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/reference_derived.sql
[0m21:34:46.426531 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_model_features_dbt.sql
[0m21:34:46.426602 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies.sql
[0m21:34:46.426682 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_all_models.yml
[0m21:34:46.426751 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_alerts_fired_automation_dbt.sql
[0m21:34:46.426829 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_model_features_dbt.yml
[0m21:34:46.426906 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_long.yml
[0m21:34:46.426982 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_long.yml
[0m21:34:46.427050 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_short.sql
[0m21:34:46.427118 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_nonrecent_events.sql
[0m21:34:46.427194 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies_results.yml
[0m21:34:46.427270 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_phase.yml
[0m21:34:46.427346 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_12hr.yml
[0m21:34:46.427416 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD.sql
[0m21:34:46.427484 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_long.sql
[0m21:34:46.427551 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_long.sql
[0m21:34:46.427618 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features.sql
[0m21:34:46.427697 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_4hr.yml
[0m21:34:46.427767 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_phase.sql
[0m21:34:46.427841 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff.yml
[0m21:34:46.427905 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_events_with_anomalies.sql
[0m21:34:46.427970 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_12hr.sql
[0m21:34:46.428043 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_nonrecent_events.yml
[0m21:34:46.428117 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_8hr.yml
[0m21:34:46.428190 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_4hr.yml
[0m21:34:46.428263 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_short.yml
[0m21:34:46.428336 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_24hr.yml
[0m21:34:46.428412 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies.yml
[0m21:34:46.428486 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/dev_ml_model_constraints.yml
[0m21:34:46.428558 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_24hr.yml
[0m21:34:46.428626 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_short.sql
[0m21:34:46.428747 [debug] [MainThread]: Partial parsing: deleted source source.anomaly_detection.derived_web_3month.aggregations_main
[0m21:34:46.428814 [debug] [MainThread]: Partial parsing: update schema file: anomaly_detection://models/example/sources.yml
[0m21:34:46.428883 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/my_first_dbt_model.sql
[0m21:34:46.436768 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_24hr.sql
[0m21:34:46.446796 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_24hr.sql
[0m21:34:46.447941 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies_results.sql
[0m21:34:46.449267 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_12hr.sql
[0m21:34:46.454484 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_12hr.sql
[0m21:34:46.455140 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_4hr.sql
[0m21:34:46.459973 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_4hr.sql
[0m21:34:46.460811 [debug] [MainThread]: 1699: static parser successfully parsed example/all_agg_derived_cutoff.sql
[0m21:34:46.462343 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_24hr.sql
[0m21:34:46.467255 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_24hr.sql
[0m21:34:46.467881 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_short.sql
[0m21:34:46.469571 [debug] [MainThread]: 1699: static parser successfully parsed example/dev_ml_model_constraints.sql
[0m21:34:46.471513 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_min_anomalies.sql
[0m21:34:46.473072 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_4hr.sql
[0m21:34:46.477760 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_4hr.sql
[0m21:34:46.478362 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_12hr.sql
[0m21:34:46.483777 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_12hr.sql
[0m21:34:46.484514 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_all_models.sql
[0m21:34:46.485756 [debug] [MainThread]: 1699: static parser successfully parsed example/count_cutoff.sql
[0m21:34:46.486936 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events.sql
[0m21:34:46.488213 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_max_RMSD.sql
[0m21:34:46.489645 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_ref_including_LoBs.sql
[0m21:34:46.490838 [debug] [MainThread]: 1699: static parser successfully parsed example/ref_distinct_tuples.sql
[0m21:34:46.491989 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_8hr.sql
[0m21:34:46.496846 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_8hr.sql
[0m21:34:46.497441 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_bounds_long.sql
[0m21:34:46.498671 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_quartiles_long.sql
[0m21:34:46.499924 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_RMSD_results.sql
[0m21:34:46.501323 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_4hr.sql
[0m21:34:46.506030 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_4hr.sql
[0m21:34:46.506569 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_24hr.sql
[0m21:34:46.511265 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_24hr.sql
[0m21:34:46.511917 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_model_features_dbt.sql
[0m21:34:46.513099 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_8hr.sql
[0m21:34:46.518008 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_8hr.sql
[0m21:34:46.518608 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_min_anomalies_results.sql
[0m21:34:46.519705 [debug] [MainThread]: 1603: static parser failed on example/all_agg_derived.sql
[0m21:34:46.553350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b94700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10585b5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105903040>]}
[0m21:34:46.553547 [debug] [MainThread]: Flushing usage events
[0m21:34:47.047182 [error] [MainThread]: Encountered an error:
'NoneType' object is not iterable
[0m21:34:47.049282 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 349, in load
    self.parse_project(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 473, in parse_project
    parser.parse_file(block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 392, in parse_file
    self.parse_node(file_block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 366, in parse_node
    self.render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/models.py", line 147, in render_update
    super().render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 341, in render_update
    context = self.render_with_context(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 238, in render_with_context
    get_rendered(parsed_node.raw_sql, context, parsed_node, capture_macros=True)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 571, in get_rendered
    return render_template(template, ctx, node)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 526, in render_template
    return template.render(ctx)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 1090, in render
    self.environment.handle_exception()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 832, in handle_exception
    reraise(*rewrite_traceback_stack(source=source))
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/_compat.py", line 28, in reraise
    raise value.with_traceback(tb)
  File "<template>", line 3, in top-level template code
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 403, in __init__
    self._iterator = self._to_iterator(iterable)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 411, in _to_iterator
    return iter(iterable)
TypeError: 'NoneType' object is not iterable



============================== 2023-02-09 21:35:29.697430 | b7037e5c-0b40-475c-a5d3-8c93f3383c0e ==============================
[0m21:35:29.697457 [info ] [MainThread]: Running with dbt=1.2.1
[0m21:35:29.697972 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m21:35:29.698075 [debug] [MainThread]: Tracking: tracking
[0m21:35:29.705280 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107520460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107520040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107520ca0>]}
[0m21:35:29.772806 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 96 files added, 1 files changed.
[0m21:35:29.773056 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_phase.yml
[0m21:35:29.773172 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_12hr.sql
[0m21:35:29.773266 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_short.yml
[0m21:35:29.773358 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_8hr.yml
[0m21:35:29.773442 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_4hr.yml
[0m21:35:29.773517 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_models.sql
[0m21:35:29.773596 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies.yml
[0m21:35:29.773677 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_all_models.yml
[0m21:35:29.773756 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ref_including_LoBs.yml
[0m21:35:29.773826 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_4hr.sql
[0m21:35:29.773896 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_long.sql
[0m21:35:29.773976 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/dev_ml_model_constraints.yml
[0m21:35:29.774058 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_4hr.yml
[0m21:35:29.774129 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies_results.sql
[0m21:35:29.774208 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_long.yml
[0m21:35:29.774289 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_12hr.yml
[0m21:35:29.774376 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_4hr.yml
[0m21:35:29.774460 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_alerts_fired_automation_dbt.yml
[0m21:35:29.774544 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_nonrecent_events.yml
[0m21:35:29.774616 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived.sql
[0m21:35:29.774694 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies.sql
[0m21:35:29.774771 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_model_features_dbt.sql
[0m21:35:29.774846 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_4hr.sql
[0m21:35:29.774949 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events.yml
[0m21:35:29.775019 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_24hr.sql
[0m21:35:29.775099 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD_results.yml
[0m21:35:29.775167 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff.sql
[0m21:35:29.775254 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_24hr.yml
[0m21:35:29.775328 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/dev_ml_model_constraints.sql
[0m21:35:29.775408 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_model_features_dbt.yml
[0m21:35:29.775478 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_4hr.sql
[0m21:35:29.775557 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_long.yml
[0m21:35:29.775624 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_nonrecent_events.sql
[0m21:35:29.775708 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_events_with_anomalies.yml
[0m21:35:29.775779 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_quartiles_short.sql
[0m21:35:29.775849 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_model_features_dbt.sql
[0m21:35:29.775928 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_8hr.yml
[0m21:35:29.776001 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ref_distinct_tuples.sql
[0m21:35:29.776070 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_phase.sql
[0m21:35:29.776149 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ml_detect_tweaked.yml
[0m21:35:29.776215 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_12hr.sql
[0m21:35:29.776289 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_8hr.sql
[0m21:35:29.776372 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features.yml
[0m21:35:29.776443 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD.sql
[0m21:35:29.776521 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_12hr.sql
[0m21:35:29.776592 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD.sql
[0m21:35:29.776660 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_all_models.sql
[0m21:35:29.776732 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_long.sql
[0m21:35:29.776802 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/count_cutoff.sql
[0m21:35:29.776882 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_12hr.yml
[0m21:35:29.776953 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ml_detect.sql
[0m21:35:29.777030 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ml_detect.yml
[0m21:35:29.777108 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ref_distinct_tuples.yml
[0m21:35:29.777174 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_long.sql
[0m21:35:29.777252 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_8hr.yml
[0m21:35:29.777336 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies.yml
[0m21:35:29.777418 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD_results.yml
[0m21:35:29.777486 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_1mon_24hr.sql
[0m21:35:29.777568 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_12hr.yml
[0m21:35:29.777647 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies_results.yml
[0m21:35:29.777719 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/reference_derived.sql
[0m21:35:29.777785 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies_results.sql
[0m21:35:29.777863 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_24hr.yml
[0m21:35:29.777940 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_models.yml
[0m21:35:29.778033 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/filtered_model_features_dbt.yml
[0m21:35:29.778114 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD.yml
[0m21:35:29.778189 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff.yml
[0m21:35:29.778266 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_24hr.yml
[0m21:35:29.778347 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_long.yml
[0m21:35:29.778430 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_min_anomalies_results.yml
[0m21:35:29.778501 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_events_with_anomalies.sql
[0m21:35:29.778571 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features.sql
[0m21:35:29.778652 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/reference_derived.yml
[0m21:35:29.778734 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD_results.sql
[0m21:35:29.778812 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_short.yml
[0m21:35:29.778893 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/count_cutoff.yml
[0m21:35:29.778962 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_24hr.sql
[0m21:35:29.779033 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_short.sql
[0m21:35:29.779098 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events.sql
[0m21:35:29.779164 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_alerts_fired_automation_dbt.sql
[0m21:35:29.779232 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_RMSD_results.sql
[0m21:35:29.779310 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features_null_filtered.yml
[0m21:35:29.779382 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_2mon_8hr.sql
[0m21:35:29.779460 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/trade_off_max_RMSD.yml
[0m21:35:29.779529 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_short.sql
[0m21:35:29.779610 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_short.yml
[0m21:35:29.779677 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_min_anomalies.sql
[0m21:35:29.779746 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_long.sql
[0m21:35:29.779827 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_bounds_long.yml
[0m21:35:29.779896 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived_cutoff_short.sql
[0m21:35:29.779971 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/all_agg_derived.yml
[0m21:35:29.780037 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/remaining_events_features_null_filtered.sql
[0m21:35:29.780120 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/ml_detect_tweaked.sql
[0m21:35:29.780190 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_models_05mon_8hr.sql
[0m21:35:29.780261 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/derived_ref_including_LoBs.sql
[0m21:35:29.780343 [debug] [MainThread]: Partial parsing: added file: anomaly_detection://models/example/aggregation_outliers_short.yml
[0m21:35:29.780461 [debug] [MainThread]: Partial parsing: deleted source source.anomaly_detection.derived_web_3month.aggregations_main
[0m21:35:29.780527 [debug] [MainThread]: Partial parsing: update schema file: anomaly_detection://models/example/sources.yml
[0m21:35:29.780596 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/my_first_dbt_model.sql
[0m21:35:29.788019 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_12hr.sql
[0m21:35:29.797663 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_12hr.sql
[0m21:35:29.798873 [debug] [MainThread]: 1699: static parser successfully parsed example/all_models.sql
[0m21:35:29.800187 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_4hr.sql
[0m21:35:29.804930 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_4hr.sql
[0m21:35:29.805460 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_quartiles_long.sql
[0m21:35:29.806613 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies_results.sql
[0m21:35:29.808008 [debug] [MainThread]: 1603: static parser failed on example/all_agg_derived.sql
[0m21:35:29.811075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10776ef10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10784ef70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10784e1f0>]}
[0m21:35:29.811248 [debug] [MainThread]: Flushing usage events
[0m21:35:30.161661 [error] [MainThread]: Encountered an error:
'NoneType' object is not iterable
[0m21:35:30.164630 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 349, in load
    self.parse_project(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 473, in parse_project
    parser.parse_file(block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 392, in parse_file
    self.parse_node(file_block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 366, in parse_node
    self.render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/models.py", line 147, in render_update
    super().render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 341, in render_update
    context = self.render_with_context(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 238, in render_with_context
    get_rendered(parsed_node.raw_sql, context, parsed_node, capture_macros=True)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 571, in get_rendered
    return render_template(template, ctx, node)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 526, in render_template
    return template.render(ctx)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 1090, in render
    self.environment.handle_exception()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 832, in handle_exception
    reraise(*rewrite_traceback_stack(source=source))
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/_compat.py", line 28, in reraise
    raise value.with_traceback(tb)
  File "<template>", line 3, in top-level template code
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 403, in __init__
    self._iterator = self._to_iterator(iterable)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 411, in _to_iterator
    return iter(iterable)
TypeError: 'NoneType' object is not iterable



============================== 2023-02-09 21:36:25.651667 | e35c54bf-88f3-4eb3-a38b-20edfb0542d7 ==============================
[0m21:36:25.651710 [info ] [MainThread]: Running with dbt=1.2.1
[0m21:36:25.652358 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m21:36:25.652475 [debug] [MainThread]: Tracking: tracking
[0m21:36:25.659365 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10507a610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10507a040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10507ad30>]}
[0m21:36:25.675325 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m21:36:25.675504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'e35c54bf-88f3-4eb3-a38b-20edfb0542d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105078af0>]}
[0m21:36:25.719599 [debug] [MainThread]: Parsing macros/etc.sql
[0m21:36:25.721008 [debug] [MainThread]: Parsing macros/catalog.sql
[0m21:36:25.724462 [debug] [MainThread]: Parsing macros/adapters.sql
[0m21:36:25.736011 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m21:36:25.737135 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m21:36:25.738381 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m21:36:25.740487 [debug] [MainThread]: Parsing macros/materializations/copy.sql
[0m21:36:25.741707 [debug] [MainThread]: Parsing macros/materializations/incremental.sql
[0m21:36:25.749360 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m21:36:25.750232 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m21:36:25.750412 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m21:36:25.750731 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m21:36:25.750900 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m21:36:25.751178 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m21:36:25.751469 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m21:36:25.751915 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m21:36:25.752477 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m21:36:25.752701 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m21:36:25.752928 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m21:36:25.753165 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m21:36:25.753364 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m21:36:25.754052 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m21:36:25.754331 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m21:36:25.755904 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m21:36:25.757665 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m21:36:25.758737 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m21:36:25.759507 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m21:36:25.767634 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m21:36:25.774030 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m21:36:25.780228 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m21:36:25.782400 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m21:36:25.783244 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m21:36:25.784065 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m21:36:25.786106 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m21:36:25.794409 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m21:36:25.795097 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m21:36:25.799576 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m21:36:25.807199 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m21:36:25.809862 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m21:36:25.811242 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m21:36:25.813965 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m21:36:25.814610 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m21:36:25.816137 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m21:36:25.817171 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m21:36:25.820507 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m21:36:25.829870 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m21:36:25.830595 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m21:36:25.831907 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m21:36:25.832771 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m21:36:25.833240 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m21:36:25.833656 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m21:36:25.833990 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m21:36:25.834635 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m21:36:25.836686 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m21:36:25.840851 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m21:36:25.841285 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m21:36:25.841842 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m21:36:25.842288 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m21:36:25.842710 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m21:36:25.843266 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m21:36:25.843630 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m21:36:25.844088 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m21:36:25.844582 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m21:36:25.845726 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m21:36:25.846285 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m21:36:25.846768 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m21:36:25.847235 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m21:36:25.847689 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m21:36:25.848096 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m21:36:25.848560 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m21:36:25.848952 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m21:36:25.851638 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m21:36:25.852072 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m21:36:25.852862 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m21:36:25.853802 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m21:36:25.854266 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m21:36:25.855287 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m21:36:25.856475 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m21:36:25.863899 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m21:36:25.865338 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m21:36:25.871885 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m21:36:25.873968 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m21:36:25.877329 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m21:36:25.881933 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m21:36:25.883288 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
[0m21:36:25.883741 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
[0m21:36:25.884296 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
[0m21:36:25.884720 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
[0m21:36:25.887308 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
[0m21:36:25.887804 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_array_to_string.sql
[0m21:36:25.889005 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
[0m21:36:25.889548 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
[0m21:36:25.890833 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
[0m21:36:25.891277 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
[0m21:36:25.892054 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
[0m21:36:25.893118 [debug] [MainThread]: Parsing macros/cross_db_utils/listagg.sql
[0m21:36:25.897279 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
[0m21:36:25.902001 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
[0m21:36:25.902792 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
[0m21:36:25.903390 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
[0m21:36:25.904004 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
[0m21:36:25.904713 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
[0m21:36:25.905249 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
[0m21:36:25.905925 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
[0m21:36:25.906443 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
[0m21:36:25.908003 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
[0m21:36:25.910601 [debug] [MainThread]: Parsing macros/cross_db_utils/array_concat.sql
[0m21:36:25.911417 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
[0m21:36:25.912151 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
[0m21:36:25.913714 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
[0m21:36:25.916627 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
[0m21:36:25.917276 [debug] [MainThread]: Parsing macros/cross_db_utils/array_construct.sql
[0m21:36:25.918513 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
[0m21:36:25.919471 [debug] [MainThread]: Parsing macros/cross_db_utils/array_append.sql
[0m21:36:25.920570 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
[0m21:36:25.933148 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
[0m21:36:25.934044 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
[0m21:36:25.935236 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
[0m21:36:25.935946 [debug] [MainThread]: Parsing macros/generic_tests/fewer_rows_than.sql
[0m21:36:25.936770 [debug] [MainThread]: Parsing macros/generic_tests/equal_rowcount.sql
[0m21:36:25.937524 [debug] [MainThread]: Parsing macros/generic_tests/relationships_where.sql
[0m21:36:25.938569 [debug] [MainThread]: Parsing macros/generic_tests/recency.sql
[0m21:36:25.939381 [debug] [MainThread]: Parsing macros/generic_tests/not_constant.sql
[0m21:36:25.939950 [debug] [MainThread]: Parsing macros/generic_tests/accepted_range.sql
[0m21:36:25.941299 [debug] [MainThread]: Parsing macros/generic_tests/not_accepted_values.sql
[0m21:36:25.942422 [debug] [MainThread]: Parsing macros/generic_tests/test_unique_where.sql
[0m21:36:25.943077 [debug] [MainThread]: Parsing macros/generic_tests/at_least_one.sql
[0m21:36:25.943654 [debug] [MainThread]: Parsing macros/generic_tests/unique_combination_of_columns.sql
[0m21:36:25.944941 [debug] [MainThread]: Parsing macros/generic_tests/cardinality_equality.sql
[0m21:36:25.945888 [debug] [MainThread]: Parsing macros/generic_tests/expression_is_true.sql
[0m21:36:25.946711 [debug] [MainThread]: Parsing macros/generic_tests/not_null_proportion.sql
[0m21:36:25.947747 [debug] [MainThread]: Parsing macros/generic_tests/sequential_values.sql
[0m21:36:25.949305 [debug] [MainThread]: Parsing macros/generic_tests/test_not_null_where.sql
[0m21:36:25.949924 [debug] [MainThread]: Parsing macros/generic_tests/equality.sql
[0m21:36:25.951581 [debug] [MainThread]: Parsing macros/generic_tests/mutually_exclusive_ranges.sql
[0m21:36:25.956091 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
[0m21:36:25.956701 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
[0m21:36:25.957310 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
[0m21:36:25.957824 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
[0m21:36:25.958368 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
[0m21:36:25.960351 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
[0m21:36:25.961167 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
[0m21:36:25.962888 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
[0m21:36:25.965016 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
[0m21:36:25.966586 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
[0m21:36:25.967349 [debug] [MainThread]: Parsing macros/sql/star.sql
[0m21:36:25.969088 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
[0m21:36:25.972966 [debug] [MainThread]: Parsing macros/sql/union.sql
[0m21:36:25.978547 [debug] [MainThread]: Parsing macros/sql/groupby.sql
[0m21:36:25.979194 [debug] [MainThread]: Parsing macros/sql/deduplicate.sql
[0m21:36:25.982537 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
[0m21:36:25.984110 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
[0m21:36:25.984829 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
[0m21:36:25.985542 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
[0m21:36:25.988758 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
[0m21:36:25.991634 [debug] [MainThread]: Parsing macros/sql/pivot.sql
[0m21:36:25.993582 [debug] [MainThread]: Parsing macros/sql/get_filtered_columns_in_relation.sql
[0m21:36:25.994811 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
[0m21:36:25.995854 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
[0m21:36:25.996808 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
[0m21:36:26.000783 [debug] [MainThread]: Parsing macros/predict.sql
[0m21:36:26.001212 [debug] [MainThread]: Parsing macros/recommend.sql
[0m21:36:26.001590 [debug] [MainThread]: Parsing macros/detect_anomalies.sql
[0m21:36:26.001895 [debug] [MainThread]: Parsing macros/hparam.sql
[0m21:36:26.002341 [debug] [MainThread]: Parsing macros/materializations/model.sql
[0m21:36:26.008160 [debug] [MainThread]: Parsing macros/hooks/model_audit.sql
[0m21:36:26.246736 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_4hr.sql
[0m21:36:26.256263 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_4hr.sql
[0m21:36:26.257492 [debug] [MainThread]: 1699: static parser successfully parsed example/ml_detect_tweaked.sql
[0m21:36:26.258790 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_12hr.sql
[0m21:36:26.264086 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_12hr.sql
[0m21:36:26.264846 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_max_RMSD_results.sql
[0m21:36:26.266290 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_8hr.sql
[0m21:36:26.271295 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_8hr.sql
[0m21:36:26.271882 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_8hr.sql
[0m21:36:26.276680 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_8hr.sql
[0m21:36:26.277333 [debug] [MainThread]: 1699: static parser successfully parsed example/count_cutoff.sql
[0m21:36:26.278489 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_features.sql
[0m21:36:26.279719 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_24hr.sql
[0m21:36:26.284337 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_24hr.sql
[0m21:36:26.285001 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_model_features_dbt.sql
[0m21:36:26.286196 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_min_anomalies_results.sql
[0m21:36:26.287475 [debug] [MainThread]: 1699: static parser successfully parsed example/dev_ml_model_constraints.sql
[0m21:36:26.289335 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_long.sql
[0m21:36:26.290450 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_model_features_dbt.sql
[0m21:36:26.291691 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_quartiles_short.sql
[0m21:36:26.292729 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_events_with_anomalies.sql
[0m21:36:26.293916 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_alerts_fired_automation_dbt.sql
[0m21:36:26.295181 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_all_models.sql
[0m21:36:26.296314 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies_results.sql
[0m21:36:26.297508 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies.sql
[0m21:36:26.298727 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_12hr.sql
[0m21:36:26.303962 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_12hr.sql
[0m21:36:26.304488 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_bounds_short.sql
[0m21:36:26.305730 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_short.sql
[0m21:36:26.306989 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_RMSD_results.sql
[0m21:36:26.308143 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_min_anomalies.sql
[0m21:36:26.309607 [debug] [MainThread]: 1699: static parser successfully parsed example/reference_derived.sql
[0m21:36:26.310696 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_bounds_long.sql
[0m21:36:26.311750 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_phase.sql
[0m21:36:26.312845 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_24hr.sql
[0m21:36:26.317506 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_24hr.sql
[0m21:36:26.318187 [debug] [MainThread]: 1603: static parser failed on example/derived_ml_detect.sql
[0m21:36:26.324461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105147640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105338a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105338ca0>]}
[0m21:36:26.324646 [debug] [MainThread]: Flushing usage events
[0m21:36:26.821591 [error] [MainThread]: Encountered an error:
'NoneType' object is not iterable
[0m21:36:26.824046 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 349, in load
    self.parse_project(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 473, in parse_project
    parser.parse_file(block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 392, in parse_file
    self.parse_node(file_block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 366, in parse_node
    self.render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/models.py", line 147, in render_update
    super().render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 341, in render_update
    context = self.render_with_context(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 238, in render_with_context
    get_rendered(parsed_node.raw_sql, context, parsed_node, capture_macros=True)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 571, in get_rendered
    return render_template(template, ctx, node)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 526, in render_template
    return template.render(ctx)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 1090, in render
    self.environment.handle_exception()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 832, in handle_exception
    reraise(*rewrite_traceback_stack(source=source))
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/_compat.py", line 28, in reraise
    raise value.with_traceback(tb)
  File "<template>", line 20, in top-level template code
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 403, in __init__
    self._iterator = self._to_iterator(iterable)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 411, in _to_iterator
    return iter(iterable)
TypeError: 'NoneType' object is not iterable



============================== 2023-02-09 21:36:31.593563 | ada27b5a-a6a1-4a97-8534-44177663bb85 ==============================
[0m21:36:31.593593 [info ] [MainThread]: Running with dbt=1.2.1
[0m21:36:31.594069 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'parse_only': False, 'which': 'compile', 'rpc_method': 'compile', 'indirect_selection': 'eager'}
[0m21:36:31.594170 [debug] [MainThread]: Tracking: tracking
[0m21:36:31.601967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a804490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a804d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a804eb0>]}
[0m21:36:31.616902 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m21:36:31.617107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'ada27b5a-a6a1-4a97-8534-44177663bb85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a83e5b0>]}
[0m21:36:31.649849 [debug] [MainThread]: Parsing macros/etc.sql
[0m21:36:31.651085 [debug] [MainThread]: Parsing macros/catalog.sql
[0m21:36:31.654664 [debug] [MainThread]: Parsing macros/adapters.sql
[0m21:36:31.666603 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m21:36:31.667777 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m21:36:31.669038 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m21:36:31.671136 [debug] [MainThread]: Parsing macros/materializations/copy.sql
[0m21:36:31.672585 [debug] [MainThread]: Parsing macros/materializations/incremental.sql
[0m21:36:31.680339 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m21:36:31.681247 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m21:36:31.681457 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m21:36:31.681784 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m21:36:31.681978 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m21:36:31.682286 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m21:36:31.682596 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m21:36:31.683052 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m21:36:31.683627 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m21:36:31.683868 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m21:36:31.684104 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m21:36:31.684356 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m21:36:31.684560 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m21:36:31.685184 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m21:36:31.685440 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m21:36:31.687032 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m21:36:31.688862 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m21:36:31.690019 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m21:36:31.690818 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m21:36:31.699110 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m21:36:31.705682 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m21:36:31.711500 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m21:36:31.713730 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m21:36:31.714641 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m21:36:31.715444 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m21:36:31.717580 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m21:36:31.725843 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m21:36:31.726752 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m21:36:31.731273 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m21:36:31.738907 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m21:36:31.741526 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m21:36:31.742796 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m21:36:31.745304 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m21:36:31.745876 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m21:36:31.747381 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m21:36:31.748363 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m21:36:31.751881 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m21:36:31.761028 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m21:36:31.761767 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m21:36:31.762898 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m21:36:31.763596 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m21:36:31.764068 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m21:36:31.764439 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m21:36:31.764741 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m21:36:31.765360 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m21:36:31.767403 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m21:36:31.771410 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m21:36:31.771784 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m21:36:31.772318 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m21:36:31.772735 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m21:36:31.773137 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m21:36:31.773685 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m21:36:31.774041 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m21:36:31.774509 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m21:36:31.775005 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m21:36:31.776141 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m21:36:31.776699 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m21:36:31.777188 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m21:36:31.777652 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m21:36:31.778120 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m21:36:31.778520 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m21:36:31.778987 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m21:36:31.779377 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m21:36:31.782058 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m21:36:31.782475 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m21:36:31.783292 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m21:36:31.784240 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m21:36:31.784708 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m21:36:31.785720 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m21:36:31.786941 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m21:36:31.793778 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m21:36:31.795067 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m21:36:31.801467 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m21:36:31.803471 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m21:36:31.806715 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m21:36:31.811273 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m21:36:31.812642 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
[0m21:36:31.813103 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
[0m21:36:31.813713 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
[0m21:36:31.814151 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
[0m21:36:31.816759 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
[0m21:36:31.817290 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_array_to_string.sql
[0m21:36:31.818516 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
[0m21:36:31.819127 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
[0m21:36:31.820387 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
[0m21:36:31.820924 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
[0m21:36:31.821773 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
[0m21:36:31.822941 [debug] [MainThread]: Parsing macros/cross_db_utils/listagg.sql
[0m21:36:31.827131 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
[0m21:36:31.831963 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
[0m21:36:31.832825 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
[0m21:36:31.833488 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
[0m21:36:31.834141 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
[0m21:36:31.834913 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
[0m21:36:31.835508 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
[0m21:36:31.836305 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
[0m21:36:31.836984 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
[0m21:36:31.838569 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
[0m21:36:31.841113 [debug] [MainThread]: Parsing macros/cross_db_utils/array_concat.sql
[0m21:36:31.841941 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
[0m21:36:31.842674 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
[0m21:36:31.844222 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
[0m21:36:31.847022 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
[0m21:36:31.847656 [debug] [MainThread]: Parsing macros/cross_db_utils/array_construct.sql
[0m21:36:31.848838 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
[0m21:36:31.850019 [debug] [MainThread]: Parsing macros/cross_db_utils/array_append.sql
[0m21:36:31.851228 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
[0m21:36:31.863635 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
[0m21:36:31.864616 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
[0m21:36:31.865813 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
[0m21:36:31.866524 [debug] [MainThread]: Parsing macros/generic_tests/fewer_rows_than.sql
[0m21:36:31.867329 [debug] [MainThread]: Parsing macros/generic_tests/equal_rowcount.sql
[0m21:36:31.868072 [debug] [MainThread]: Parsing macros/generic_tests/relationships_where.sql
[0m21:36:31.869108 [debug] [MainThread]: Parsing macros/generic_tests/recency.sql
[0m21:36:31.869889 [debug] [MainThread]: Parsing macros/generic_tests/not_constant.sql
[0m21:36:31.870442 [debug] [MainThread]: Parsing macros/generic_tests/accepted_range.sql
[0m21:36:31.871620 [debug] [MainThread]: Parsing macros/generic_tests/not_accepted_values.sql
[0m21:36:31.872645 [debug] [MainThread]: Parsing macros/generic_tests/test_unique_where.sql
[0m21:36:31.873294 [debug] [MainThread]: Parsing macros/generic_tests/at_least_one.sql
[0m21:36:31.873857 [debug] [MainThread]: Parsing macros/generic_tests/unique_combination_of_columns.sql
[0m21:36:31.875157 [debug] [MainThread]: Parsing macros/generic_tests/cardinality_equality.sql
[0m21:36:31.876106 [debug] [MainThread]: Parsing macros/generic_tests/expression_is_true.sql
[0m21:36:31.876940 [debug] [MainThread]: Parsing macros/generic_tests/not_null_proportion.sql
[0m21:36:31.877943 [debug] [MainThread]: Parsing macros/generic_tests/sequential_values.sql
[0m21:36:31.879494 [debug] [MainThread]: Parsing macros/generic_tests/test_not_null_where.sql
[0m21:36:31.880138 [debug] [MainThread]: Parsing macros/generic_tests/equality.sql
[0m21:36:31.881916 [debug] [MainThread]: Parsing macros/generic_tests/mutually_exclusive_ranges.sql
[0m21:36:31.886452 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
[0m21:36:31.886932 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
[0m21:36:31.887449 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
[0m21:36:31.887917 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
[0m21:36:31.888425 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
[0m21:36:31.890297 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
[0m21:36:31.891059 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
[0m21:36:31.892791 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
[0m21:36:31.894927 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
[0m21:36:31.896478 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
[0m21:36:31.897230 [debug] [MainThread]: Parsing macros/sql/star.sql
[0m21:36:31.899091 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
[0m21:36:31.903256 [debug] [MainThread]: Parsing macros/sql/union.sql
[0m21:36:31.909197 [debug] [MainThread]: Parsing macros/sql/groupby.sql
[0m21:36:31.909922 [debug] [MainThread]: Parsing macros/sql/deduplicate.sql
[0m21:36:31.913302 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
[0m21:36:31.914913 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
[0m21:36:31.915639 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
[0m21:36:31.916376 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
[0m21:36:31.919501 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
[0m21:36:31.922253 [debug] [MainThread]: Parsing macros/sql/pivot.sql
[0m21:36:31.924219 [debug] [MainThread]: Parsing macros/sql/get_filtered_columns_in_relation.sql
[0m21:36:31.925483 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
[0m21:36:31.926514 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
[0m21:36:31.927257 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
[0m21:36:31.930059 [debug] [MainThread]: Parsing macros/predict.sql
[0m21:36:31.930453 [debug] [MainThread]: Parsing macros/recommend.sql
[0m21:36:31.930856 [debug] [MainThread]: Parsing macros/detect_anomalies.sql
[0m21:36:31.931170 [debug] [MainThread]: Parsing macros/hparam.sql
[0m21:36:31.931607 [debug] [MainThread]: Parsing macros/materializations/model.sql
[0m21:36:31.937303 [debug] [MainThread]: Parsing macros/hooks/model_audit.sql
[0m21:36:32.175568 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_4hr.sql
[0m21:36:32.184770 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_4hr.sql
[0m21:36:32.185630 [debug] [MainThread]: 1699: static parser successfully parsed example/ml_detect_tweaked.sql
[0m21:36:32.187205 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_12hr.sql
[0m21:36:32.192400 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_12hr.sql
[0m21:36:32.192986 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_max_RMSD_results.sql
[0m21:36:32.194456 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_8hr.sql
[0m21:36:32.199527 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_8hr.sql
[0m21:36:32.200170 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_8hr.sql
[0m21:36:32.205226 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_8hr.sql
[0m21:36:32.205860 [debug] [MainThread]: 1699: static parser successfully parsed example/count_cutoff.sql
[0m21:36:32.207062 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_features.sql
[0m21:36:32.208568 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_24hr.sql
[0m21:36:32.213322 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_24hr.sql
[0m21:36:32.213894 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_model_features_dbt.sql
[0m21:36:32.215263 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_min_anomalies_results.sql
[0m21:36:32.216461 [debug] [MainThread]: 1699: static parser successfully parsed example/dev_ml_model_constraints.sql
[0m21:36:32.218110 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_long.sql
[0m21:36:32.219435 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_model_features_dbt.sql
[0m21:36:32.220596 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_quartiles_short.sql
[0m21:36:32.221725 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_events_with_anomalies.sql
[0m21:36:32.223151 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_alerts_fired_automation_dbt.sql
[0m21:36:32.224235 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_all_models.sql
[0m21:36:32.225317 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies_results.sql
[0m21:36:32.226595 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies.sql
[0m21:36:32.227924 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_12hr.sql
[0m21:36:32.233370 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_12hr.sql
[0m21:36:32.233987 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_bounds_short.sql
[0m21:36:32.235232 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_short.sql
[0m21:36:32.236395 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_RMSD_results.sql
[0m21:36:32.237564 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_min_anomalies.sql
[0m21:36:32.238833 [debug] [MainThread]: 1699: static parser successfully parsed example/reference_derived.sql
[0m21:36:32.240036 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_bounds_long.sql
[0m21:36:32.241160 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_phase.sql
[0m21:36:32.242493 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_24hr.sql
[0m21:36:32.247309 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_24hr.sql
[0m21:36:32.248034 [debug] [MainThread]: 1603: static parser failed on example/derived_ml_detect.sql
[0m21:36:32.253820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a8ada60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aba6e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aba6df0>]}
[0m21:36:32.254001 [debug] [MainThread]: Flushing usage events
[0m21:36:32.607817 [error] [MainThread]: Encountered an error:
'NoneType' object is not iterable
[0m21:36:32.609940 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 349, in load
    self.parse_project(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 473, in parse_project
    parser.parse_file(block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 392, in parse_file
    self.parse_node(file_block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 366, in parse_node
    self.render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/models.py", line 147, in render_update
    super().render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 341, in render_update
    context = self.render_with_context(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 238, in render_with_context
    get_rendered(parsed_node.raw_sql, context, parsed_node, capture_macros=True)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 571, in get_rendered
    return render_template(template, ctx, node)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 526, in render_template
    return template.render(ctx)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 1090, in render
    self.environment.handle_exception()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 832, in handle_exception
    reraise(*rewrite_traceback_stack(source=source))
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/_compat.py", line 28, in reraise
    raise value.with_traceback(tb)
  File "<template>", line 20, in top-level template code
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 403, in __init__
    self._iterator = self._to_iterator(iterable)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 411, in _to_iterator
    return iter(iterable)
TypeError: 'NoneType' object is not iterable



============================== 2023-02-09 21:40:41.221467 | 5a296b9a-37c2-4394-b10d-4344f0bf554d ==============================
[0m21:40:41.221517 [info ] [MainThread]: Running with dbt=1.2.1
[0m21:40:41.222115 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'parse_only': False, 'which': 'compile', 'rpc_method': 'compile', 'indirect_selection': 'eager'}
[0m21:40:41.222239 [debug] [MainThread]: Tracking: tracking
[0m21:40:41.231491 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112327370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112327790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112327a60>]}
[0m21:40:41.248020 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m21:40:41.248229 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '5a296b9a-37c2-4394-b10d-4344f0bf554d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112382fd0>]}
[0m21:40:41.299923 [debug] [MainThread]: Parsing macros/etc.sql
[0m21:40:41.301233 [debug] [MainThread]: Parsing macros/catalog.sql
[0m21:40:41.304590 [debug] [MainThread]: Parsing macros/adapters.sql
[0m21:40:41.316550 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m21:40:41.317755 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m21:40:41.319004 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m21:40:41.321074 [debug] [MainThread]: Parsing macros/materializations/copy.sql
[0m21:40:41.322295 [debug] [MainThread]: Parsing macros/materializations/incremental.sql
[0m21:40:41.330091 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m21:40:41.330937 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m21:40:41.331119 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m21:40:41.331421 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m21:40:41.331590 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m21:40:41.331856 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m21:40:41.332152 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m21:40:41.332603 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m21:40:41.333172 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m21:40:41.333403 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m21:40:41.333634 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m21:40:41.333877 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m21:40:41.334075 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m21:40:41.334801 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m21:40:41.335101 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m21:40:41.336663 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m21:40:41.338426 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m21:40:41.339428 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m21:40:41.340175 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m21:40:41.348203 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m21:40:41.354675 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m21:40:41.360938 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m21:40:41.363093 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m21:40:41.363947 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m21:40:41.364766 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m21:40:41.367000 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m21:40:41.375359 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m21:40:41.376073 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m21:40:41.380835 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m21:40:41.388420 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m21:40:41.390988 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m21:40:41.392270 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m21:40:41.394819 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m21:40:41.395422 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m21:40:41.396974 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m21:40:41.398018 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m21:40:41.401343 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m21:40:41.410271 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m21:40:41.410968 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m21:40:41.412136 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m21:40:41.412840 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m21:40:41.413251 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m21:40:41.413607 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m21:40:41.413931 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m21:40:41.414545 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m21:40:41.416597 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m21:40:41.420625 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m21:40:41.421014 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m21:40:41.421558 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m21:40:41.422000 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m21:40:41.422425 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m21:40:41.422976 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m21:40:41.423323 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m21:40:41.423786 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m21:40:41.424277 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m21:40:41.425427 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m21:40:41.425968 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m21:40:41.426443 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m21:40:41.426911 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m21:40:41.427407 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m21:40:41.427827 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m21:40:41.428305 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m21:40:41.428714 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m21:40:41.431512 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m21:40:41.432008 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m21:40:41.432827 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m21:40:41.433801 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m21:40:41.434258 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m21:40:41.435263 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m21:40:41.436477 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m21:40:41.443814 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m21:40:41.445192 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m21:40:41.451720 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m21:40:41.453769 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m21:40:41.457062 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m21:40:41.461664 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m21:40:41.463051 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
[0m21:40:41.463502 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
[0m21:40:41.464052 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
[0m21:40:41.464520 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
[0m21:40:41.467122 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
[0m21:40:41.467625 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_array_to_string.sql
[0m21:40:41.468820 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
[0m21:40:41.469431 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
[0m21:40:41.470793 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
[0m21:40:41.471267 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
[0m21:40:41.472047 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
[0m21:40:41.473137 [debug] [MainThread]: Parsing macros/cross_db_utils/listagg.sql
[0m21:40:41.477416 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
[0m21:40:41.482436 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
[0m21:40:41.483274 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
[0m21:40:41.483902 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
[0m21:40:41.484526 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
[0m21:40:41.485245 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
[0m21:40:41.485829 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
[0m21:40:41.486552 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
[0m21:40:41.487020 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
[0m21:40:41.488494 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
[0m21:40:41.491015 [debug] [MainThread]: Parsing macros/cross_db_utils/array_concat.sql
[0m21:40:41.491892 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
[0m21:40:41.492643 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
[0m21:40:41.494227 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
[0m21:40:41.497218 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
[0m21:40:41.497923 [debug] [MainThread]: Parsing macros/cross_db_utils/array_construct.sql
[0m21:40:41.499158 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
[0m21:40:41.500115 [debug] [MainThread]: Parsing macros/cross_db_utils/array_append.sql
[0m21:40:41.501036 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
[0m21:40:41.513517 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
[0m21:40:41.514496 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
[0m21:40:41.515716 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
[0m21:40:41.516420 [debug] [MainThread]: Parsing macros/generic_tests/fewer_rows_than.sql
[0m21:40:41.517214 [debug] [MainThread]: Parsing macros/generic_tests/equal_rowcount.sql
[0m21:40:41.517975 [debug] [MainThread]: Parsing macros/generic_tests/relationships_where.sql
[0m21:40:41.518996 [debug] [MainThread]: Parsing macros/generic_tests/recency.sql
[0m21:40:41.519762 [debug] [MainThread]: Parsing macros/generic_tests/not_constant.sql
[0m21:40:41.520308 [debug] [MainThread]: Parsing macros/generic_tests/accepted_range.sql
[0m21:40:41.521497 [debug] [MainThread]: Parsing macros/generic_tests/not_accepted_values.sql
[0m21:40:41.522453 [debug] [MainThread]: Parsing macros/generic_tests/test_unique_where.sql
[0m21:40:41.523104 [debug] [MainThread]: Parsing macros/generic_tests/at_least_one.sql
[0m21:40:41.523877 [debug] [MainThread]: Parsing macros/generic_tests/unique_combination_of_columns.sql
[0m21:40:41.525250 [debug] [MainThread]: Parsing macros/generic_tests/cardinality_equality.sql
[0m21:40:41.526182 [debug] [MainThread]: Parsing macros/generic_tests/expression_is_true.sql
[0m21:40:41.526974 [debug] [MainThread]: Parsing macros/generic_tests/not_null_proportion.sql
[0m21:40:41.527952 [debug] [MainThread]: Parsing macros/generic_tests/sequential_values.sql
[0m21:40:41.529439 [debug] [MainThread]: Parsing macros/generic_tests/test_not_null_where.sql
[0m21:40:41.530058 [debug] [MainThread]: Parsing macros/generic_tests/equality.sql
[0m21:40:41.531681 [debug] [MainThread]: Parsing macros/generic_tests/mutually_exclusive_ranges.sql
[0m21:40:41.536474 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
[0m21:40:41.537052 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
[0m21:40:41.537606 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
[0m21:40:41.538086 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
[0m21:40:41.538603 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
[0m21:40:41.540552 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
[0m21:40:41.541352 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
[0m21:40:41.542944 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
[0m21:40:41.544964 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
[0m21:40:41.546532 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
[0m21:40:41.547279 [debug] [MainThread]: Parsing macros/sql/star.sql
[0m21:40:41.549060 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
[0m21:40:41.552911 [debug] [MainThread]: Parsing macros/sql/union.sql
[0m21:40:41.558789 [debug] [MainThread]: Parsing macros/sql/groupby.sql
[0m21:40:41.559600 [debug] [MainThread]: Parsing macros/sql/deduplicate.sql
[0m21:40:41.562976 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
[0m21:40:41.564541 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
[0m21:40:41.565366 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
[0m21:40:41.566268 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
[0m21:40:41.569464 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
[0m21:40:41.572157 [debug] [MainThread]: Parsing macros/sql/pivot.sql
[0m21:40:41.574071 [debug] [MainThread]: Parsing macros/sql/get_filtered_columns_in_relation.sql
[0m21:40:41.575373 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
[0m21:40:41.576407 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
[0m21:40:41.577218 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
[0m21:40:41.580118 [debug] [MainThread]: Parsing macros/predict.sql
[0m21:40:41.580525 [debug] [MainThread]: Parsing macros/recommend.sql
[0m21:40:41.580924 [debug] [MainThread]: Parsing macros/detect_anomalies.sql
[0m21:40:41.581228 [debug] [MainThread]: Parsing macros/hparam.sql
[0m21:40:41.581671 [debug] [MainThread]: Parsing macros/materializations/model.sql
[0m21:40:41.587489 [debug] [MainThread]: Parsing macros/hooks/model_audit.sql
[0m21:40:41.828647 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_4hr.sql
[0m21:40:41.838390 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_4hr.sql
[0m21:40:41.839773 [debug] [MainThread]: 1699: static parser successfully parsed example/ml_detect_tweaked.sql
[0m21:40:41.841211 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_12hr.sql
[0m21:40:41.846606 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_12hr.sql
[0m21:40:41.847346 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_max_RMSD_results.sql
[0m21:40:41.848802 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_8hr.sql
[0m21:40:41.853619 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_8hr.sql
[0m21:40:41.854179 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_8hr.sql
[0m21:40:41.859033 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_8hr.sql
[0m21:40:41.859787 [debug] [MainThread]: 1699: static parser successfully parsed example/count_cutoff.sql
[0m21:40:41.861164 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_features.sql
[0m21:40:41.862428 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_24hr.sql
[0m21:40:41.867036 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_24hr.sql
[0m21:40:41.867627 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_model_features_dbt.sql
[0m21:40:41.868765 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_min_anomalies_results.sql
[0m21:40:41.870135 [debug] [MainThread]: 1699: static parser successfully parsed example/dev_ml_model_constraints.sql
[0m21:40:41.871903 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_long.sql
[0m21:40:41.873111 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_model_features_dbt.sql
[0m21:40:41.874414 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_quartiles_short.sql
[0m21:40:41.875620 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_events_with_anomalies.sql
[0m21:40:41.877051 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_alerts_fired_automation_dbt.sql
[0m21:40:41.878290 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_all_models.sql
[0m21:40:41.879612 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies_results.sql
[0m21:40:41.880852 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies.sql
[0m21:40:41.882274 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_12hr.sql
[0m21:40:41.887628 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_12hr.sql
[0m21:40:41.888233 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_bounds_short.sql
[0m21:40:41.889463 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_short.sql
[0m21:40:41.890753 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_RMSD_results.sql
[0m21:40:41.891897 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_min_anomalies.sql
[0m21:40:41.893174 [debug] [MainThread]: 1699: static parser successfully parsed example/reference_derived.sql
[0m21:40:41.894357 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_bounds_long.sql
[0m21:40:41.895495 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_phase.sql
[0m21:40:41.896693 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_24hr.sql
[0m21:40:41.901598 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_24hr.sql
[0m21:40:41.902402 [debug] [MainThread]: 1603: static parser failed on example/derived_ml_detect.sql
[0m21:40:41.908465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1123f6c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1125dbf70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1125db1f0>]}
[0m21:40:41.908625 [debug] [MainThread]: Flushing usage events
[0m21:40:42.330318 [error] [MainThread]: Encountered an error:
'NoneType' object is not iterable
[0m21:40:42.332443 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 349, in load
    self.parse_project(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 473, in parse_project
    parser.parse_file(block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 392, in parse_file
    self.parse_node(file_block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 366, in parse_node
    self.render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/models.py", line 147, in render_update
    super().render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 341, in render_update
    context = self.render_with_context(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 238, in render_with_context
    get_rendered(parsed_node.raw_sql, context, parsed_node, capture_macros=True)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 571, in get_rendered
    return render_template(template, ctx, node)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 526, in render_template
    return template.render(ctx)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 1090, in render
    self.environment.handle_exception()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 832, in handle_exception
    reraise(*rewrite_traceback_stack(source=source))
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/_compat.py", line 28, in reraise
    raise value.with_traceback(tb)
  File "<template>", line 20, in top-level template code
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 403, in __init__
    self._iterator = self._to_iterator(iterable)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 411, in _to_iterator
    return iter(iterable)
TypeError: 'NoneType' object is not iterable



============================== 2023-02-09 21:44:17.299062 | d2ff9d8f-e846-4617-8a41-a7d298dddae5 ==============================
[0m21:44:17.299084 [info ] [MainThread]: Running with dbt=1.2.1
[0m21:44:17.300066 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'parse_only': False, 'which': 'compile', 'rpc_method': 'compile', 'indirect_selection': 'eager'}
[0m21:44:17.300208 [debug] [MainThread]: Tracking: tracking
[0m21:44:17.309846 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c9d370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c9cc40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c9c340>]}
[0m21:44:17.326262 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m21:44:17.326503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'd2ff9d8f-e846-4617-8a41-a7d298dddae5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107cacbb0>]}
[0m21:44:17.379225 [debug] [MainThread]: Parsing macros/etc.sql
[0m21:44:17.380701 [debug] [MainThread]: Parsing macros/catalog.sql
[0m21:44:17.384300 [debug] [MainThread]: Parsing macros/adapters.sql
[0m21:44:17.396424 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m21:44:17.397718 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m21:44:17.399096 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m21:44:17.401806 [debug] [MainThread]: Parsing macros/materializations/copy.sql
[0m21:44:17.403315 [debug] [MainThread]: Parsing macros/materializations/incremental.sql
[0m21:44:17.411342 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m21:44:17.412172 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m21:44:17.412346 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m21:44:17.412634 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m21:44:17.412795 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m21:44:17.413049 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m21:44:17.413341 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m21:44:17.413786 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m21:44:17.414338 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m21:44:17.414563 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m21:44:17.414786 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m21:44:17.415083 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m21:44:17.415308 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m21:44:17.415955 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m21:44:17.416196 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m21:44:17.417750 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m21:44:17.419534 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m21:44:17.420815 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m21:44:17.421792 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m21:44:17.429928 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m21:44:17.436765 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m21:44:17.443602 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m21:44:17.445790 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m21:44:17.446603 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m21:44:17.447397 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m21:44:17.449427 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m21:44:17.458516 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m21:44:17.459748 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m21:44:17.464632 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m21:44:17.472762 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m21:44:17.475557 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m21:44:17.476995 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m21:44:17.479592 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m21:44:17.480179 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m21:44:17.481725 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m21:44:17.482936 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m21:44:17.486557 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m21:44:17.496060 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m21:44:17.496959 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m21:44:17.498181 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m21:44:17.498905 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m21:44:17.499311 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m21:44:17.499672 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m21:44:17.499976 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m21:44:17.500577 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m21:44:17.502646 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m21:44:17.506549 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m21:44:17.506923 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m21:44:17.507664 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m21:44:17.508152 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m21:44:17.508584 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m21:44:17.509152 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m21:44:17.509517 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m21:44:17.509984 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m21:44:17.510464 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m21:44:17.511587 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m21:44:17.512141 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m21:44:17.512620 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m21:44:17.513086 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m21:44:17.513540 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m21:44:17.513947 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m21:44:17.514421 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m21:44:17.514831 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m21:44:17.517661 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m21:44:17.518429 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m21:44:17.519301 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m21:44:17.520264 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m21:44:17.520739 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m21:44:17.521764 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m21:44:17.522980 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m21:44:17.530382 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m21:44:17.531798 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m21:44:17.538564 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m21:44:17.540841 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m21:44:17.544348 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m21:44:17.549142 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m21:44:17.550704 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
[0m21:44:17.551266 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
[0m21:44:17.551863 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
[0m21:44:17.552304 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
[0m21:44:17.555032 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
[0m21:44:17.555539 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_array_to_string.sql
[0m21:44:17.556730 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
[0m21:44:17.557275 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
[0m21:44:17.558524 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
[0m21:44:17.558969 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
[0m21:44:17.559747 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
[0m21:44:17.561065 [debug] [MainThread]: Parsing macros/cross_db_utils/listagg.sql
[0m21:44:17.565373 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
[0m21:44:17.570099 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
[0m21:44:17.571215 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
[0m21:44:17.571964 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
[0m21:44:17.572608 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
[0m21:44:17.573338 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
[0m21:44:17.573893 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
[0m21:44:17.574584 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
[0m21:44:17.575050 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
[0m21:44:17.576537 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
[0m21:44:17.579251 [debug] [MainThread]: Parsing macros/cross_db_utils/array_concat.sql
[0m21:44:17.580165 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
[0m21:44:17.581004 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
[0m21:44:17.582764 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
[0m21:44:17.585666 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
[0m21:44:17.586312 [debug] [MainThread]: Parsing macros/cross_db_utils/array_construct.sql
[0m21:44:17.587579 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
[0m21:44:17.588514 [debug] [MainThread]: Parsing macros/cross_db_utils/array_append.sql
[0m21:44:17.589419 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
[0m21:44:17.601501 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
[0m21:44:17.602629 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
[0m21:44:17.604075 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
[0m21:44:17.604838 [debug] [MainThread]: Parsing macros/generic_tests/fewer_rows_than.sql
[0m21:44:17.605658 [debug] [MainThread]: Parsing macros/generic_tests/equal_rowcount.sql
[0m21:44:17.606416 [debug] [MainThread]: Parsing macros/generic_tests/relationships_where.sql
[0m21:44:17.607459 [debug] [MainThread]: Parsing macros/generic_tests/recency.sql
[0m21:44:17.608249 [debug] [MainThread]: Parsing macros/generic_tests/not_constant.sql
[0m21:44:17.608946 [debug] [MainThread]: Parsing macros/generic_tests/accepted_range.sql
[0m21:44:17.610263 [debug] [MainThread]: Parsing macros/generic_tests/not_accepted_values.sql
[0m21:44:17.611263 [debug] [MainThread]: Parsing macros/generic_tests/test_unique_where.sql
[0m21:44:17.611904 [debug] [MainThread]: Parsing macros/generic_tests/at_least_one.sql
[0m21:44:17.612467 [debug] [MainThread]: Parsing macros/generic_tests/unique_combination_of_columns.sql
[0m21:44:17.613768 [debug] [MainThread]: Parsing macros/generic_tests/cardinality_equality.sql
[0m21:44:17.614697 [debug] [MainThread]: Parsing macros/generic_tests/expression_is_true.sql
[0m21:44:17.615494 [debug] [MainThread]: Parsing macros/generic_tests/not_null_proportion.sql
[0m21:44:17.616479 [debug] [MainThread]: Parsing macros/generic_tests/sequential_values.sql
[0m21:44:17.618331 [debug] [MainThread]: Parsing macros/generic_tests/test_not_null_where.sql
[0m21:44:17.619106 [debug] [MainThread]: Parsing macros/generic_tests/equality.sql
[0m21:44:17.620958 [debug] [MainThread]: Parsing macros/generic_tests/mutually_exclusive_ranges.sql
[0m21:44:17.625468 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
[0m21:44:17.625955 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
[0m21:44:17.626487 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
[0m21:44:17.626971 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
[0m21:44:17.627489 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
[0m21:44:17.629466 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
[0m21:44:17.630477 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
[0m21:44:17.632104 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
[0m21:44:17.634073 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
[0m21:44:17.635659 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
[0m21:44:17.636410 [debug] [MainThread]: Parsing macros/sql/star.sql
[0m21:44:17.638109 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
[0m21:44:17.642266 [debug] [MainThread]: Parsing macros/sql/union.sql
[0m21:44:17.648105 [debug] [MainThread]: Parsing macros/sql/groupby.sql
[0m21:44:17.648737 [debug] [MainThread]: Parsing macros/sql/deduplicate.sql
[0m21:44:17.652389 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
[0m21:44:17.654019 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
[0m21:44:17.654739 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
[0m21:44:17.655457 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
[0m21:44:17.658647 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
[0m21:44:17.661589 [debug] [MainThread]: Parsing macros/sql/pivot.sql
[0m21:44:17.663570 [debug] [MainThread]: Parsing macros/sql/get_filtered_columns_in_relation.sql
[0m21:44:17.664896 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
[0m21:44:17.665960 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
[0m21:44:17.666733 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
[0m21:44:17.669547 [debug] [MainThread]: Parsing macros/predict.sql
[0m21:44:17.670257 [debug] [MainThread]: Parsing macros/recommend.sql
[0m21:44:17.670710 [debug] [MainThread]: Parsing macros/detect_anomalies.sql
[0m21:44:17.671059 [debug] [MainThread]: Parsing macros/hparam.sql
[0m21:44:17.671536 [debug] [MainThread]: Parsing macros/materializations/model.sql
[0m21:44:17.677480 [debug] [MainThread]: Parsing macros/hooks/model_audit.sql
[0m21:44:17.925243 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_4hr.sql
[0m21:44:17.934922 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_4hr.sql
[0m21:44:17.936258 [debug] [MainThread]: 1699: static parser successfully parsed example/ml_detect_tweaked.sql
[0m21:44:17.937746 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_12hr.sql
[0m21:44:17.944170 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_12hr.sql
[0m21:44:17.948016 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_max_RMSD_results.sql
[0m21:44:17.949670 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_8hr.sql
[0m21:44:17.955269 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_8hr.sql
[0m21:44:17.956025 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_8hr.sql
[0m21:44:17.961439 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_8hr.sql
[0m21:44:17.962608 [debug] [MainThread]: 1699: static parser successfully parsed example/count_cutoff.sql
[0m21:44:17.964384 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_features.sql
[0m21:44:17.966341 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_24hr.sql
[0m21:44:17.971771 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_24hr.sql
[0m21:44:17.972714 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_model_features_dbt.sql
[0m21:44:17.974278 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_min_anomalies_results.sql
[0m21:44:17.975926 [debug] [MainThread]: 1699: static parser successfully parsed example/dev_ml_model_constraints.sql
[0m21:44:17.978062 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_long.sql
[0m21:44:17.979754 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_model_features_dbt.sql
[0m21:44:17.985730 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_quartiles_short.sql
[0m21:44:17.987638 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_events_with_anomalies.sql
[0m21:44:17.989231 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_alerts_fired_automation_dbt.sql
[0m21:44:17.990802 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_all_models.sql
[0m21:44:17.993617 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies_results.sql
[0m21:44:17.994768 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies.sql
[0m21:44:17.996027 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_12hr.sql
[0m21:44:18.001592 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_12hr.sql
[0m21:44:18.002252 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_bounds_short.sql
[0m21:44:18.003707 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_short.sql
[0m21:44:18.005110 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_RMSD_results.sql
[0m21:44:18.006251 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_min_anomalies.sql
[0m21:44:18.007727 [debug] [MainThread]: 1699: static parser successfully parsed example/reference_derived.sql
[0m21:44:18.010520 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_bounds_long.sql
[0m21:44:18.011794 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_phase.sql
[0m21:44:18.013103 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_24hr.sql
[0m21:44:18.016162 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_24hr.sql
[0m21:44:18.016895 [debug] [MainThread]: 1603: static parser failed on example/derived_ml_detect.sql
[0m21:44:18.023264 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107081970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f57910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f57b50>]}
[0m21:44:18.023480 [debug] [MainThread]: Flushing usage events
[0m21:44:18.477587 [error] [MainThread]: Encountered an error:
'NoneType' object is not iterable
[0m21:44:18.478706 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 349, in load
    self.parse_project(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 473, in parse_project
    parser.parse_file(block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 392, in parse_file
    self.parse_node(file_block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 366, in parse_node
    self.render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/models.py", line 147, in render_update
    super().render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 341, in render_update
    context = self.render_with_context(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 238, in render_with_context
    get_rendered(parsed_node.raw_sql, context, parsed_node, capture_macros=True)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 571, in get_rendered
    return render_template(template, ctx, node)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 526, in render_template
    return template.render(ctx)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 1090, in render
    self.environment.handle_exception()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 832, in handle_exception
    reraise(*rewrite_traceback_stack(source=source))
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/_compat.py", line 28, in reraise
    raise value.with_traceback(tb)
  File "<template>", line 20, in top-level template code
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 403, in __init__
    self._iterator = self._to_iterator(iterable)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 411, in _to_iterator
    return iter(iterable)
TypeError: 'NoneType' object is not iterable



============================== 2023-02-09 21:45:20.471366 | 60935854-3e98-4055-8cdb-d976e95fa2c9 ==============================
[0m21:45:20.471390 [info ] [MainThread]: Running with dbt=1.2.1
[0m21:45:20.471936 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'parse_only': False, 'which': 'compile', 'rpc_method': 'compile', 'indirect_selection': 'eager'}
[0m21:45:20.472042 [debug] [MainThread]: Tracking: tracking
[0m21:45:20.480342 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c84ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c846d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c846a0>]}
[0m21:45:20.496296 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m21:45:20.496506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '60935854-3e98-4055-8cdb-d976e95fa2c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105cc45b0>]}
[0m21:45:20.542110 [debug] [MainThread]: Parsing macros/etc.sql
[0m21:45:20.543454 [debug] [MainThread]: Parsing macros/catalog.sql
[0m21:45:20.546924 [debug] [MainThread]: Parsing macros/adapters.sql
[0m21:45:20.559084 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m21:45:20.560311 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m21:45:20.561656 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m21:45:20.563866 [debug] [MainThread]: Parsing macros/materializations/copy.sql
[0m21:45:20.565178 [debug] [MainThread]: Parsing macros/materializations/incremental.sql
[0m21:45:20.572680 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m21:45:20.573489 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m21:45:20.573676 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m21:45:20.573985 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m21:45:20.574157 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m21:45:20.574419 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m21:45:20.574704 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m21:45:20.575153 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m21:45:20.575715 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m21:45:20.575936 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m21:45:20.576165 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m21:45:20.576396 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m21:45:20.576584 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m21:45:20.577198 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m21:45:20.577430 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m21:45:20.578970 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m21:45:20.580743 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m21:45:20.581775 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m21:45:20.582528 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m21:45:20.590447 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m21:45:20.596914 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m21:45:20.603199 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m21:45:20.605464 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m21:45:20.606327 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m21:45:20.607176 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m21:45:20.609231 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m21:45:20.618026 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m21:45:20.618728 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m21:45:20.623349 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m21:45:20.631389 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m21:45:20.634170 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m21:45:20.635508 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m21:45:20.638201 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m21:45:20.638925 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m21:45:20.640592 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m21:45:20.641699 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m21:45:20.645124 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m21:45:20.654654 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m21:45:20.655405 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m21:45:20.656543 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m21:45:20.657245 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m21:45:20.657647 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m21:45:20.658005 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m21:45:20.658316 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m21:45:20.658981 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m21:45:20.661060 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m21:45:20.665128 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m21:45:20.665514 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m21:45:20.666058 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m21:45:20.666486 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m21:45:20.666899 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m21:45:20.667443 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m21:45:20.667796 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m21:45:20.668253 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m21:45:20.668758 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m21:45:20.669940 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m21:45:20.670544 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m21:45:20.671045 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m21:45:20.671509 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m21:45:20.671956 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m21:45:20.672353 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m21:45:20.672816 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m21:45:20.673214 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m21:45:20.675993 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m21:45:20.676450 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m21:45:20.677259 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m21:45:20.678222 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m21:45:20.678693 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m21:45:20.679757 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m21:45:20.681060 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m21:45:20.688065 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m21:45:20.689373 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m21:45:20.695789 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m21:45:20.697856 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m21:45:20.701103 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m21:45:20.705744 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m21:45:20.707138 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
[0m21:45:20.707642 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
[0m21:45:20.708209 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
[0m21:45:20.708636 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
[0m21:45:20.711323 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
[0m21:45:20.711873 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_array_to_string.sql
[0m21:45:20.713110 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
[0m21:45:20.713672 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
[0m21:45:20.714912 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
[0m21:45:20.715363 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
[0m21:45:20.716141 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
[0m21:45:20.717204 [debug] [MainThread]: Parsing macros/cross_db_utils/listagg.sql
[0m21:45:20.721479 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
[0m21:45:20.726261 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
[0m21:45:20.727067 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
[0m21:45:20.727665 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
[0m21:45:20.728292 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
[0m21:45:20.729050 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
[0m21:45:20.729609 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
[0m21:45:20.730283 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
[0m21:45:20.730747 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
[0m21:45:20.732212 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
[0m21:45:20.734772 [debug] [MainThread]: Parsing macros/cross_db_utils/array_concat.sql
[0m21:45:20.735608 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
[0m21:45:20.736351 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
[0m21:45:20.737920 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
[0m21:45:20.740973 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
[0m21:45:20.741722 [debug] [MainThread]: Parsing macros/cross_db_utils/array_construct.sql
[0m21:45:20.742981 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
[0m21:45:20.743963 [debug] [MainThread]: Parsing macros/cross_db_utils/array_append.sql
[0m21:45:20.744879 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
[0m21:45:20.757209 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
[0m21:45:20.758186 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
[0m21:45:20.759433 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
[0m21:45:20.760214 [debug] [MainThread]: Parsing macros/generic_tests/fewer_rows_than.sql
[0m21:45:20.761069 [debug] [MainThread]: Parsing macros/generic_tests/equal_rowcount.sql
[0m21:45:20.761841 [debug] [MainThread]: Parsing macros/generic_tests/relationships_where.sql
[0m21:45:20.762887 [debug] [MainThread]: Parsing macros/generic_tests/recency.sql
[0m21:45:20.763673 [debug] [MainThread]: Parsing macros/generic_tests/not_constant.sql
[0m21:45:20.764213 [debug] [MainThread]: Parsing macros/generic_tests/accepted_range.sql
[0m21:45:20.765389 [debug] [MainThread]: Parsing macros/generic_tests/not_accepted_values.sql
[0m21:45:20.766400 [debug] [MainThread]: Parsing macros/generic_tests/test_unique_where.sql
[0m21:45:20.767050 [debug] [MainThread]: Parsing macros/generic_tests/at_least_one.sql
[0m21:45:20.767615 [debug] [MainThread]: Parsing macros/generic_tests/unique_combination_of_columns.sql
[0m21:45:20.768940 [debug] [MainThread]: Parsing macros/generic_tests/cardinality_equality.sql
[0m21:45:20.769880 [debug] [MainThread]: Parsing macros/generic_tests/expression_is_true.sql
[0m21:45:20.770689 [debug] [MainThread]: Parsing macros/generic_tests/not_null_proportion.sql
[0m21:45:20.771685 [debug] [MainThread]: Parsing macros/generic_tests/sequential_values.sql
[0m21:45:20.773189 [debug] [MainThread]: Parsing macros/generic_tests/test_not_null_where.sql
[0m21:45:20.773806 [debug] [MainThread]: Parsing macros/generic_tests/equality.sql
[0m21:45:20.775454 [debug] [MainThread]: Parsing macros/generic_tests/mutually_exclusive_ranges.sql
[0m21:45:20.780109 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
[0m21:45:20.780694 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
[0m21:45:20.781229 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
[0m21:45:20.781713 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
[0m21:45:20.782231 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
[0m21:45:20.784118 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
[0m21:45:20.784895 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
[0m21:45:20.786432 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
[0m21:45:20.788579 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
[0m21:45:20.790261 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
[0m21:45:20.791043 [debug] [MainThread]: Parsing macros/sql/star.sql
[0m21:45:20.792814 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
[0m21:45:20.796615 [debug] [MainThread]: Parsing macros/sql/union.sql
[0m21:45:20.802400 [debug] [MainThread]: Parsing macros/sql/groupby.sql
[0m21:45:20.803283 [debug] [MainThread]: Parsing macros/sql/deduplicate.sql
[0m21:45:20.806733 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
[0m21:45:20.808516 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
[0m21:45:20.809242 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
[0m21:45:20.809953 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
[0m21:45:20.812998 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
[0m21:45:20.815702 [debug] [MainThread]: Parsing macros/sql/pivot.sql
[0m21:45:20.817810 [debug] [MainThread]: Parsing macros/sql/get_filtered_columns_in_relation.sql
[0m21:45:20.819247 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
[0m21:45:20.820415 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
[0m21:45:20.821215 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
[0m21:45:20.824230 [debug] [MainThread]: Parsing macros/predict.sql
[0m21:45:20.824720 [debug] [MainThread]: Parsing macros/recommend.sql
[0m21:45:20.825133 [debug] [MainThread]: Parsing macros/detect_anomalies.sql
[0m21:45:20.825461 [debug] [MainThread]: Parsing macros/hparam.sql
[0m21:45:20.825916 [debug] [MainThread]: Parsing macros/materializations/model.sql
[0m21:45:20.831845 [debug] [MainThread]: Parsing macros/hooks/model_audit.sql
[0m21:45:21.070276 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_4hr.sql
[0m21:45:21.079134 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_4hr.sql
[0m21:45:21.080259 [debug] [MainThread]: 1699: static parser successfully parsed example/ml_detect_tweaked.sql
[0m21:45:21.081934 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_12hr.sql
[0m21:45:21.087148 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_12hr.sql
[0m21:45:21.087726 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_max_RMSD_results.sql
[0m21:45:21.089143 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_8hr.sql
[0m21:45:21.094103 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_8hr.sql
[0m21:45:21.094695 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_8hr.sql
[0m21:45:21.099444 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_8hr.sql
[0m21:45:21.100141 [debug] [MainThread]: 1699: static parser successfully parsed example/count_cutoff.sql
[0m21:45:21.101455 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_features.sql
[0m21:45:21.103002 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_24hr.sql
[0m21:45:21.107757 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_24hr.sql
[0m21:45:21.108401 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_model_features_dbt.sql
[0m21:45:21.109660 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_min_anomalies_results.sql
[0m21:45:21.110828 [debug] [MainThread]: 1699: static parser successfully parsed example/dev_ml_model_constraints.sql
[0m21:45:21.112522 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_long.sql
[0m21:45:21.113781 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_model_features_dbt.sql
[0m21:45:21.114910 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_quartiles_short.sql
[0m21:45:21.116126 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_events_with_anomalies.sql
[0m21:45:21.117587 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_alerts_fired_automation_dbt.sql
[0m21:45:21.118821 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_all_models.sql
[0m21:45:21.120083 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies_results.sql
[0m21:45:21.121174 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies.sql
[0m21:45:21.122644 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_12hr.sql
[0m21:45:21.127914 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_12hr.sql
[0m21:45:21.128554 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_bounds_short.sql
[0m21:45:21.129778 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_short.sql
[0m21:45:21.131029 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_RMSD_results.sql
[0m21:45:21.132131 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_min_anomalies.sql
[0m21:45:21.133772 [debug] [MainThread]: 1699: static parser successfully parsed example/reference_derived.sql
[0m21:45:21.135137 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_bounds_long.sql
[0m21:45:21.136393 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_phase.sql
[0m21:45:21.137666 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_24hr.sql
[0m21:45:21.140601 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_24hr.sql
[0m21:45:21.141344 [debug] [MainThread]: 1603: static parser failed on example/derived_ml_detect.sql
[0m21:45:21.147570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d2de80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106816f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068161f0>]}
[0m21:45:21.147729 [debug] [MainThread]: Flushing usage events
[0m21:45:21.518626 [error] [MainThread]: Encountered an error:
'NoneType' object is not iterable
[0m21:45:21.520119 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 349, in load
    self.parse_project(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 473, in parse_project
    parser.parse_file(block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 392, in parse_file
    self.parse_node(file_block)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 366, in parse_node
    self.render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/models.py", line 147, in render_update
    super().render_update(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 341, in render_update
    context = self.render_with_context(node, config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/base.py", line 238, in render_with_context
    get_rendered(parsed_node.raw_sql, context, parsed_node, capture_macros=True)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 571, in get_rendered
    return render_template(template, ctx, node)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/clients/jinja.py", line 526, in render_template
    return template.render(ctx)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 1090, in render
    self.environment.handle_exception()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/environment.py", line 832, in handle_exception
    reraise(*rewrite_traceback_stack(source=source))
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/_compat.py", line 28, in reraise
    raise value.with_traceback(tb)
  File "<template>", line 20, in top-level template code
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 403, in __init__
    self._iterator = self._to_iterator(iterable)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/jinja2/runtime.py", line 411, in _to_iterator
    return iter(iterable)
TypeError: 'NoneType' object is not iterable



============================== 2023-02-09 21:48:57.137455 | fa261689-df89-42e4-9b83-75e5a314ca82 ==============================
[0m21:48:57.137463 [info ] [MainThread]: Running with dbt=1.2.1
[0m21:48:57.137798 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m21:48:57.137866 [debug] [MainThread]: Tracking: tracking
[0m21:48:57.147825 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104576400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104576190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104570280>]}
[0m21:48:57.378927 [debug] [MainThread]: Executing "git --help"
[0m21:48:57.385174 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m21:48:57.385664 [debug] [MainThread]: STDERR: "b''"
[0m21:48:57.389623 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m21:48:57.390291 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:48:57.863095 [debug] [MainThread]: On debug: select 1 as id
[0m21:48:59.265341 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058f2be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058f2400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058cb460>]}
[0m21:48:59.266078 [debug] [MainThread]: Flushing usage events
[0m21:48:59.712864 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-09 21:49:04.315200 | c157fc0c-1e50-4e3c-af11-9caeb078be17 ==============================
[0m21:49:04.315239 [info ] [MainThread]: Running with dbt=1.2.1
[0m21:49:04.317630 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'parse_only': False, 'which': 'compile', 'rpc_method': 'compile', 'indirect_selection': 'eager'}
[0m21:49:04.317759 [debug] [MainThread]: Tracking: tracking
[0m21:49:04.326665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11221db20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112231ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112231a60>]}
[0m21:49:04.342808 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m21:49:04.343021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'c157fc0c-1e50-4e3c-af11-9caeb078be17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112231760>]}
[0m21:49:04.392050 [debug] [MainThread]: Parsing macros/etc.sql
[0m21:49:04.393381 [debug] [MainThread]: Parsing macros/catalog.sql
[0m21:49:04.396721 [debug] [MainThread]: Parsing macros/adapters.sql
[0m21:49:04.408473 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m21:49:04.409873 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m21:49:04.411217 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m21:49:04.413301 [debug] [MainThread]: Parsing macros/materializations/copy.sql
[0m21:49:04.414536 [debug] [MainThread]: Parsing macros/materializations/incremental.sql
[0m21:49:04.422457 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m21:49:04.423344 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m21:49:04.423534 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m21:49:04.423838 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m21:49:04.424005 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m21:49:04.424269 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m21:49:04.424575 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m21:49:04.425071 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m21:49:04.425686 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m21:49:04.425924 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m21:49:04.426158 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m21:49:04.426401 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m21:49:04.426597 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m21:49:04.427254 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m21:49:04.427501 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m21:49:04.428840 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m21:49:04.430743 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m21:49:04.431834 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m21:49:04.432595 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m21:49:04.440477 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m21:49:04.447181 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m21:49:04.453294 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m21:49:04.455468 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m21:49:04.456283 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m21:49:04.457085 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m21:49:04.459090 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m21:49:04.467741 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m21:49:04.468515 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m21:49:04.472935 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m21:49:04.480584 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m21:49:04.483190 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m21:49:04.484492 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m21:49:04.487021 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m21:49:04.487606 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m21:49:04.489182 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m21:49:04.490189 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m21:49:04.493469 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m21:49:04.502467 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m21:49:04.503252 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m21:49:04.504388 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m21:49:04.505085 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m21:49:04.505491 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m21:49:04.505849 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m21:49:04.506150 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m21:49:04.506773 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m21:49:04.508800 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m21:49:04.513136 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m21:49:04.513592 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m21:49:04.514148 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m21:49:04.514575 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m21:49:04.514999 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m21:49:04.515585 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m21:49:04.515963 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m21:49:04.516427 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m21:49:04.516909 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m21:49:04.518072 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m21:49:04.518685 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m21:49:04.519180 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m21:49:04.519656 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m21:49:04.520112 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m21:49:04.520546 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m21:49:04.521087 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m21:49:04.521497 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m21:49:04.524166 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m21:49:04.524574 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m21:49:04.525374 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m21:49:04.526391 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m21:49:04.526857 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m21:49:04.527868 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m21:49:04.529077 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m21:49:04.536284 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m21:49:04.537618 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m21:49:04.543878 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m21:49:04.546160 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m21:49:04.549673 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m21:49:04.554391 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m21:49:04.555971 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
[0m21:49:04.556469 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
[0m21:49:04.557038 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
[0m21:49:04.557490 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
[0m21:49:04.560168 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
[0m21:49:04.560721 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_array_to_string.sql
[0m21:49:04.561935 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
[0m21:49:04.562490 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
[0m21:49:04.563776 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
[0m21:49:04.564225 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
[0m21:49:04.565004 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
[0m21:49:04.566087 [debug] [MainThread]: Parsing macros/cross_db_utils/listagg.sql
[0m21:49:04.570329 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
[0m21:49:04.575186 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
[0m21:49:04.576001 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
[0m21:49:04.576644 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
[0m21:49:04.577261 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
[0m21:49:04.577978 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
[0m21:49:04.578517 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
[0m21:49:04.579207 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
[0m21:49:04.579712 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
[0m21:49:04.581195 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
[0m21:49:04.583714 [debug] [MainThread]: Parsing macros/cross_db_utils/array_concat.sql
[0m21:49:04.584520 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
[0m21:49:04.585283 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
[0m21:49:04.586851 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
[0m21:49:04.589671 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
[0m21:49:04.590376 [debug] [MainThread]: Parsing macros/cross_db_utils/array_construct.sql
[0m21:49:04.591630 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
[0m21:49:04.592587 [debug] [MainThread]: Parsing macros/cross_db_utils/array_append.sql
[0m21:49:04.593575 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
[0m21:49:04.605722 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
[0m21:49:04.606660 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
[0m21:49:04.607827 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
[0m21:49:04.608515 [debug] [MainThread]: Parsing macros/generic_tests/fewer_rows_than.sql
[0m21:49:04.609315 [debug] [MainThread]: Parsing macros/generic_tests/equal_rowcount.sql
[0m21:49:04.610059 [debug] [MainThread]: Parsing macros/generic_tests/relationships_where.sql
[0m21:49:04.611278 [debug] [MainThread]: Parsing macros/generic_tests/recency.sql
[0m21:49:04.612306 [debug] [MainThread]: Parsing macros/generic_tests/not_constant.sql
[0m21:49:04.612889 [debug] [MainThread]: Parsing macros/generic_tests/accepted_range.sql
[0m21:49:04.614329 [debug] [MainThread]: Parsing macros/generic_tests/not_accepted_values.sql
[0m21:49:04.615341 [debug] [MainThread]: Parsing macros/generic_tests/test_unique_where.sql
[0m21:49:04.615996 [debug] [MainThread]: Parsing macros/generic_tests/at_least_one.sql
[0m21:49:04.616561 [debug] [MainThread]: Parsing macros/generic_tests/unique_combination_of_columns.sql
[0m21:49:04.618130 [debug] [MainThread]: Parsing macros/generic_tests/cardinality_equality.sql
[0m21:49:04.619192 [debug] [MainThread]: Parsing macros/generic_tests/expression_is_true.sql
[0m21:49:04.620055 [debug] [MainThread]: Parsing macros/generic_tests/not_null_proportion.sql
[0m21:49:04.621168 [debug] [MainThread]: Parsing macros/generic_tests/sequential_values.sql
[0m21:49:04.622683 [debug] [MainThread]: Parsing macros/generic_tests/test_not_null_where.sql
[0m21:49:04.623328 [debug] [MainThread]: Parsing macros/generic_tests/equality.sql
[0m21:49:04.625046 [debug] [MainThread]: Parsing macros/generic_tests/mutually_exclusive_ranges.sql
[0m21:49:04.629529 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
[0m21:49:04.630033 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
[0m21:49:04.630570 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
[0m21:49:04.631051 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
[0m21:49:04.631566 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
[0m21:49:04.633481 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
[0m21:49:04.634266 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
[0m21:49:04.635870 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
[0m21:49:04.637842 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
[0m21:49:04.639480 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
[0m21:49:04.640265 [debug] [MainThread]: Parsing macros/sql/star.sql
[0m21:49:04.642012 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
[0m21:49:04.645812 [debug] [MainThread]: Parsing macros/sql/union.sql
[0m21:49:04.651358 [debug] [MainThread]: Parsing macros/sql/groupby.sql
[0m21:49:04.651985 [debug] [MainThread]: Parsing macros/sql/deduplicate.sql
[0m21:49:04.655312 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
[0m21:49:04.656846 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
[0m21:49:04.657562 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
[0m21:49:04.658262 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
[0m21:49:04.661379 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
[0m21:49:04.664029 [debug] [MainThread]: Parsing macros/sql/pivot.sql
[0m21:49:04.665965 [debug] [MainThread]: Parsing macros/sql/get_filtered_columns_in_relation.sql
[0m21:49:04.667200 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
[0m21:49:04.668231 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
[0m21:49:04.668971 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
[0m21:49:04.673221 [debug] [MainThread]: Parsing macros/predict.sql
[0m21:49:04.673653 [debug] [MainThread]: Parsing macros/recommend.sql
[0m21:49:04.674032 [debug] [MainThread]: Parsing macros/detect_anomalies.sql
[0m21:49:04.674333 [debug] [MainThread]: Parsing macros/hparam.sql
[0m21:49:04.674773 [debug] [MainThread]: Parsing macros/materializations/model.sql
[0m21:49:04.680734 [debug] [MainThread]: Parsing macros/hooks/model_audit.sql
[0m21:49:04.920631 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_4hr.sql
[0m21:49:04.929818 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_4hr.sql
[0m21:49:04.931035 [debug] [MainThread]: 1699: static parser successfully parsed example/ml_detect_tweaked.sql
[0m21:49:04.932693 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_12hr.sql
[0m21:49:04.937341 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_12hr.sql
[0m21:49:04.937946 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_max_RMSD_results.sql
[0m21:49:04.939216 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_8hr.sql
[0m21:49:04.943874 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_8hr.sql
[0m21:49:04.944453 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_8hr.sql
[0m21:49:04.949041 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_8hr.sql
[0m21:49:04.949660 [debug] [MainThread]: 1699: static parser successfully parsed example/count_cutoff.sql
[0m21:49:04.950775 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_features.sql
[0m21:49:04.952042 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_24hr.sql
[0m21:49:04.956576 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_24hr.sql
[0m21:49:04.957171 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_model_features_dbt.sql
[0m21:49:04.958640 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_min_anomalies_results.sql
[0m21:49:04.959910 [debug] [MainThread]: 1699: static parser successfully parsed example/dev_ml_model_constraints.sql
[0m21:49:04.961947 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_long.sql
[0m21:49:04.963259 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_model_features_dbt.sql
[0m21:49:04.964391 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_quartiles_short.sql
[0m21:49:04.965520 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_events_with_anomalies.sql
[0m21:49:04.966857 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_alerts_fired_automation_dbt.sql
[0m21:49:04.968124 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_all_models.sql
[0m21:49:04.969380 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies_results.sql
[0m21:49:04.970712 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies.sql
[0m21:49:04.972676 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_12hr.sql
[0m21:49:04.977435 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_12hr.sql
[0m21:49:04.978048 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_bounds_short.sql
[0m21:49:04.979213 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_short.sql
[0m21:49:04.980468 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_RMSD_results.sql
[0m21:49:04.981643 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_min_anomalies.sql
[0m21:49:04.982937 [debug] [MainThread]: 1699: static parser successfully parsed example/reference_derived.sql
[0m21:49:04.984011 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_bounds_long.sql
[0m21:49:04.985101 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_phase.sql
[0m21:49:04.986507 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_24hr.sql
[0m21:49:04.989393 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_24hr.sql
[0m21:49:04.989917 [debug] [MainThread]: 1603: static parser failed on example/derived_ml_detect.sql
[0m21:49:04.993329 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_ml_detect.sql
[0m21:49:04.993959 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_12hr.sql
[0m21:49:04.998793 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_12hr.sql
[0m21:49:04.999446 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_quartiles_long.sql
[0m21:49:05.000666 [debug] [MainThread]: 1699: static parser successfully parsed example/ref_distinct_tuples.sql
[0m21:49:05.001786 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_ref_including_LoBs.sql
[0m21:49:05.003066 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_RMSD.sql
[0m21:49:05.004304 [debug] [MainThread]: 1699: static parser successfully parsed example/all_agg_derived_cutoff_long.sql
[0m21:49:05.005401 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_nonrecent_events.sql
[0m21:49:05.006553 [debug] [MainThread]: 1699: static parser successfully parsed example/all_models.sql
[0m21:49:05.007684 [debug] [MainThread]: 1699: static parser successfully parsed example/all_agg_derived_cutoff_short.sql
[0m21:49:05.009898 [debug] [MainThread]: 1699: static parser successfully parsed example/all_agg_derived_cutoff.sql
[0m21:49:05.011064 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_features_null_filtered.sql
[0m21:49:05.012176 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events.sql
[0m21:49:05.013344 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_24hr.sql
[0m21:49:05.018235 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_24hr.sql
[0m21:49:05.018832 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_4hr.sql
[0m21:49:05.021628 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_4hr.sql
[0m21:49:05.022186 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_4hr.sql
[0m21:49:05.026655 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_4hr.sql
[0m21:49:05.027155 [debug] [MainThread]: 1603: static parser failed on example/all_agg_derived.sql
[0m21:49:05.030243 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/all_agg_derived.sql
[0m21:49:05.030798 [debug] [MainThread]: 1699: static parser successfully parsed example/trade_off_max_RMSD.sql
[0m21:49:05.031884 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_8hr.sql
[0m21:49:05.036336 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_8hr.sql
[0m21:49:05.055612 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'my_first_dbt_model' in the 'models' section of file 'models/example/schema.yml'
[0m21:49:05.080212 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.anomaly_detection.unique_my_first_dbt_model_id.16e066b321' (models/example/schema.yml) depends on a node named 'my_first_dbt_model' which was not found
[0m21:49:05.080416 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.anomaly_detection.not_null_my_first_dbt_model_id.5fb22c2710' (models/example/schema.yml) depends on a node named 'my_first_dbt_model' which was not found
[0m21:49:05.102737 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c157fc0c-1e50-4e3c-af11-9caeb078be17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1127270d0>]}
[0m21:49:05.110763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c157fc0c-1e50-4e3c-af11-9caeb078be17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11260b3a0>]}
[0m21:49:05.110940 [info ] [MainThread]: Found 48 models, 0 tests, 0 snapshots, 0 analyses, 540 macros, 0 operations, 0 seed files, 2 sources, 0 exposures, 0 metrics
[0m21:49:05.111061 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c157fc0c-1e50-4e3c-af11-9caeb078be17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112275ca0>]}
[0m21:49:05.112414 [info ] [MainThread]: 
[0m21:49:05.112649 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m21:49:05.114073 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow_dbt_anomaly_detection"
[0m21:49:05.114188 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:49:06.065178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c157fc0c-1e50-4e3c-af11-9caeb078be17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112231ee0>]}
[0m21:49:06.066193 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:49:06.066574 [info ] [MainThread]: 
[0m21:49:06.071080 [debug] [Thread-1  ]: Began running node model.anomaly_detection.reference_derived
[0m21:49:06.071679 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.reference_derived"
[0m21:49:06.071824 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.reference_derived
[0m21:49:06.072028 [debug] [Thread-1  ]: Compiling model.anomaly_detection.reference_derived
[0m21:49:06.076520 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.reference_derived"
[0m21:49:06.077283 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.077434 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.reference_derived
[0m21:49:06.077559 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.077934 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.reference_derived
[0m21:49:06.078346 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ref_including_LoBs
[0m21:49:06.078820 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_ref_including_LoBs"
[0m21:49:06.078987 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_ref_including_LoBs
[0m21:49:06.079074 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_ref_including_LoBs
[0m21:49:06.082152 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_ref_including_LoBs"
[0m21:49:06.082670 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.082819 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_ref_including_LoBs
[0m21:49:06.082951 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.083319 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ref_including_LoBs
[0m21:49:06.083855 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived
[0m21:49:06.084279 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived"
[0m21:49:06.084442 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived
[0m21:49:06.084553 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived
[0m21:49:06.089070 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived"
[0m21:49:06.089515 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.089655 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived
[0m21:49:06.089775 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.090130 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived
[0m21:49:06.090674 [debug] [Thread-1  ]: Began running node model.anomaly_detection.count_cutoff
[0m21:49:06.091121 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.count_cutoff"
[0m21:49:06.091249 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.count_cutoff
[0m21:49:06.091356 [debug] [Thread-1  ]: Compiling model.anomaly_detection.count_cutoff
[0m21:49:06.096552 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.count_cutoff"
[0m21:49:06.096964 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.097092 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.count_cutoff
[0m21:49:06.097192 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.097489 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.count_cutoff
[0m21:49:06.097620 [debug] [Thread-1  ]: Began running node model.anomaly_detection.dev_ml_model_constraints
[0m21:49:06.097921 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.dev_ml_model_constraints"
[0m21:49:06.098015 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.dev_ml_model_constraints
[0m21:49:06.098122 [debug] [Thread-1  ]: Compiling model.anomaly_detection.dev_ml_model_constraints
[0m21:49:06.101484 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.dev_ml_model_constraints"
[0m21:49:06.119112 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.119575 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.dev_ml_model_constraints
[0m21:49:06.119705 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff
[0m21:49:06.119968 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff"
[0m21:49:06.120061 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff
[0m21:49:06.120145 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff
[0m21:49:06.122603 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m21:49:06.122960 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.123052 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff
[0m21:49:06.123130 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.123370 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff
[0m21:49:06.123768 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m21:49:06.124060 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m21:49:06.124159 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_long
[0m21:49:06.124243 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_long
[0m21:49:06.126483 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m21:49:06.126770 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.126860 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_long
[0m21:49:06.126940 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.127166 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m21:49:06.127260 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m21:49:06.127497 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m21:49:06.127573 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_short
[0m21:49:06.127639 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_short
[0m21:49:06.129500 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m21:49:06.129740 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.129823 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_short
[0m21:49:06.129897 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.130105 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m21:49:06.130201 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_nonrecent_events
[0m21:49:06.130437 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_nonrecent_events"
[0m21:49:06.130525 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_nonrecent_events
[0m21:49:06.130598 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_nonrecent_events
[0m21:49:06.132425 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m21:49:06.132634 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.132706 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_nonrecent_events
[0m21:49:06.132771 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.132954 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_nonrecent_events
[0m21:49:06.133034 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_long
[0m21:49:06.133204 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_long"
[0m21:49:06.133270 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_long
[0m21:49:06.133330 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_long
[0m21:49:06.135217 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m21:49:06.135497 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.135579 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_long
[0m21:49:06.135651 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.135860 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_long
[0m21:49:06.135949 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_short
[0m21:49:06.136256 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_short"
[0m21:49:06.136363 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_short
[0m21:49:06.136434 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_short
[0m21:49:06.138937 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m21:49:06.139172 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.139249 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_short
[0m21:49:06.139319 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.139520 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_short
[0m21:49:06.139610 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_long
[0m21:49:06.139944 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_long"
[0m21:49:06.140057 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_long
[0m21:49:06.140153 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_long
[0m21:49:06.142267 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m21:49:06.142593 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.142685 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_long
[0m21:49:06.142764 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.142986 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_long
[0m21:49:06.143090 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_short
[0m21:49:06.143332 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_short"
[0m21:49:06.143409 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_short
[0m21:49:06.143482 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_short
[0m21:49:06.145148 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m21:49:06.145378 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.145463 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_short
[0m21:49:06.145527 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.145727 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_short
[0m21:49:06.145807 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_long
[0m21:49:06.146095 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_long"
[0m21:49:06.146192 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_long
[0m21:49:06.146259 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_long
[0m21:49:06.148104 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m21:49:06.148299 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.148371 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_long
[0m21:49:06.148432 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.148616 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_long
[0m21:49:06.148693 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_short
[0m21:49:06.149019 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_short"
[0m21:49:06.149120 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_short
[0m21:49:06.149187 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_short
[0m21:49:06.151036 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m21:49:06.151287 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.151361 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_short
[0m21:49:06.151426 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.151618 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_short
[0m21:49:06.151703 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_4hr
[0m21:49:06.151980 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_4hr"
[0m21:49:06.152058 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_4hr
[0m21:49:06.152119 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_4hr
[0m21:49:06.164864 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m21:49:06.165152 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.165241 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_4hr
[0m21:49:06.165301 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.165507 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_4hr
[0m21:49:06.165590 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_8hr
[0m21:49:06.165771 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_8hr"
[0m21:49:06.165852 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_8hr
[0m21:49:06.165917 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_8hr
[0m21:49:06.174509 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m21:49:06.174779 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.174849 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_8hr
[0m21:49:06.174910 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.175096 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_8hr
[0m21:49:06.175174 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_4hr
[0m21:49:06.175294 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_4hr"
[0m21:49:06.175355 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_4hr
[0m21:49:06.175410 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_4hr
[0m21:49:06.177633 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m21:49:06.177962 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.178069 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_4hr
[0m21:49:06.178142 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.178369 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_4hr
[0m21:49:06.178466 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_8hr
[0m21:49:06.178612 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_8hr"
[0m21:49:06.178682 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_8hr
[0m21:49:06.178741 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_8hr
[0m21:49:06.187353 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m21:49:06.187642 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.187745 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_8hr
[0m21:49:06.187813 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.188010 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_8hr
[0m21:49:06.188095 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_4hr
[0m21:49:06.188328 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_4hr"
[0m21:49:06.188412 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_4hr
[0m21:49:06.188471 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_4hr
[0m21:49:06.197288 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m21:49:06.197583 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.197663 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_4hr
[0m21:49:06.197733 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.197958 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_4hr
[0m21:49:06.198045 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_8hr
[0m21:49:06.198246 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_8hr"
[0m21:49:06.198338 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_8hr
[0m21:49:06.198406 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_8hr
[0m21:49:06.207983 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m21:49:06.208335 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.208430 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_8hr
[0m21:49:06.208508 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.208751 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_8hr
[0m21:49:06.208848 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_12hr
[0m21:49:06.209139 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_12hr"
[0m21:49:06.209253 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_12hr
[0m21:49:06.209322 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_12hr
[0m21:49:06.218229 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m21:49:06.218579 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.218670 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_12hr
[0m21:49:06.218753 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.218983 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_12hr
[0m21:49:06.219206 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_24hr
[0m21:49:06.219410 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_24hr"
[0m21:49:06.219502 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_24hr
[0m21:49:06.219661 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_24hr
[0m21:49:06.227931 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m21:49:06.228171 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.228240 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_24hr
[0m21:49:06.228299 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.228492 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_24hr
[0m21:49:06.228568 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_12hr
[0m21:49:06.228806 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_12hr"
[0m21:49:06.228908 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_12hr
[0m21:49:06.228977 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_12hr
[0m21:49:06.237617 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m21:49:06.237938 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.238045 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_12hr
[0m21:49:06.238212 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.238482 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_12hr
[0m21:49:06.238582 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_24hr
[0m21:49:06.238827 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_24hr"
[0m21:49:06.238909 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_24hr
[0m21:49:06.238969 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_24hr
[0m21:49:06.241246 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m21:49:06.241458 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.241527 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_24hr
[0m21:49:06.241589 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.241770 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_24hr
[0m21:49:06.241844 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_12hr
[0m21:49:06.241993 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_12hr"
[0m21:49:06.242054 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_12hr
[0m21:49:06.242108 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_12hr
[0m21:49:06.251118 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m21:49:06.251472 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.251605 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_12hr
[0m21:49:06.251675 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.251903 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_12hr
[0m21:49:06.251992 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_24hr
[0m21:49:06.252250 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_24hr"
[0m21:49:06.252361 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_24hr
[0m21:49:06.252429 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_24hr
[0m21:49:06.261381 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m21:49:06.261692 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.261769 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_24hr
[0m21:49:06.261837 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.262066 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_24hr
[0m21:49:06.265964 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ml_detect
[0m21:49:06.266223 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_ml_detect"
[0m21:49:06.266304 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_ml_detect
[0m21:49:06.266368 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_ml_detect
[0m21:49:06.269194 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_ml_detect"
[0m21:49:06.269467 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.269539 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_ml_detect
[0m21:49:06.269602 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.269788 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ml_detect
[0m21:49:06.270135 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ml_detect_tweaked
[0m21:49:06.270420 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ml_detect_tweaked"
[0m21:49:06.270503 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ml_detect_tweaked
[0m21:49:06.270569 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ml_detect_tweaked
[0m21:49:06.272335 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ml_detect_tweaked"
[0m21:49:06.272594 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.272669 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ml_detect_tweaked
[0m21:49:06.272732 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.272921 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ml_detect_tweaked
[0m21:49:06.273121 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_model_features_dbt
[0m21:49:06.273321 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_model_features_dbt"
[0m21:49:06.273395 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_model_features_dbt
[0m21:49:06.273455 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_model_features_dbt
[0m21:49:06.275005 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_model_features_dbt"
[0m21:49:06.275220 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.275294 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_model_features_dbt
[0m21:49:06.275378 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.275557 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_model_features_dbt
[0m21:49:06.275738 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_model_features_dbt
[0m21:49:06.275933 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_model_features_dbt"
[0m21:49:06.276013 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_model_features_dbt
[0m21:49:06.276073 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_model_features_dbt
[0m21:49:06.278247 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_model_features_dbt"
[0m21:49:06.278466 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.278535 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_model_features_dbt
[0m21:49:06.278597 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.278781 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_model_features_dbt
[0m21:49:06.278989 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ref_distinct_tuples
[0m21:49:06.279126 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ref_distinct_tuples"
[0m21:49:06.279186 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ref_distinct_tuples
[0m21:49:06.279238 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ref_distinct_tuples
[0m21:49:06.280891 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ref_distinct_tuples"
[0m21:49:06.281082 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.281148 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ref_distinct_tuples
[0m21:49:06.281206 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.281382 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ref_distinct_tuples
[0m21:49:06.281466 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_phase
[0m21:49:06.281590 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.trade_off_phase"
[0m21:49:06.281722 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.trade_off_phase
[0m21:49:06.281805 [debug] [Thread-1  ]: Compiling model.anomaly_detection.trade_off_phase
[0m21:49:06.283379 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.trade_off_phase"
[0m21:49:06.283597 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.283670 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.trade_off_phase
[0m21:49:06.283732 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.283925 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_phase
[0m21:49:06.284102 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_min_anomalies
[0m21:49:06.284266 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.trade_off_min_anomalies"
[0m21:49:06.284332 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.trade_off_min_anomalies
[0m21:49:06.284388 [debug] [Thread-1  ]: Compiling model.anomaly_detection.trade_off_min_anomalies
[0m21:49:06.285954 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.trade_off_min_anomalies"
[0m21:49:06.286238 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.286316 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.trade_off_min_anomalies
[0m21:49:06.286381 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.286565 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_min_anomalies
[0m21:49:06.286754 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_min_anomalies_results
[0m21:49:06.286937 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.trade_off_min_anomalies_results"
[0m21:49:06.287005 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.trade_off_min_anomalies_results
[0m21:49:06.287063 [debug] [Thread-1  ]: Compiling model.anomaly_detection.trade_off_min_anomalies_results
[0m21:49:06.288714 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.trade_off_min_anomalies_results"
[0m21:49:06.288916 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.288985 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.trade_off_min_anomalies_results
[0m21:49:06.289051 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.289223 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_min_anomalies_results
[0m21:49:06.289430 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_max_RMSD
[0m21:49:06.289605 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.trade_off_max_RMSD"
[0m21:49:06.289671 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.trade_off_max_RMSD
[0m21:49:06.289730 [debug] [Thread-1  ]: Compiling model.anomaly_detection.trade_off_max_RMSD
[0m21:49:06.291237 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.trade_off_max_RMSD"
[0m21:49:06.291485 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.291600 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.trade_off_max_RMSD
[0m21:49:06.291670 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.291857 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_max_RMSD
[0m21:49:06.292071 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_max_RMSD_results
[0m21:49:06.292300 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.trade_off_max_RMSD_results"
[0m21:49:06.292369 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.trade_off_max_RMSD_results
[0m21:49:06.292429 [debug] [Thread-1  ]: Compiling model.anomaly_detection.trade_off_max_RMSD_results
[0m21:49:06.294829 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.trade_off_max_RMSD_results"
[0m21:49:06.295054 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.295124 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.trade_off_max_RMSD_results
[0m21:49:06.295185 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.295390 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_max_RMSD_results
[0m21:49:06.295579 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events
[0m21:49:06.295750 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events"
[0m21:49:06.295812 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events
[0m21:49:06.295866 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events
[0m21:49:06.297457 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events"
[0m21:49:06.297673 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.297740 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events
[0m21:49:06.297801 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.297968 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events
[0m21:49:06.298135 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_features
[0m21:49:06.298318 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_features"
[0m21:49:06.298396 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_features
[0m21:49:06.298466 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_features
[0m21:49:06.300081 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_features"
[0m21:49:06.301090 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.301162 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_features
[0m21:49:06.301224 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.301413 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_features
[0m21:49:06.301656 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_features_null_filtered
[0m21:49:06.301840 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_features_null_filtered"
[0m21:49:06.301912 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_features_null_filtered
[0m21:49:06.301973 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_features_null_filtered
[0m21:49:06.303640 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_features_null_filtered"
[0m21:49:06.303876 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.303943 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_features_null_filtered
[0m21:49:06.304001 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.304178 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_features_null_filtered
[0m21:49:06.304412 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies
[0m21:49:06.304677 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies"
[0m21:49:06.304756 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies
[0m21:49:06.304825 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies
[0m21:49:06.306375 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies"
[0m21:49:06.306551 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.306614 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies
[0m21:49:06.306672 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.306838 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies
[0m21:49:06.307058 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m21:49:06.307275 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m21:49:06.307389 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies_results
[0m21:49:06.307477 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies_results
[0m21:49:06.309022 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m21:49:06.309181 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.309242 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies_results
[0m21:49:06.309297 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.309461 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m21:49:06.309630 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD
[0m21:49:06.309800 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD"
[0m21:49:06.309918 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD
[0m21:49:06.310017 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD
[0m21:49:06.311742 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD"
[0m21:49:06.312000 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.312072 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD
[0m21:49:06.312137 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.312323 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD
[0m21:49:06.312570 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m21:49:06.312741 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m21:49:06.312807 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD_results
[0m21:49:06.312863 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD_results
[0m21:49:06.315344 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m21:49:06.315612 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.315748 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD_results
[0m21:49:06.315815 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.316019 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m21:49:06.316265 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_models
[0m21:49:06.316508 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_models"
[0m21:49:06.316587 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_models
[0m21:49:06.316648 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_models
[0m21:49:06.318408 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_models"
[0m21:49:06.318665 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.318746 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_models
[0m21:49:06.318818 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.319013 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_models
[0m21:49:06.320294 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_all_models
[0m21:49:06.320538 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_all_models"
[0m21:49:06.320616 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_all_models
[0m21:49:06.320682 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_all_models
[0m21:49:06.323471 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_all_models"
[0m21:49:06.323758 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.323835 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_all_models
[0m21:49:06.323899 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.324103 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_all_models
[0m21:49:06.324381 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m21:49:06.324669 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m21:49:06.324750 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m21:49:06.324817 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m21:49:06.326767 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m21:49:06.327086 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.327164 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m21:49:06.327229 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.327431 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m21:49:06.327675 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_events_with_anomalies
[0m21:49:06.327956 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_events_with_anomalies"
[0m21:49:06.328066 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_events_with_anomalies
[0m21:49:06.328136 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_events_with_anomalies
[0m21:49:06.329861 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_events_with_anomalies"
[0m21:49:06.330141 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.330216 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_events_with_anomalies
[0m21:49:06.330284 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:06.330490 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_events_with_anomalies
[0m21:49:06.331018 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:49:06.331143 [debug] [MainThread]: Connection 'model.anomaly_detection.derived_events_with_anomalies' was properly closed.
[0m21:49:06.373975 [info ] [MainThread]: Done.
[0m21:49:06.374215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112509250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112509130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112509220>]}
[0m21:49:06.374340 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 21:49:27.029910 | 983c995d-84f6-4fb5-8fc3-d79a104fa7cf ==============================
[0m21:49:27.029950 [info ] [MainThread]: Running with dbt=1.2.1
[0m21:49:27.030771 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:49:27.030878 [debug] [MainThread]: Tracking: tracking
[0m21:49:27.038511 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075449a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107544a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075445e0>]}
[0m21:49:27.100652 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:49:27.100805 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:49:27.105471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10795f0d0>]}
[0m21:49:27.113408 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107802070>]}
[0m21:49:27.113684 [info ] [MainThread]: Found 48 models, 0 tests, 0 snapshots, 0 analyses, 540 macros, 0 operations, 0 seed files, 2 sources, 0 exposures, 0 metrics
[0m21:49:27.113840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107802040>]}
[0m21:49:27.115382 [info ] [MainThread]: 
[0m21:49:27.115630 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m21:49:27.117188 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow"
[0m21:49:27.117415 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:49:28.290536 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow_dbt_anomaly_detection"
[0m21:49:28.290856 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:49:28.684183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10757ac40>]}
[0m21:49:28.684650 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:49:28.684834 [info ] [MainThread]: 
[0m21:49:28.690454 [debug] [Thread-1  ]: Began running node model.anomaly_detection.reference_derived
[0m21:49:28.690734 [info ] [Thread-1  ]: 1 of 47 START table model dbt_anomaly_detection.reference_derived .............. [RUN]
[0m21:49:28.691095 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.reference_derived"
[0m21:49:28.691200 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.reference_derived
[0m21:49:28.691368 [debug] [Thread-1  ]: Compiling model.anomaly_detection.reference_derived
[0m21:49:28.695186 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.reference_derived"
[0m21:49:28.695617 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:28.695732 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.reference_derived
[0m21:49:28.718030 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.reference_derived"
[0m21:49:28.718610 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:49:28.718750 [debug] [Thread-1  ]: On model.anomaly_detection.reference_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.reference_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived`
  
  
  OPTIONS()
  as (
    
SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`web_purchase_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY) AND DATE(collector_tstamp) < CURRENT_DATE()

UNION ALL

SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`pco_web_apply_filter_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY) AND DATE(collector_tstamp) < CURRENT_DATE()
  );
  
[0m21:49:33.663103 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:33.663518 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1079e7e50>]}
[0m21:49:33.663771 [info ] [Thread-1  ]: 1 of 47 OK created table model dbt_anomaly_detection.reference_derived ......... [[32mCREATE TABLE (1.1m rows, 34.8 MB processed)[0m in 4.97s]
[0m21:49:33.664058 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.reference_derived
[0m21:49:33.664613 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ref_including_LoBs
[0m21:49:33.664904 [info ] [Thread-1  ]: 2 of 47 START table model dbt_anomaly_detection.derived_ref_including_LoBs ..... [RUN]
[0m21:49:33.665265 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_ref_including_LoBs"
[0m21:49:33.665388 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_ref_including_LoBs
[0m21:49:33.665501 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_ref_including_LoBs
[0m21:49:33.668175 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_ref_including_LoBs"
[0m21:49:33.668640 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:33.668767 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_ref_including_LoBs
[0m21:49:33.670973 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_ref_including_LoBs"
[0m21:49:33.671323 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:49:33.671457 [debug] [Thread-1  ]: On model.anomaly_detection.derived_ref_including_LoBs: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_ref_including_LoBs"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs`
  
  
  OPTIONS()
  as (
    

SELECT collector_tstamp, event, user_event_name, app_id,
    CASE
    WHEN LOWER(app_id) LIKE '%pcexpress%' OR LOWER(app_id) LIKE '%pcx%' THEN "PCX"
    WHEN LOWER(app_id) LIKE '%sdm%' OR LOWER(app_id) LIKE '%beauty%' THEN "SDM Shop"
    WHEN LOWER(app_id) LIKE '%drx%' THEN "SDM Drx" 
    -- WHEN LOWER(page_urlhost) like '%pcoptimum.ca%' and (LOWER(app_id) like '%ios%' or LOWER(app_id) like '%android%') then 'PC Optimum' 
    -- not recommended to use page_urlhost for mobile apps
    WHEN LOWER(app_id) like '%pco%' THEN 'PC Optimum'
    -- WHEN LOWER(page_urlhost) like '%presidentschoice.ca%' THEN 'PC' -- placeholder for after the launch 
    -- WHEN LOWER(page_urlhost) like '%joefresh.com%' and LOWER(app_id) like '%ios%' then 'JF' -- placeholder for after the launch 
    WHEN LOWER(app_id) like '%jf%' THEN 'JF'
    ELSE app_id END AS LoB,
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived`
  );
  
[0m21:49:38.225405 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:38.227593 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b2d070>]}
[0m21:49:38.228091 [info ] [Thread-1  ]: 2 of 47 OK created table model dbt_anomaly_detection.derived_ref_including_LoBs  [[32mCREATE TABLE (1.1m rows, 34.8 MB processed)[0m in 4.56s]
[0m21:49:38.228455 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ref_including_LoBs
[0m21:49:38.228975 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived
[0m21:49:38.229269 [info ] [Thread-1  ]: 3 of 47 START table model dbt_anomaly_detection.all_agg_derived ................ [RUN]
[0m21:49:38.229611 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived"
[0m21:49:38.229740 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived
[0m21:49:38.229879 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived
[0m21:49:38.276822 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived"
[0m21:49:38.277269 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:38.277376 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived
[0m21:49:38.279161 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived"
[0m21:49:38.279417 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:49:38.279524 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
  
  
  OPTIONS()
  as (
    


SELECT "4hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_4hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/4)*4 AS _4hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  WHERE app_id LIKE "%prod%"
  GROUP BY
    date_trunc,
    _4hr_trunc,
    app_id,
    LoB,
    event_type)

UNION ALL


SELECT "8hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_8hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/8)*8 AS _8hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  WHERE app_id LIKE "%prod%"
  GROUP BY
    date_trunc,
    _8hr_trunc,
    app_id,
    LoB,
    event_type)

UNION ALL


SELECT "12hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_12hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/12)*12 AS _12hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  WHERE app_id LIKE "%prod%"
  GROUP BY
    date_trunc,
    _12hr_trunc,
    app_id,
    LoB,
    event_type)

UNION ALL


SELECT "24hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_24hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/24)*24 AS _24hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  WHERE app_id LIKE "%prod%"
  GROUP BY
    date_trunc,
    _24hr_trunc,
    app_id,
    LoB,
    event_type)

ORDER BY
  agg_tag,
  time_stamps,
  app_id,
  LoB,
  event_type


  );
  
[0m21:49:41.105434 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:41.106226 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1079cf130>]}
[0m21:49:41.106610 [info ] [Thread-1  ]: 3 of 47 OK created table model dbt_anomaly_detection.all_agg_derived ........... [[32mCREATE TABLE (0.0 rows, 42.0 MB processed)[0m in 2.88s]
[0m21:49:41.106993 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived
[0m21:49:41.107855 [debug] [Thread-1  ]: Began running node model.anomaly_detection.count_cutoff
[0m21:49:41.108098 [info ] [Thread-1  ]: 4 of 47 START table model dbt_anomaly_detection.count_cutoff ................... [RUN]
[0m21:49:41.108479 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.count_cutoff"
[0m21:49:41.108622 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.count_cutoff
[0m21:49:41.108760 [debug] [Thread-1  ]: Compiling model.anomaly_detection.count_cutoff
[0m21:49:41.113835 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.count_cutoff"
[0m21:49:41.115439 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:41.115638 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.count_cutoff
[0m21:49:41.118981 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.count_cutoff"
[0m21:49:41.119756 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:49:41.119991 [debug] [Thread-1  ]: On model.anomaly_detection.count_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.count_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, LoB, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select agg_tag, app_event, LoB, min(time_stamps) as strt_time
from pairs
where event_count > 50
group by app_event, LoB, agg_tag 
order by app_event, LoB, agg_tag
  );
  
[0m21:49:43.842821 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:43.843193 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b5ff40>]}
[0m21:49:43.843576 [info ] [Thread-1  ]: 4 of 47 OK created table model dbt_anomaly_detection.count_cutoff .............. [[32mCREATE TABLE (0.0 rows, 0 processed)[0m in 2.73s]
[0m21:49:43.844101 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.count_cutoff
[0m21:49:43.844277 [debug] [Thread-1  ]: Began running node model.anomaly_detection.dev_ml_model_constraints
[0m21:49:43.844695 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.dev_ml_model_constraints"
[0m21:49:43.844818 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.dev_ml_model_constraints
[0m21:49:43.844899 [debug] [Thread-1  ]: Compiling model.anomaly_detection.dev_ml_model_constraints
[0m21:49:43.847231 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.dev_ml_model_constraints"
[0m21:49:43.847581 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:43.847819 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.dev_ml_model_constraints
[0m21:49:43.847916 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff
[0m21:49:43.848030 [info ] [Thread-1  ]: 5 of 47 START table model dbt_anomaly_detection.all_agg_derived_cutoff ......... [RUN]
[0m21:49:43.848442 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff"
[0m21:49:43.848594 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff
[0m21:49:43.848676 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff
[0m21:49:43.852127 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m21:49:43.852468 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:43.852556 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff
[0m21:49:43.855403 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m21:49:43.855800 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:49:43.855953 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, LoB, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select time_stamps, app_event, LoB, agg_tag, event_count
from(
select time_stamps, strt_time, pairs.app_event, pairs.LoB, pairs.agg_tag, event_count
from pairs
inner join `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff` as cutoff
on pairs.agg_tag = cutoff.agg_tag
and pairs.app_event = cutoff.app_event
and pairs.LoB = cutoff.LoB
order by pairs.app_event, pairs.LoB, pairs.agg_tag, time_stamps)
where time_stamps >= strt_time
  );
  
[0m21:49:46.577873 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:46.578830 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074b9970>]}
[0m21:49:46.579222 [info ] [Thread-1  ]: 5 of 47 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff .... [[32mCREATE TABLE (0.0 rows, 0 processed)[0m in 2.73s]
[0m21:49:46.579620 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff
[0m21:49:46.580107 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m21:49:46.580473 [info ] [Thread-1  ]: 6 of 47 START table model dbt_anomaly_detection.all_agg_derived_cutoff_long .... [RUN]
[0m21:49:46.580976 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m21:49:46.581085 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_long
[0m21:49:46.581182 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_long
[0m21:49:46.584606 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m21:49:46.585044 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:46.585155 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_long
[0m21:49:46.587332 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m21:49:46.587619 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:49:46.587741 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB(CURRENT_DATE(), INTERVAL 10 DAY)
  );
  
[0m21:49:48.959646 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:48.961117 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075016d0>]}
[0m21:49:48.962669 [info ] [Thread-1  ]: 6 of 47 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_long  [[32mCREATE TABLE (0.0 rows, 0 processed)[0m in 2.38s]
[0m21:49:48.964012 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m21:49:48.964483 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m21:49:48.966504 [info ] [Thread-1  ]: 7 of 47 START table model dbt_anomaly_detection.all_agg_derived_cutoff_short ... [RUN]
[0m21:49:48.967878 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m21:49:48.968235 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_short
[0m21:49:48.968510 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_short
[0m21:49:48.978155 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m21:49:48.978618 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:48.978719 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_short
[0m21:49:48.980780 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m21:49:48.981158 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:49:48.981272 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB(CURRENT_DATE(), INTERVAL 15 DAY)
  );
  
[0m21:49:51.323120 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:51.324435 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b70f70>]}
[0m21:49:51.325414 [info ] [Thread-1  ]: 7 of 47 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_short  [[32mCREATE TABLE (0.0 rows, 0 processed)[0m in 2.36s]
[0m21:49:51.326347 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m21:49:51.326813 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_nonrecent_events
[0m21:49:51.327368 [info ] [Thread-1  ]: 8 of 47 START table model dbt_anomaly_detection.derived_nonrecent_events ....... [RUN]
[0m21:49:51.328776 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_nonrecent_events"
[0m21:49:51.329668 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_nonrecent_events
[0m21:49:51.329968 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_nonrecent_events
[0m21:49:51.343233 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m21:49:51.344615 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:51.345429 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_nonrecent_events
[0m21:49:51.347661 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m21:49:51.348039 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:49:51.348166 [debug] [Thread-1  ]: On model.anomaly_detection.derived_nonrecent_events: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_nonrecent_events"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events`
  
  
  OPTIONS()
  as (
    

SELECT MIN(time_stamps) AS strt_time, app_event, LoB 
FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
GROUP BY app_event, LoB
HAVING DATE(MIN(time_stamps)) < DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
ORDER BY strt_time
  );
  
[0m21:49:53.771272 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:53.772085 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1079e7100>]}
[0m21:49:53.772511 [info ] [Thread-1  ]: 8 of 47 OK created table model dbt_anomaly_detection.derived_nonrecent_events .. [[32mCREATE TABLE (0.0 rows, 0 processed)[0m in 2.44s]
[0m21:49:53.772991 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_nonrecent_events
[0m21:49:53.773244 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_long
[0m21:49:53.774405 [info ] [Thread-1  ]: 9 of 47 START table model dbt_anomaly_detection.aggregation_quartiles_long ..... [RUN]
[0m21:49:53.776288 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_long"
[0m21:49:53.777083 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_long
[0m21:49:53.777429 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_long
[0m21:49:53.789269 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m21:49:53.789761 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:53.789851 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_long
[0m21:49:53.792195 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m21:49:53.792564 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:49:53.792676 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m21:49:56.457844 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:56.458172 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1079de550>]}
[0m21:49:56.458356 [info ] [Thread-1  ]: 9 of 47 OK created table model dbt_anomaly_detection.aggregation_quartiles_long  [[32mCREATE TABLE (0.0 rows, 0 processed)[0m in 2.68s]
[0m21:49:56.458540 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_long
[0m21:49:56.458626 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_short
[0m21:49:56.458854 [info ] [Thread-1  ]: 10 of 47 START table model dbt_anomaly_detection.aggregation_quartiles_short ... [RUN]
[0m21:49:56.459191 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_short"
[0m21:49:56.459289 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_short
[0m21:49:56.459359 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_short
[0m21:49:56.461933 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m21:49:56.462302 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:56.462374 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_short
[0m21:49:56.463886 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m21:49:56.464090 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:49:56.464182 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m21:49:59.203003 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:59.204199 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b99760>]}
[0m21:49:59.205009 [info ] [Thread-1  ]: 10 of 47 OK created table model dbt_anomaly_detection.aggregation_quartiles_short  [[32mCREATE TABLE (0.0 rows, 0 processed)[0m in 2.74s]
[0m21:49:59.205857 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_short
[0m21:49:59.206396 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_long
[0m21:49:59.206816 [info ] [Thread-1  ]: 11 of 47 START table model dbt_anomaly_detection.aggregation_bounds_long ....... [RUN]
[0m21:49:59.208587 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_long"
[0m21:49:59.209171 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_long
[0m21:49:59.209460 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_long
[0m21:49:59.217329 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m21:49:59.218341 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:59.218733 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_long
[0m21:49:59.222065 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m21:49:59.222417 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:49:59.222523 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m21:50:01.572070 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:01.572783 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074d0ca0>]}
[0m21:50:01.573523 [info ] [Thread-1  ]: 11 of 47 OK created table model dbt_anomaly_detection.aggregation_bounds_long .. [[32mCREATE TABLE (0.0 rows, 0 processed)[0m in 2.36s]
[0m21:50:01.575501 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_long
[0m21:50:01.576202 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_short
[0m21:50:01.576828 [info ] [Thread-1  ]: 12 of 47 START table model dbt_anomaly_detection.aggregation_bounds_short ...... [RUN]
[0m21:50:01.577701 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_short"
[0m21:50:01.578199 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_short
[0m21:50:01.578523 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_short
[0m21:50:01.589186 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m21:50:01.589638 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:01.589752 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_short
[0m21:50:01.591939 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m21:50:01.592307 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:50:01.592418 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m21:50:04.767775 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:04.768289 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1079c8e20>]}
[0m21:50:04.768536 [info ] [Thread-1  ]: 12 of 47 OK created table model dbt_anomaly_detection.aggregation_bounds_short . [[32mCREATE TABLE (0.0 rows, 0 processed)[0m in 3.19s]
[0m21:50:04.768744 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_short
[0m21:50:04.768840 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_long
[0m21:50:04.769001 [info ] [Thread-1  ]: 13 of 47 START table model dbt_anomaly_detection.aggregation_outliers_long ..... [RUN]
[0m21:50:04.769449 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_long"
[0m21:50:04.769530 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_long
[0m21:50:04.769599 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_long
[0m21:50:04.771476 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m21:50:04.771796 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:04.771867 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_long
[0m21:50:04.774555 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m21:50:04.774832 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:50:04.774936 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, LoB, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag, LoB,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m21:50:07.428139 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:07.429685 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bad760>]}
[0m21:50:07.429957 [info ] [Thread-1  ]: 13 of 47 OK created table model dbt_anomaly_detection.aggregation_outliers_long  [[32mCREATE TABLE (0.0 rows, 0 processed)[0m in 2.66s]
[0m21:50:07.430219 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_long
[0m21:50:07.430342 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_short
[0m21:50:07.430750 [info ] [Thread-1  ]: 14 of 47 START table model dbt_anomaly_detection.aggregation_outliers_short .... [RUN]
[0m21:50:07.431363 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_short"
[0m21:50:07.431457 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_short
[0m21:50:07.431544 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_short
[0m21:50:07.434079 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m21:50:07.434553 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:07.434679 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_short
[0m21:50:07.436869 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m21:50:07.437243 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:50:07.437371 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, LoB, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag, LoB,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m21:50:09.816901 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:09.817950 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b53880>]}
[0m21:50:09.818528 [info ] [Thread-1  ]: 14 of 47 OK created table model dbt_anomaly_detection.aggregation_outliers_short  [[32mCREATE TABLE (0.0 rows, 0 processed)[0m in 2.39s]
[0m21:50:09.818909 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_short
[0m21:50:09.819041 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_4hr
[0m21:50:09.819267 [info ] [Thread-1  ]: 15 of 47 START model model dbt_anomaly_detection.derived_models_05mon_4hr ...... [RUN]
[0m21:50:09.819693 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_4hr"
[0m21:50:09.819818 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_4hr
[0m21:50:09.819942 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_4hr
[0m21:50:09.837666 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m21:50:09.838137 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:09.838255 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_4hr
[0m21:50:09.849784 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m21:50:09.850139 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:50:09.850250 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH  __dbt__cte__dev_ml_model_constraints as (

-- This materialization is used to constrain the model during dev due to lack of data. Current Date is set to latest_date

WITH latest_earliest AS (
SELECT 
EXTRACT(DATE FROM (SELECT MAX(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS latest_date, -- always sep 21
EXTRACT(DATE FROM (SELECT MIN(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS earliest_date -- always sep 17
)
SELECT *,
-- CASE WHEN DATE_DIFF(latest_date, earliest_date, DAY) >= 35 
-- THEN 35 
     DATE_DIFF(latest_date, earliest_date, DAY) -- always 4
     AS training_interval
FROM latest_earliest
),date_control as (
SELECT 

(SELECT latest_date FROM __dbt__cte__dev_ml_model_constraints) AS cur_date

),
interval_control as (
SELECT 

(SELECT training_interval FROM __dbt__cte__dev_ml_model_constraints) AS training_interval

),
lookback as (
SELECT 

training_interval - 3 AS lookback_interval

FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30"
    );
    
[0m21:50:13.226308 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest("Input data doesn't contain any rows.")
[0m21:50:17.259759 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:17.261474 [debug] [Thread-1  ]: Database Error in model derived_models_05mon_4hr (models/example/derived_models_05mon_4hr.sql)
  Input data doesn't contain any rows.
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_4hr.sql
[0m21:50:17.261678 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b95c40>]}
[0m21:50:17.261923 [error] [Thread-1  ]: 15 of 47 ERROR creating model model dbt_anomaly_detection.derived_models_05mon_4hr  [[31mERROR[0m in 7.44s]
[0m21:50:17.262251 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_4hr
[0m21:50:17.262380 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_8hr
[0m21:50:17.262603 [info ] [Thread-1  ]: 16 of 47 START model model dbt_anomaly_detection.derived_models_05mon_8hr ...... [RUN]
[0m21:50:17.262872 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_8hr"
[0m21:50:17.262959 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_8hr
[0m21:50:17.263046 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_8hr
[0m21:50:17.276624 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m21:50:17.277148 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:17.277262 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_8hr
[0m21:50:17.279658 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m21:50:17.280016 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:50:17.280158 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH  __dbt__cte__dev_ml_model_constraints as (

-- This materialization is used to constrain the model during dev due to lack of data. Current Date is set to latest_date

WITH latest_earliest AS (
SELECT 
EXTRACT(DATE FROM (SELECT MAX(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS latest_date, -- always sep 21
EXTRACT(DATE FROM (SELECT MIN(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS earliest_date -- always sep 17
)
SELECT *,
-- CASE WHEN DATE_DIFF(latest_date, earliest_date, DAY) >= 35 
-- THEN 35 
     DATE_DIFF(latest_date, earliest_date, DAY) -- always 4
     AS training_interval
FROM latest_earliest
),date_control as (
SELECT 

(SELECT latest_date FROM __dbt__cte__dev_ml_model_constraints) AS cur_date

),
interval_control as (
SELECT 

(SELECT training_interval FROM __dbt__cte__dev_ml_model_constraints) AS training_interval

),
lookback as (
SELECT 

training_interval - 3 AS lookback_interval

FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30"
    );
    
[0m21:50:20.329214 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest("Input data doesn't contain any rows.")
[0m21:50:23.729431 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:23.730194 [debug] [Thread-1  ]: Database Error in model derived_models_05mon_8hr (models/example/derived_models_05mon_8hr.sql)
  Input data doesn't contain any rows.
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_8hr.sql
[0m21:50:23.730985 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1079e7760>]}
[0m21:50:23.731314 [error] [Thread-1  ]: 16 of 47 ERROR creating model model dbt_anomaly_detection.derived_models_05mon_8hr  [[31mERROR[0m in 6.47s]
[0m21:50:23.731552 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_8hr
[0m21:50:23.731666 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_4hr
[0m21:50:23.731874 [info ] [Thread-1  ]: 17 of 47 START model model dbt_anomaly_detection.derived_models_1mon_4hr ....... [RUN]
[0m21:50:23.732194 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_4hr"
[0m21:50:23.732289 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_4hr
[0m21:50:23.732371 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_4hr
[0m21:50:23.735414 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m21:50:23.735795 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:23.735888 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_4hr
[0m21:50:23.737668 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m21:50:23.737898 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:50:23.738005 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m21:50:26.592638 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest("Input data doesn't contain any rows.")
[0m21:50:30.222305 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:30.223065 [debug] [Thread-1  ]: Database Error in model derived_models_1mon_4hr (models/example/derived_models_1mon_4hr.sql)
  Input data doesn't contain any rows.
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_4hr.sql
[0m21:50:30.223457 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11107ea90>]}
[0m21:50:30.223941 [error] [Thread-1  ]: 17 of 47 ERROR creating model model dbt_anomaly_detection.derived_models_1mon_4hr  [[31mERROR[0m in 6.49s]
[0m21:50:30.224652 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_4hr
[0m21:50:30.225053 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_8hr
[0m21:50:30.225407 [info ] [Thread-1  ]: 18 of 47 START model model dbt_anomaly_detection.derived_models_1mon_8hr ....... [RUN]
[0m21:50:30.225738 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_8hr"
[0m21:50:30.225830 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_8hr
[0m21:50:30.225918 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_8hr
[0m21:50:30.237303 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m21:50:30.237767 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:30.237866 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_8hr
[0m21:50:30.239972 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m21:50:30.240387 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:50:30.240509 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH  __dbt__cte__dev_ml_model_constraints as (

-- This materialization is used to constrain the model during dev due to lack of data. Current Date is set to latest_date

WITH latest_earliest AS (
SELECT 
EXTRACT(DATE FROM (SELECT MAX(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS latest_date, -- always sep 21
EXTRACT(DATE FROM (SELECT MIN(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS earliest_date -- always sep 17
)
SELECT *,
-- CASE WHEN DATE_DIFF(latest_date, earliest_date, DAY) >= 35 
-- THEN 35 
     DATE_DIFF(latest_date, earliest_date, DAY) -- always 4
     AS training_interval
FROM latest_earliest
),date_control as (
SELECT 

(SELECT latest_date FROM __dbt__cte__dev_ml_model_constraints) AS cur_date

),
interval_control as (
SELECT 

(SELECT training_interval FROM __dbt__cte__dev_ml_model_constraints) AS training_interval

),
lookback as (
SELECT 

training_interval - 3 AS lookback_interval

FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30"
    );
    
[0m21:50:33.530340 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest("Input data doesn't contain any rows.")
[0m21:50:36.737056 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:36.738058 [debug] [Thread-1  ]: Database Error in model derived_models_1mon_8hr (models/example/derived_models_1mon_8hr.sql)
  Input data doesn't contain any rows.
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_8hr.sql
[0m21:50:36.738443 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111053b20>]}
[0m21:50:36.738941 [error] [Thread-1  ]: 18 of 47 ERROR creating model model dbt_anomaly_detection.derived_models_1mon_8hr  [[31mERROR[0m in 6.51s]
[0m21:50:36.739482 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_8hr
[0m21:50:36.739820 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_4hr
[0m21:50:36.740249 [info ] [Thread-1  ]: 19 of 47 START model model dbt_anomaly_detection.derived_models_2mon_4hr ....... [RUN]
[0m21:50:36.740639 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_4hr"
[0m21:50:36.740742 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_4hr
[0m21:50:36.740826 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_4hr
[0m21:50:36.752388 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m21:50:36.752807 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:36.752904 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_4hr
[0m21:50:36.754775 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m21:50:36.755033 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:50:36.755149 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH  __dbt__cte__dev_ml_model_constraints as (

-- This materialization is used to constrain the model during dev due to lack of data. Current Date is set to latest_date

WITH latest_earliest AS (
SELECT 
EXTRACT(DATE FROM (SELECT MAX(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS latest_date, -- always sep 21
EXTRACT(DATE FROM (SELECT MIN(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS earliest_date -- always sep 17
)
SELECT *,
-- CASE WHEN DATE_DIFF(latest_date, earliest_date, DAY) >= 35 
-- THEN 35 
     DATE_DIFF(latest_date, earliest_date, DAY) -- always 4
     AS training_interval
FROM latest_earliest
),date_control as (
SELECT 

(SELECT latest_date FROM __dbt__cte__dev_ml_model_constraints) AS cur_date

),
interval_control as (
SELECT 

(SELECT training_interval FROM __dbt__cte__dev_ml_model_constraints) AS training_interval

),
lookback as (
SELECT 

training_interval - 3 AS lookback_interval

FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30"
    );
    
[0m21:50:39.994334 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest("Input data doesn't contain any rows.")
[0m21:50:44.289277 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:44.290422 [debug] [Thread-1  ]: Database Error in model derived_models_2mon_4hr (models/example/derived_models_2mon_4hr.sql)
  Input data doesn't contain any rows.
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_4hr.sql
[0m21:50:44.290843 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1110baf10>]}
[0m21:50:44.291418 [error] [Thread-1  ]: 19 of 47 ERROR creating model model dbt_anomaly_detection.derived_models_2mon_4hr  [[31mERROR[0m in 7.55s]
[0m21:50:44.292095 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_4hr
[0m21:50:44.292460 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_8hr
[0m21:50:44.292857 [info ] [Thread-1  ]: 20 of 47 START model model dbt_anomaly_detection.derived_models_2mon_8hr ....... [RUN]
[0m21:50:44.293316 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_8hr"
[0m21:50:44.293441 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_8hr
[0m21:50:44.293548 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_8hr
[0m21:50:44.307785 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m21:50:44.308286 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:44.308391 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_8hr
[0m21:50:44.311547 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m21:50:44.311930 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:50:44.312079 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH  __dbt__cte__dev_ml_model_constraints as (

-- This materialization is used to constrain the model during dev due to lack of data. Current Date is set to latest_date

WITH latest_earliest AS (
SELECT 
EXTRACT(DATE FROM (SELECT MAX(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS latest_date, -- always sep 21
EXTRACT(DATE FROM (SELECT MIN(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS earliest_date -- always sep 17
)
SELECT *,
-- CASE WHEN DATE_DIFF(latest_date, earliest_date, DAY) >= 35 
-- THEN 35 
     DATE_DIFF(latest_date, earliest_date, DAY) -- always 4
     AS training_interval
FROM latest_earliest
),date_control as (
SELECT 

(SELECT latest_date FROM __dbt__cte__dev_ml_model_constraints) AS cur_date

),
interval_control as (
SELECT 

(SELECT training_interval FROM __dbt__cte__dev_ml_model_constraints) AS training_interval

),
lookback as (
SELECT 

training_interval - 3 AS lookback_interval

FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30"
    );
    
[0m21:50:47.349401 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest("Input data doesn't contain any rows.")
[0m21:50:50.743230 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:50.744061 [debug] [Thread-1  ]: Database Error in model derived_models_2mon_8hr (models/example/derived_models_2mon_8hr.sql)
  Input data doesn't contain any rows.
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_8hr.sql
[0m21:50:50.744456 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1110ae340>]}
[0m21:50:50.744981 [error] [Thread-1  ]: 20 of 47 ERROR creating model model dbt_anomaly_detection.derived_models_2mon_8hr  [[31mERROR[0m in 6.45s]
[0m21:50:50.745373 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_8hr
[0m21:50:50.745501 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_12hr
[0m21:50:50.745754 [info ] [Thread-1  ]: 21 of 47 START model model dbt_anomaly_detection.derived_models_05mon_12hr ..... [RUN]
[0m21:50:50.746068 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_12hr"
[0m21:50:50.746167 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_12hr
[0m21:50:50.746249 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_12hr
[0m21:50:50.757643 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m21:50:50.758137 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:50.758249 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_12hr
[0m21:50:50.760481 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m21:50:50.760986 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:50:50.761191 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH  __dbt__cte__dev_ml_model_constraints as (

-- This materialization is used to constrain the model during dev due to lack of data. Current Date is set to latest_date

WITH latest_earliest AS (
SELECT 
EXTRACT(DATE FROM (SELECT MAX(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS latest_date, -- always sep 21
EXTRACT(DATE FROM (SELECT MIN(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS earliest_date -- always sep 17
)
SELECT *,
-- CASE WHEN DATE_DIFF(latest_date, earliest_date, DAY) >= 35 
-- THEN 35 
     DATE_DIFF(latest_date, earliest_date, DAY) -- always 4
     AS training_interval
FROM latest_earliest
),date_control as (
SELECT 

(SELECT latest_date FROM __dbt__cte__dev_ml_model_constraints) AS cur_date

),
interval_control as (
SELECT 

(SELECT training_interval FROM __dbt__cte__dev_ml_model_constraints) AS training_interval

),
lookback as (
SELECT 

training_interval - 3 AS lookback_interval

FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30"
    );
    
[0m21:50:53.955738 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest("Input data doesn't contain any rows.")
[0m21:50:57.001218 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:57.002217 [debug] [Thread-1  ]: Database Error in model derived_models_05mon_12hr (models/example/derived_models_05mon_12hr.sql)
  Input data doesn't contain any rows.
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_12hr.sql
[0m21:50:57.002592 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11113c340>]}
[0m21:50:57.003082 [error] [Thread-1  ]: 21 of 47 ERROR creating model model dbt_anomaly_detection.derived_models_05mon_12hr  [[31mERROR[0m in 6.26s]
[0m21:50:57.003812 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_12hr
[0m21:50:57.004114 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_24hr
[0m21:50:57.004449 [info ] [Thread-1  ]: 22 of 47 START model model dbt_anomaly_detection.derived_models_05mon_24hr ..... [RUN]
[0m21:50:57.004894 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_24hr"
[0m21:50:57.005007 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_24hr
[0m21:50:57.005107 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_24hr
[0m21:50:57.020448 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m21:50:57.021058 [debug] [Thread-1  ]: finished collecting timing info
[0m21:50:57.021212 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_24hr
[0m21:50:57.025033 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m21:50:57.025519 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:50:57.025695 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH  __dbt__cte__dev_ml_model_constraints as (

-- This materialization is used to constrain the model during dev due to lack of data. Current Date is set to latest_date

WITH latest_earliest AS (
SELECT 
EXTRACT(DATE FROM (SELECT MAX(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS latest_date, -- always sep 21
EXTRACT(DATE FROM (SELECT MIN(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS earliest_date -- always sep 17
)
SELECT *,
-- CASE WHEN DATE_DIFF(latest_date, earliest_date, DAY) >= 35 
-- THEN 35 
     DATE_DIFF(latest_date, earliest_date, DAY) -- always 4
     AS training_interval
FROM latest_earliest
),date_control as (
SELECT 

(SELECT latest_date FROM __dbt__cte__dev_ml_model_constraints) AS cur_date

),
interval_control as (
SELECT 

(SELECT training_interval FROM __dbt__cte__dev_ml_model_constraints) AS training_interval

),
lookback as (
SELECT 

training_interval - 3 AS lookback_interval

FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30"
    );
    
[0m21:51:00.170532 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest("Input data doesn't contain any rows.")
[0m21:51:03.284841 [debug] [Thread-1  ]: finished collecting timing info
[0m21:51:03.286492 [debug] [Thread-1  ]: Database Error in model derived_models_05mon_24hr (models/example/derived_models_05mon_24hr.sql)
  Input data doesn't contain any rows.
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_24hr.sql
[0m21:51:03.286942 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1110ae0d0>]}
[0m21:51:03.287503 [error] [Thread-1  ]: 22 of 47 ERROR creating model model dbt_anomaly_detection.derived_models_05mon_24hr  [[31mERROR[0m in 6.28s]
[0m21:51:03.288063 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_24hr
[0m21:51:03.288303 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_12hr
[0m21:51:03.288490 [info ] [Thread-1  ]: 23 of 47 START model model dbt_anomaly_detection.derived_models_1mon_12hr ...... [RUN]
[0m21:51:03.288966 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_12hr"
[0m21:51:03.289087 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_12hr
[0m21:51:03.289187 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_12hr
[0m21:51:03.303270 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m21:51:03.303888 [debug] [Thread-1  ]: finished collecting timing info
[0m21:51:03.304124 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_12hr
[0m21:51:03.306756 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m21:51:03.307204 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:51:03.307361 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH  __dbt__cte__dev_ml_model_constraints as (

-- This materialization is used to constrain the model during dev due to lack of data. Current Date is set to latest_date

WITH latest_earliest AS (
SELECT 
EXTRACT(DATE FROM (SELECT MAX(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS latest_date, -- always sep 21
EXTRACT(DATE FROM (SELECT MIN(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS earliest_date -- always sep 17
)
SELECT *,
-- CASE WHEN DATE_DIFF(latest_date, earliest_date, DAY) >= 35 
-- THEN 35 
     DATE_DIFF(latest_date, earliest_date, DAY) -- always 4
     AS training_interval
FROM latest_earliest
),date_control as (
SELECT 

(SELECT latest_date FROM __dbt__cte__dev_ml_model_constraints) AS cur_date

),
interval_control as (
SELECT 

(SELECT training_interval FROM __dbt__cte__dev_ml_model_constraints) AS training_interval

),
lookback as (
SELECT 

training_interval - 3 AS lookback_interval

FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30"
    );
    
[0m21:51:06.872509 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest("Input data doesn't contain any rows.")
[0m21:51:10.739145 [debug] [Thread-1  ]: finished collecting timing info
[0m21:51:10.739975 [debug] [Thread-1  ]: Database Error in model derived_models_1mon_12hr (models/example/derived_models_1mon_12hr.sql)
  Input data doesn't contain any rows.
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_12hr.sql
[0m21:51:10.740411 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111197fd0>]}
[0m21:51:10.740953 [error] [Thread-1  ]: 23 of 47 ERROR creating model model dbt_anomaly_detection.derived_models_1mon_12hr  [[31mERROR[0m in 7.45s]
[0m21:51:10.741375 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_12hr
[0m21:51:10.741496 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_24hr
[0m21:51:10.741716 [info ] [Thread-1  ]: 24 of 47 START model model dbt_anomaly_detection.derived_models_1mon_24hr ...... [RUN]
[0m21:51:10.742014 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_24hr"
[0m21:51:10.742103 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_24hr
[0m21:51:10.742181 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_24hr
[0m21:51:10.745109 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m21:51:10.745505 [debug] [Thread-1  ]: finished collecting timing info
[0m21:51:10.745597 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_24hr
[0m21:51:10.747557 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m21:51:10.747930 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:51:10.748054 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m21:51:13.383829 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest("Input data doesn't contain any rows.")
[0m21:51:16.635437 [debug] [Thread-1  ]: finished collecting timing info
[0m21:51:16.636264 [debug] [Thread-1  ]: Database Error in model derived_models_1mon_24hr (models/example/derived_models_1mon_24hr.sql)
  Input data doesn't contain any rows.
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_24hr.sql
[0m21:51:16.636705 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1111f12b0>]}
[0m21:51:16.636996 [error] [Thread-1  ]: 24 of 47 ERROR creating model model dbt_anomaly_detection.derived_models_1mon_24hr  [[31mERROR[0m in 5.89s]
[0m21:51:16.637246 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_24hr
[0m21:51:16.637362 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_12hr
[0m21:51:16.637776 [info ] [Thread-1  ]: 25 of 47 START model model dbt_anomaly_detection.derived_models_2mon_12hr ...... [RUN]
[0m21:51:16.638311 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_12hr"
[0m21:51:16.638411 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_12hr
[0m21:51:16.638582 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_12hr
[0m21:51:16.653218 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m21:51:16.653689 [debug] [Thread-1  ]: finished collecting timing info
[0m21:51:16.653795 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_12hr
[0m21:51:16.655750 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m21:51:16.656034 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:51:16.656152 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH  __dbt__cte__dev_ml_model_constraints as (

-- This materialization is used to constrain the model during dev due to lack of data. Current Date is set to latest_date

WITH latest_earliest AS (
SELECT 
EXTRACT(DATE FROM (SELECT MAX(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS latest_date, -- always sep 21
EXTRACT(DATE FROM (SELECT MIN(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS earliest_date -- always sep 17
)
SELECT *,
-- CASE WHEN DATE_DIFF(latest_date, earliest_date, DAY) >= 35 
-- THEN 35 
     DATE_DIFF(latest_date, earliest_date, DAY) -- always 4
     AS training_interval
FROM latest_earliest
),date_control as (
SELECT 

(SELECT latest_date FROM __dbt__cte__dev_ml_model_constraints) AS cur_date

),
interval_control as (
SELECT 

(SELECT training_interval FROM __dbt__cte__dev_ml_model_constraints) AS training_interval

),
lookback as (
SELECT 

training_interval - 3 AS lookback_interval

FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30"
    );
    
[0m21:51:19.695362 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest("Input data doesn't contain any rows.")
[0m21:51:23.738629 [debug] [Thread-1  ]: finished collecting timing info
[0m21:51:23.739431 [debug] [Thread-1  ]: Database Error in model derived_models_2mon_12hr (models/example/derived_models_2mon_12hr.sql)
  Input data doesn't contain any rows.
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_12hr.sql
[0m21:51:23.739812 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1111b5ee0>]}
[0m21:51:24.337627 [error] [Thread-1  ]: 25 of 47 ERROR creating model model dbt_anomaly_detection.derived_models_2mon_12hr  [[31mERROR[0m in 7.10s]
[0m21:51:24.338390 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_12hr
[0m21:51:24.338569 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_24hr
[0m21:51:24.338904 [info ] [Thread-1  ]: 26 of 47 START model model dbt_anomaly_detection.derived_models_2mon_24hr ...... [RUN]
[0m21:51:24.339512 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_24hr"
[0m21:51:24.339865 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_24hr
[0m21:51:24.340022 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_24hr
[0m21:51:24.356860 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m21:51:24.357749 [debug] [Thread-1  ]: finished collecting timing info
[0m21:51:24.357915 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_24hr
[0m21:51:24.361262 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m21:51:24.361976 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:51:24.362235 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH  __dbt__cte__dev_ml_model_constraints as (

-- This materialization is used to constrain the model during dev due to lack of data. Current Date is set to latest_date

WITH latest_earliest AS (
SELECT 
EXTRACT(DATE FROM (SELECT MAX(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS latest_date, -- always sep 21
EXTRACT(DATE FROM (SELECT MIN(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS earliest_date -- always sep 17
)
SELECT *,
-- CASE WHEN DATE_DIFF(latest_date, earliest_date, DAY) >= 35 
-- THEN 35 
     DATE_DIFF(latest_date, earliest_date, DAY) -- always 4
     AS training_interval
FROM latest_earliest
),date_control as (
SELECT 

(SELECT latest_date FROM __dbt__cte__dev_ml_model_constraints) AS cur_date

),
interval_control as (
SELECT 

(SELECT training_interval FROM __dbt__cte__dev_ml_model_constraints) AS training_interval

),
lookback as (
SELECT 

training_interval - 3 AS lookback_interval

FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30"
    );
    
[0m21:51:27.563220 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest("Input data doesn't contain any rows.")
[0m21:51:32.315384 [debug] [Thread-1  ]: finished collecting timing info
[0m21:51:32.318124 [debug] [Thread-1  ]: Database Error in model derived_models_2mon_24hr (models/example/derived_models_2mon_24hr.sql)
  Input data doesn't contain any rows.
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_24hr.sql
[0m21:51:32.318607 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '983c995d-84f6-4fb5-8fc3-d79a104fa7cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111229970>]}
[0m21:51:32.318951 [error] [Thread-1  ]: 26 of 47 ERROR creating model model dbt_anomaly_detection.derived_models_2mon_24hr  [[31mERROR[0m in 7.98s]
[0m21:51:32.319271 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_24hr
[0m21:51:32.319404 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ml_detect
[0m21:51:32.319741 [info ] [Thread-1  ]: 27 of 47 SKIP relation dbt_anomaly_detection.derived_ml_detect ................. [[33mSKIP[0m]
[0m21:51:32.320271 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ml_detect
[0m21:51:32.320875 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ml_detect_tweaked
[0m21:51:32.321140 [info ] [Thread-1  ]: 28 of 47 SKIP relation dbt_anomaly_detection.ml_detect_tweaked ................. [[33mSKIP[0m]
[0m21:51:32.321481 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ml_detect_tweaked
[0m21:51:32.321934 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_model_features_dbt
[0m21:51:32.322108 [info ] [Thread-1  ]: 29 of 47 SKIP relation dbt_anomaly_detection.derived_model_features_dbt ........ [[33mSKIP[0m]
[0m21:51:32.322370 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_model_features_dbt
[0m21:51:32.322878 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_model_features_dbt
[0m21:51:32.323065 [info ] [Thread-1  ]: 30 of 47 SKIP relation dbt_anomaly_detection.filtered_model_features_dbt ....... [[33mSKIP[0m]
[0m21:51:32.323386 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_model_features_dbt
[0m21:51:32.323877 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ref_distinct_tuples
[0m21:51:32.324268 [info ] [Thread-1  ]: 31 of 47 SKIP relation dbt_anomaly_detection.ref_distinct_tuples ............... [[33mSKIP[0m]
[0m21:51:32.324708 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ref_distinct_tuples
[0m21:51:32.324899 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_phase
[0m21:51:32.325024 [info ] [Thread-1  ]: 32 of 47 SKIP relation dbt_anomaly_detection.trade_off_phase ................... [[33mSKIP[0m]
[0m21:51:32.325675 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_phase
[0m21:51:32.326211 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_min_anomalies
[0m21:51:32.326488 [info ] [Thread-1  ]: 33 of 47 SKIP relation dbt_anomaly_detection.trade_off_min_anomalies ........... [[33mSKIP[0m]
[0m21:51:32.326859 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_min_anomalies
[0m21:51:32.327193 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_min_anomalies_results
[0m21:51:32.327436 [info ] [Thread-1  ]: 34 of 47 SKIP relation dbt_anomaly_detection.trade_off_min_anomalies_results ... [[33mSKIP[0m]
[0m21:51:32.327811 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_min_anomalies_results
[0m21:51:32.328268 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_max_RMSD
[0m21:51:32.328528 [info ] [Thread-1  ]: 35 of 47 SKIP relation dbt_anomaly_detection.trade_off_max_RMSD ................ [[33mSKIP[0m]
[0m21:51:32.328863 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_max_RMSD
[0m21:51:32.329288 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_max_RMSD_results
[0m21:51:32.329435 [info ] [Thread-1  ]: 36 of 47 SKIP relation dbt_anomaly_detection.trade_off_max_RMSD_results ........ [[33mSKIP[0m]
[0m21:51:32.329705 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_max_RMSD_results
[0m21:51:32.330031 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events
[0m21:51:32.330213 [info ] [Thread-1  ]: 37 of 47 SKIP relation dbt_anomaly_detection.remaining_events .................. [[33mSKIP[0m]
[0m21:51:32.330495 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events
[0m21:51:32.330949 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_features
[0m21:51:32.331107 [info ] [Thread-1  ]: 38 of 47 SKIP relation dbt_anomaly_detection.remaining_events_features ......... [[33mSKIP[0m]
[0m21:51:32.331416 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_features
[0m21:51:32.331841 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_features_null_filtered
[0m21:51:32.332079 [info ] [Thread-1  ]: 39 of 47 SKIP relation dbt_anomaly_detection.remaining_events_features_null_filtered  [[33mSKIP[0m]
[0m21:51:32.332415 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_features_null_filtered
[0m21:51:32.332727 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies
[0m21:51:32.332924 [info ] [Thread-1  ]: 40 of 47 SKIP relation dbt_anomaly_detection.remaining_events_min_anomalies .... [[33mSKIP[0m]
[0m21:51:32.333182 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies
[0m21:51:32.333614 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m21:51:32.333755 [info ] [Thread-1  ]: 41 of 47 SKIP relation dbt_anomaly_detection.remaining_events_min_anomalies_results  [[33mSKIP[0m]
[0m21:51:32.334181 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m21:51:32.334601 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD
[0m21:51:32.334832 [info ] [Thread-1  ]: 42 of 47 SKIP relation dbt_anomaly_detection.remaining_events_min_RMSD ......... [[33mSKIP[0m]
[0m21:51:32.335133 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD
[0m21:51:32.335437 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m21:51:32.335679 [info ] [Thread-1  ]: 43 of 47 SKIP relation dbt_anomaly_detection.remaining_events_min_RMSD_results . [[33mSKIP[0m]
[0m21:51:32.335933 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m21:51:32.336260 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_models
[0m21:51:32.336431 [info ] [Thread-1  ]: 44 of 47 SKIP relation dbt_anomaly_detection.all_models ........................ [[33mSKIP[0m]
[0m21:51:32.336669 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_models
[0m21:51:32.336998 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_all_models
[0m21:51:32.337130 [info ] [Thread-1  ]: 45 of 47 SKIP relation dbt_anomaly_detection.filtered_all_models ............... [[33mSKIP[0m]
[0m21:51:32.337384 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_all_models
[0m21:51:32.337660 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m21:51:32.337872 [info ] [Thread-1  ]: 46 of 47 SKIP relation dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [[33mSKIP[0m]
[0m21:51:32.338118 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m21:51:32.338352 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_events_with_anomalies
[0m21:51:32.338539 [info ] [Thread-1  ]: 47 of 47 SKIP relation dbt_anomaly_detection.derived_events_with_anomalies ..... [[33mSKIP[0m]
[0m21:51:32.338784 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_events_with_anomalies
[0m21:51:32.339691 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m21:51:32.340131 [info ] [MainThread]: 
[0m21:51:32.340327 [info ] [MainThread]: Finished running 35 table models, 12 model models in 0 hours 2 minutes and 5.22 seconds (125.22s).
[0m21:51:32.340502 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:51:32.340594 [debug] [MainThread]: Connection 'model.anomaly_detection.derived_models_2mon_24hr' was properly closed.
[0m21:51:32.352129 [info ] [MainThread]: 
[0m21:51:32.352343 [info ] [MainThread]: [31mCompleted with 12 errors and 0 warnings:[0m
[0m21:51:32.352508 [info ] [MainThread]: 
[0m21:51:32.352640 [error] [MainThread]: [33mDatabase Error in model derived_models_05mon_4hr (models/example/derived_models_05mon_4hr.sql)[0m
[0m21:51:32.352764 [error] [MainThread]:   Input data doesn't contain any rows.
[0m21:51:32.352883 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_4hr.sql
[0m21:51:32.353000 [info ] [MainThread]: 
[0m21:51:32.353120 [error] [MainThread]: [33mDatabase Error in model derived_models_05mon_8hr (models/example/derived_models_05mon_8hr.sql)[0m
[0m21:51:32.353301 [error] [MainThread]:   Input data doesn't contain any rows.
[0m21:51:32.353436 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_8hr.sql
[0m21:51:32.353563 [info ] [MainThread]: 
[0m21:51:32.353685 [error] [MainThread]: [33mDatabase Error in model derived_models_1mon_4hr (models/example/derived_models_1mon_4hr.sql)[0m
[0m21:51:32.353804 [error] [MainThread]:   Input data doesn't contain any rows.
[0m21:51:32.353919 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_4hr.sql
[0m21:51:32.354035 [info ] [MainThread]: 
[0m21:51:32.354154 [error] [MainThread]: [33mDatabase Error in model derived_models_1mon_8hr (models/example/derived_models_1mon_8hr.sql)[0m
[0m21:51:32.354267 [error] [MainThread]:   Input data doesn't contain any rows.
[0m21:51:32.354379 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_8hr.sql
[0m21:51:32.354492 [info ] [MainThread]: 
[0m21:51:32.354605 [error] [MainThread]: [33mDatabase Error in model derived_models_2mon_4hr (models/example/derived_models_2mon_4hr.sql)[0m
[0m21:51:32.354718 [error] [MainThread]:   Input data doesn't contain any rows.
[0m21:51:32.354829 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_4hr.sql
[0m21:51:32.354944 [info ] [MainThread]: 
[0m21:51:32.355059 [error] [MainThread]: [33mDatabase Error in model derived_models_2mon_8hr (models/example/derived_models_2mon_8hr.sql)[0m
[0m21:51:32.355172 [error] [MainThread]:   Input data doesn't contain any rows.
[0m21:51:32.355310 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_8hr.sql
[0m21:51:32.355502 [info ] [MainThread]: 
[0m21:51:32.355640 [error] [MainThread]: [33mDatabase Error in model derived_models_05mon_12hr (models/example/derived_models_05mon_12hr.sql)[0m
[0m21:51:32.355767 [error] [MainThread]:   Input data doesn't contain any rows.
[0m21:51:32.355885 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_12hr.sql
[0m21:51:32.356009 [info ] [MainThread]: 
[0m21:51:32.356130 [error] [MainThread]: [33mDatabase Error in model derived_models_05mon_24hr (models/example/derived_models_05mon_24hr.sql)[0m
[0m21:51:32.356247 [error] [MainThread]:   Input data doesn't contain any rows.
[0m21:51:32.356362 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_24hr.sql
[0m21:51:32.356480 [info ] [MainThread]: 
[0m21:51:32.356613 [error] [MainThread]: [33mDatabase Error in model derived_models_1mon_12hr (models/example/derived_models_1mon_12hr.sql)[0m
[0m21:51:32.356730 [error] [MainThread]:   Input data doesn't contain any rows.
[0m21:51:32.356884 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_12hr.sql
[0m21:51:32.357004 [info ] [MainThread]: 
[0m21:51:32.357163 [error] [MainThread]: [33mDatabase Error in model derived_models_1mon_24hr (models/example/derived_models_1mon_24hr.sql)[0m
[0m21:51:32.357312 [error] [MainThread]:   Input data doesn't contain any rows.
[0m21:51:32.357435 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_24hr.sql
[0m21:51:32.357557 [info ] [MainThread]: 
[0m21:51:32.357676 [error] [MainThread]: [33mDatabase Error in model derived_models_2mon_12hr (models/example/derived_models_2mon_12hr.sql)[0m
[0m21:51:32.357795 [error] [MainThread]:   Input data doesn't contain any rows.
[0m21:51:32.357956 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_12hr.sql
[0m21:51:32.358224 [info ] [MainThread]: 
[0m21:51:32.358366 [error] [MainThread]: [33mDatabase Error in model derived_models_2mon_24hr (models/example/derived_models_2mon_24hr.sql)[0m
[0m21:51:32.358498 [error] [MainThread]:   Input data doesn't contain any rows.
[0m21:51:32.358615 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_24hr.sql
[0m21:51:32.358771 [info ] [MainThread]: 
[0m21:51:32.358890 [info ] [MainThread]: Done. PASS=14 WARN=0 ERROR=12 SKIP=21 TOTAL=47
[0m21:51:32.359088 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1112464f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11124c130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1111b9c40>]}
[0m21:51:32.359239 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 21:56:29.469395 | 55747c7f-57e7-4d4e-a2a5-d950768aacc9 ==============================
[0m21:56:29.469437 [info ] [MainThread]: Running with dbt=1.2.1
[0m21:56:29.470269 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:56:29.470381 [debug] [MainThread]: Tracking: tracking
[0m21:56:29.483065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054b0820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054b0730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054b0670>]}
[0m21:56:29.540299 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:56:29.540633 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/all_agg_derived.sql
[0m21:56:29.548492 [debug] [MainThread]: 1603: static parser failed on example/all_agg_derived.sql
[0m21:56:29.555504 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/all_agg_derived.sql
[0m21:56:29.568780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058130d0>]}
[0m21:56:29.604827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053c24f0>]}
[0m21:56:29.605038 [info ] [MainThread]: Found 48 models, 0 tests, 0 snapshots, 0 analyses, 540 macros, 0 operations, 0 seed files, 2 sources, 0 exposures, 0 metrics
[0m21:56:29.605162 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105750520>]}
[0m21:56:29.606580 [info ] [MainThread]: 
[0m21:56:29.606835 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m21:56:29.608329 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow"
[0m21:56:29.608458 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:56:30.742566 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow_dbt_anomaly_detection"
[0m21:56:30.742831 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:56:31.028233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10423ab50>]}
[0m21:56:31.029281 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:56:31.029615 [info ] [MainThread]: 
[0m21:56:31.035580 [debug] [Thread-1  ]: Began running node model.anomaly_detection.reference_derived
[0m21:56:31.035871 [info ] [Thread-1  ]: 1 of 47 START table model dbt_anomaly_detection.reference_derived .............. [RUN]
[0m21:56:31.036238 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.reference_derived"
[0m21:56:31.036357 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.reference_derived
[0m21:56:31.036568 [debug] [Thread-1  ]: Compiling model.anomaly_detection.reference_derived
[0m21:56:31.044664 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.reference_derived"
[0m21:56:31.045634 [debug] [Thread-1  ]: finished collecting timing info
[0m21:56:31.045749 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.reference_derived
[0m21:56:31.056217 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:56:31.515877 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.reference_derived"
[0m21:56:31.516660 [debug] [Thread-1  ]: On model.anomaly_detection.reference_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.reference_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived`
  
  
  OPTIONS()
  as (
    
SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`web_purchase_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY) AND DATE(collector_tstamp) < CURRENT_DATE()

UNION ALL

SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`pco_web_apply_filter_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY) AND DATE(collector_tstamp) < CURRENT_DATE()
  );
  
[0m21:56:35.686790 [debug] [Thread-1  ]: finished collecting timing info
[0m21:56:35.687589 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105824ee0>]}
[0m21:56:35.687969 [info ] [Thread-1  ]: 1 of 47 OK created table model dbt_anomaly_detection.reference_derived ......... [[32mCREATE TABLE (1.1m rows, 34.8 MB processed)[0m in 4.65s]
[0m21:56:35.688357 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.reference_derived
[0m21:56:35.689059 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ref_including_LoBs
[0m21:56:35.689373 [info ] [Thread-1  ]: 2 of 47 START table model dbt_anomaly_detection.derived_ref_including_LoBs ..... [RUN]
[0m21:56:35.689799 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_ref_including_LoBs"
[0m21:56:35.689931 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_ref_including_LoBs
[0m21:56:35.690060 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_ref_including_LoBs
[0m21:56:35.693411 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_ref_including_LoBs"
[0m21:56:35.693983 [debug] [Thread-1  ]: finished collecting timing info
[0m21:56:35.694096 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_ref_including_LoBs
[0m21:56:35.696100 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:56:36.068087 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_ref_including_LoBs"
[0m21:56:36.068855 [debug] [Thread-1  ]: On model.anomaly_detection.derived_ref_including_LoBs: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_ref_including_LoBs"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs`
  
  
  OPTIONS()
  as (
    

SELECT collector_tstamp, event, user_event_name, app_id,
    CASE
    WHEN LOWER(app_id) LIKE '%pcexpress%' OR LOWER(app_id) LIKE '%pcx%' THEN "PCX"
    WHEN LOWER(app_id) LIKE '%sdm%' OR LOWER(app_id) LIKE '%beauty%' THEN "SDM Shop"
    WHEN LOWER(app_id) LIKE '%drx%' THEN "SDM Drx" 
    -- WHEN LOWER(page_urlhost) like '%pcoptimum.ca%' and (LOWER(app_id) like '%ios%' or LOWER(app_id) like '%android%') then 'PC Optimum' 
    -- not recommended to use page_urlhost for mobile apps
    WHEN LOWER(app_id) like '%pco%' THEN 'PC Optimum'
    -- WHEN LOWER(page_urlhost) like '%presidentschoice.ca%' THEN 'PC' -- placeholder for after the launch 
    -- WHEN LOWER(page_urlhost) like '%joefresh.com%' and LOWER(app_id) like '%ios%' then 'JF' -- placeholder for after the launch 
    WHEN LOWER(app_id) like '%jf%' THEN 'JF'
    ELSE app_id END AS LoB,
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived`
  );
  
[0m21:56:40.770557 [debug] [Thread-1  ]: finished collecting timing info
[0m21:56:40.771259 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058d7e50>]}
[0m21:56:40.771635 [info ] [Thread-1  ]: 2 of 47 OK created table model dbt_anomaly_detection.derived_ref_including_LoBs  [[32mCREATE TABLE (1.1m rows, 34.8 MB processed)[0m in 5.08s]
[0m21:56:40.772010 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ref_including_LoBs
[0m21:56:40.772733 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived
[0m21:56:40.773126 [info ] [Thread-1  ]: 3 of 47 START table model dbt_anomaly_detection.all_agg_derived ................ [RUN]
[0m21:56:40.773572 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived"
[0m21:56:40.773697 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived
[0m21:56:40.773815 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived
[0m21:56:40.779567 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived"
[0m21:56:40.781346 [debug] [Thread-1  ]: finished collecting timing info
[0m21:56:40.781507 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived
[0m21:56:40.784362 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:56:41.235438 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived"
[0m21:56:41.238151 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
  
  
  OPTIONS()
  as (
    


SELECT "4hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_4hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/4)*4 AS _4hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _4hr_trunc,
    app_id,
    LoB,
    event_type)

UNION ALL


SELECT "8hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_8hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/8)*8 AS _8hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _8hr_trunc,
    app_id,
    LoB,
    event_type)

UNION ALL


SELECT "12hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_12hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/12)*12 AS _12hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _12hr_trunc,
    app_id,
    LoB,
    event_type)

UNION ALL


SELECT "24hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_24hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/24)*24 AS _24hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _24hr_trunc,
    app_id,
    LoB,
    event_type)

ORDER BY
  agg_tag,
  time_stamps,
  app_id,
  LoB,
  event_type


  );
  
[0m21:56:46.255164 [debug] [Thread-1  ]: finished collecting timing info
[0m21:56:46.256276 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105850f10>]}
[0m21:56:46.256863 [info ] [Thread-1  ]: 3 of 47 OK created table model dbt_anomaly_detection.all_agg_derived ........... [[32mCREATE TABLE (2.2k rows, 42.0 MB processed)[0m in 5.48s]
[0m21:56:46.257325 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived
[0m21:56:46.257985 [debug] [Thread-1  ]: Began running node model.anomaly_detection.count_cutoff
[0m21:56:46.258312 [info ] [Thread-1  ]: 4 of 47 START table model dbt_anomaly_detection.count_cutoff ................... [RUN]
[0m21:56:46.258767 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.count_cutoff"
[0m21:56:46.258911 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.count_cutoff
[0m21:56:46.259051 [debug] [Thread-1  ]: Compiling model.anomaly_detection.count_cutoff
[0m21:56:46.262931 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.count_cutoff"
[0m21:56:46.263651 [debug] [Thread-1  ]: finished collecting timing info
[0m21:56:46.263812 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.count_cutoff
[0m21:56:46.266011 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:56:46.652325 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.count_cutoff"
[0m21:56:46.653638 [debug] [Thread-1  ]: On model.anomaly_detection.count_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.count_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, LoB, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select agg_tag, app_event, LoB, min(time_stamps) as strt_time
from pairs
where event_count > 50
group by app_event, LoB, agg_tag 
order by app_event, LoB, agg_tag
  );
  
[0m21:56:48.586377 [debug] [Thread-1  ]: finished collecting timing info
[0m21:56:48.587267 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105878ca0>]}
[0m21:56:48.587723 [info ] [Thread-1  ]: 4 of 47 OK created table model dbt_anomaly_detection.count_cutoff .............. [[32mCREATE TABLE (8.0 rows, 93.3 KB processed)[0m in 2.33s]
[0m21:56:48.588181 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.count_cutoff
[0m21:56:48.588389 [debug] [Thread-1  ]: Began running node model.anomaly_detection.dev_ml_model_constraints
[0m21:56:48.588928 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.dev_ml_model_constraints"
[0m21:56:48.589352 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.dev_ml_model_constraints
[0m21:56:48.589742 [debug] [Thread-1  ]: Compiling model.anomaly_detection.dev_ml_model_constraints
[0m21:56:48.594369 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.dev_ml_model_constraints"
[0m21:56:48.595107 [debug] [Thread-1  ]: finished collecting timing info
[0m21:56:48.595548 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.dev_ml_model_constraints
[0m21:56:48.595723 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff
[0m21:56:48.595930 [info ] [Thread-1  ]: 5 of 47 START table model dbt_anomaly_detection.all_agg_derived_cutoff ......... [RUN]
[0m21:56:48.596605 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff"
[0m21:56:48.596847 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff
[0m21:56:48.596994 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff
[0m21:56:48.600611 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m21:56:48.601721 [debug] [Thread-1  ]: finished collecting timing info
[0m21:56:48.601895 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff
[0m21:56:48.605852 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:56:49.018692 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m21:56:49.020060 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, LoB, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select time_stamps, app_event, LoB, agg_tag, event_count
from(
select time_stamps, strt_time, pairs.app_event, pairs.LoB, pairs.agg_tag, event_count
from pairs
inner join `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff` as cutoff
on pairs.agg_tag = cutoff.agg_tag
and pairs.app_event = cutoff.app_event
and pairs.LoB = cutoff.LoB
order by pairs.app_event, pairs.LoB, pairs.agg_tag, time_stamps)
where time_stamps >= strt_time
  );
  
[0m21:56:51.185762 [debug] [Thread-1  ]: finished collecting timing info
[0m21:56:51.186491 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105929b50>]}
[0m21:56:51.186878 [info ] [Thread-1  ]: 5 of 47 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff .... [[32mCREATE TABLE (2.2k rows, 93.6 KB processed)[0m in 2.59s]
[0m21:56:51.187256 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff
[0m21:56:51.188008 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m21:56:51.188256 [info ] [Thread-1  ]: 6 of 47 START table model dbt_anomaly_detection.all_agg_derived_cutoff_long .... [RUN]
[0m21:56:51.188641 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m21:56:51.188776 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_long
[0m21:56:51.188909 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_long
[0m21:56:51.192557 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m21:56:51.193382 [debug] [Thread-1  ]: finished collecting timing info
[0m21:56:51.193509 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_long
[0m21:56:51.195434 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:56:51.586505 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m21:56:51.587748 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB(CURRENT_DATE(), INTERVAL 10 DAY)
  );
  
[0m21:56:53.974467 [debug] [Thread-1  ]: finished collecting timing info
[0m21:56:53.975841 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105843e20>]}
[0m21:56:53.976364 [info ] [Thread-1  ]: 6 of 47 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_long  [[32mCREATE TABLE (1.9k rows, 91.2 KB processed)[0m in 2.79s]
[0m21:56:53.976814 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m21:56:53.977017 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m21:56:53.977390 [info ] [Thread-1  ]: 7 of 47 START table model dbt_anomaly_detection.all_agg_derived_cutoff_short ... [RUN]
[0m21:56:53.977997 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m21:56:53.978158 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_short
[0m21:56:53.978304 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_short
[0m21:56:53.982205 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m21:56:53.983097 [debug] [Thread-1  ]: finished collecting timing info
[0m21:56:53.983236 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_short
[0m21:56:53.985435 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:56:54.374268 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m21:56:54.376365 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB(CURRENT_DATE(), INTERVAL 15 DAY)
  );
  
[0m21:56:56.547231 [debug] [Thread-1  ]: finished collecting timing info
[0m21:56:56.548134 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10584bf70>]}
[0m21:56:56.548606 [info ] [Thread-1  ]: 7 of 47 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_short  [[32mCREATE TABLE (1.8k rows, 91.2 KB processed)[0m in 2.57s]
[0m21:56:56.549082 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m21:56:56.549290 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_nonrecent_events
[0m21:56:56.549782 [info ] [Thread-1  ]: 8 of 47 START table model dbt_anomaly_detection.derived_nonrecent_events ....... [RUN]
[0m21:56:56.550251 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_nonrecent_events"
[0m21:56:56.550408 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_nonrecent_events
[0m21:56:56.550566 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_nonrecent_events
[0m21:56:56.554306 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m21:56:56.556609 [debug] [Thread-1  ]: finished collecting timing info
[0m21:56:56.556753 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_nonrecent_events
[0m21:56:56.564396 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:56:57.007930 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m21:56:57.009138 [debug] [Thread-1  ]: On model.anomaly_detection.derived_nonrecent_events: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_nonrecent_events"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events`
  
  
  OPTIONS()
  as (
    

SELECT MIN(time_stamps) AS strt_time, app_event, LoB 
FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
GROUP BY app_event, LoB
HAVING DATE(MIN(time_stamps)) < DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
ORDER BY strt_time
  );
  
[0m21:56:59.075934 [debug] [Thread-1  ]: finished collecting timing info
[0m21:56:59.078101 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105820f70>]}
[0m21:56:59.078798 [info ] [Thread-1  ]: 8 of 47 OK created table model dbt_anomaly_detection.derived_nonrecent_events .. [[32mCREATE TABLE (2.0 rows, 63.3 KB processed)[0m in 2.53s]
[0m21:56:59.079266 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_nonrecent_events
[0m21:56:59.079481 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_long
[0m21:56:59.079700 [info ] [Thread-1  ]: 9 of 47 START table model dbt_anomaly_detection.aggregation_quartiles_long ..... [RUN]
[0m21:56:59.080336 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_long"
[0m21:56:59.080541 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_long
[0m21:56:59.080702 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_long
[0m21:56:59.084571 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m21:56:59.085286 [debug] [Thread-1  ]: finished collecting timing info
[0m21:56:59.085422 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_long
[0m21:56:59.087694 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:56:59.502929 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m21:56:59.504407 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m21:57:01.682048 [debug] [Thread-1  ]: finished collecting timing info
[0m21:57:01.683011 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10595b3d0>]}
[0m21:57:01.683474 [info ] [Thread-1  ]: 9 of 47 OK created table model dbt_anomaly_detection.aggregation_quartiles_long  [[32mCREATE TABLE (8.0 rows, 53.0 KB processed)[0m in 2.60s]
[0m21:57:01.683893 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_long
[0m21:57:01.684079 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_short
[0m21:57:01.684437 [info ] [Thread-1  ]: 10 of 47 START table model dbt_anomaly_detection.aggregation_quartiles_short ... [RUN]
[0m21:57:01.685429 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_short"
[0m21:57:01.685623 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_short
[0m21:57:01.685772 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_short
[0m21:57:01.689502 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m21:57:01.690149 [debug] [Thread-1  ]: finished collecting timing info
[0m21:57:01.690286 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_short
[0m21:57:01.692546 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:57:02.066598 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m21:57:02.067936 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m21:57:04.344357 [debug] [Thread-1  ]: finished collecting timing info
[0m21:57:04.345192 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10592e370>]}
[0m21:57:04.345677 [info ] [Thread-1  ]: 10 of 47 OK created table model dbt_anomaly_detection.aggregation_quartiles_short  [[32mCREATE TABLE (8.0 rows, 49.7 KB processed)[0m in 2.66s]
[0m21:57:04.346232 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_short
[0m21:57:04.346495 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_long
[0m21:57:04.346767 [info ] [Thread-1  ]: 11 of 47 START table model dbt_anomaly_detection.aggregation_bounds_long ....... [RUN]
[0m21:57:04.347733 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_long"
[0m21:57:04.348136 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_long
[0m21:57:04.348378 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_long
[0m21:57:04.359139 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m21:57:04.359735 [debug] [Thread-1  ]: finished collecting timing info
[0m21:57:04.359889 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_long
[0m21:57:04.362158 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:57:04.751224 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m21:57:04.753574 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m21:57:06.848021 [debug] [Thread-1  ]: finished collecting timing info
[0m21:57:06.862481 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105929b50>]}
[0m21:57:06.864127 [info ] [Thread-1  ]: 11 of 47 OK created table model dbt_anomaly_detection.aggregation_bounds_long .. [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.51s]
[0m21:57:06.865184 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_long
[0m21:57:06.865537 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_short
[0m21:57:06.866138 [info ] [Thread-1  ]: 12 of 47 START table model dbt_anomaly_detection.aggregation_bounds_short ...... [RUN]
[0m21:57:06.867073 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_short"
[0m21:57:06.867313 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_short
[0m21:57:06.867525 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_short
[0m21:57:06.872856 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m21:57:06.874371 [debug] [Thread-1  ]: finished collecting timing info
[0m21:57:06.874619 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_short
[0m21:57:06.877755 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:57:07.306052 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m21:57:07.307293 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m21:57:09.438352 [debug] [Thread-1  ]: finished collecting timing info
[0m21:57:09.439994 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10588f3d0>]}
[0m21:57:09.440722 [info ] [Thread-1  ]: 12 of 47 OK created table model dbt_anomaly_detection.aggregation_bounds_short . [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.57s]
[0m21:57:09.441379 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_short
[0m21:57:09.441899 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_long
[0m21:57:09.442734 [info ] [Thread-1  ]: 13 of 47 START table model dbt_anomaly_detection.aggregation_outliers_long ..... [RUN]
[0m21:57:09.443404 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_long"
[0m21:57:09.443598 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_long
[0m21:57:09.443774 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_long
[0m21:57:09.450303 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m21:57:09.451087 [debug] [Thread-1  ]: finished collecting timing info
[0m21:57:09.451256 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_long
[0m21:57:09.453838 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:57:09.833691 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m21:57:09.834799 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, LoB, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag, LoB,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m21:57:12.166704 [debug] [Thread-1  ]: finished collecting timing info
[0m21:57:12.167772 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10586e310>]}
[0m21:57:12.168351 [info ] [Thread-1  ]: 13 of 47 OK created table model dbt_anomaly_detection.aggregation_outliers_long  [[32mCREATE TABLE (1.9k rows, 81.4 KB processed)[0m in 2.72s]
[0m21:57:12.168898 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_long
[0m21:57:12.169116 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_short
[0m21:57:12.169548 [info ] [Thread-1  ]: 14 of 47 START table model dbt_anomaly_detection.aggregation_outliers_short .... [RUN]
[0m21:57:12.170275 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_short"
[0m21:57:12.170439 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_short
[0m21:57:12.170599 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_short
[0m21:57:12.176412 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m21:57:12.177075 [debug] [Thread-1  ]: finished collecting timing info
[0m21:57:12.177239 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_short
[0m21:57:12.179948 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:57:12.563087 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m21:57:12.565052 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, LoB, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag, LoB,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m21:57:15.035828 [debug] [Thread-1  ]: finished collecting timing info
[0m21:57:15.036756 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058748b0>]}
[0m21:57:15.037246 [info ] [Thread-1  ]: 14 of 47 OK created table model dbt_anomaly_detection.aggregation_outliers_short  [[32mCREATE TABLE (1.8k rows, 76.3 KB processed)[0m in 2.87s]
[0m21:57:15.037713 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_short
[0m21:57:15.037935 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_4hr
[0m21:57:15.038378 [info ] [Thread-1  ]: 15 of 47 START model model dbt_anomaly_detection.derived_models_05mon_4hr ...... [RUN]
[0m21:57:15.039158 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_4hr"
[0m21:57:15.039382 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_4hr
[0m21:57:15.039564 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_4hr
[0m21:57:15.062635 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m21:57:15.063145 [debug] [Thread-1  ]: finished collecting timing info
[0m21:57:15.063274 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_4hr
[0m21:57:15.076374 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m21:57:15.076881 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:57:15.077020 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH  __dbt__cte__dev_ml_model_constraints as (

-- This materialization is used to constrain the model during dev due to lack of data. Current Date is set to latest_date

WITH latest_earliest AS (
SELECT 
EXTRACT(DATE FROM (SELECT MAX(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS latest_date, -- always sep 21
EXTRACT(DATE FROM (SELECT MIN(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS earliest_date -- always sep 17
)
SELECT *,
-- CASE WHEN DATE_DIFF(latest_date, earliest_date, DAY) >= 35 
-- THEN 35 
     DATE_DIFF(latest_date, earliest_date, DAY) -- always 4
     AS training_interval
FROM latest_earliest
),date_control as (
SELECT 

(SELECT latest_date FROM __dbt__cte__dev_ml_model_constraints) AS cur_date

),
interval_control as (
SELECT 

(SELECT training_interval FROM __dbt__cte__dev_ml_model_constraints) AS training_interval

),
lookback as (
SELECT 

training_interval - 3 AS lookback_interval

FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30"
    );
    
[0m21:57:25.795232 [debug] [Thread-1  ]: finished collecting timing info
[0m21:57:25.797589 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105965c70>]}
[0m21:57:25.798155 [info ] [Thread-1  ]: 15 of 47 OK created model model dbt_anomaly_detection.derived_models_05mon_4hr . [[32mOK[0m in 10.76s]
[0m21:57:25.798734 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_4hr
[0m21:57:25.799005 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_8hr
[0m21:57:25.799285 [info ] [Thread-1  ]: 16 of 47 START model model dbt_anomaly_detection.derived_models_05mon_8hr ...... [RUN]
[0m21:57:25.800152 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_8hr"
[0m21:57:25.800405 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_8hr
[0m21:57:25.800617 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_8hr
[0m21:57:25.822593 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m21:57:25.823082 [debug] [Thread-1  ]: finished collecting timing info
[0m21:57:25.823197 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_8hr
[0m21:57:25.825562 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m21:57:25.826228 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:57:25.826390 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH  __dbt__cte__dev_ml_model_constraints as (

-- This materialization is used to constrain the model during dev due to lack of data. Current Date is set to latest_date

WITH latest_earliest AS (
SELECT 
EXTRACT(DATE FROM (SELECT MAX(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS latest_date, -- always sep 21
EXTRACT(DATE FROM (SELECT MIN(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS earliest_date -- always sep 17
)
SELECT *,
-- CASE WHEN DATE_DIFF(latest_date, earliest_date, DAY) >= 35 
-- THEN 35 
     DATE_DIFF(latest_date, earliest_date, DAY) -- always 4
     AS training_interval
FROM latest_earliest
),date_control as (
SELECT 

(SELECT latest_date FROM __dbt__cte__dev_ml_model_constraints) AS cur_date

),
interval_control as (
SELECT 

(SELECT training_interval FROM __dbt__cte__dev_ml_model_constraints) AS training_interval

),
lookback as (
SELECT 

training_interval - 3 AS lookback_interval

FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30"
    );
    
[0m21:57:36.760455 [debug] [Thread-1  ]: finished collecting timing info
[0m21:57:36.761539 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059cfdc0>]}
[0m21:57:36.762090 [info ] [Thread-1  ]: 16 of 47 OK created model model dbt_anomaly_detection.derived_models_05mon_8hr . [[32mOK[0m in 10.96s]
[0m21:57:36.762672 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_8hr
[0m21:57:36.762907 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_4hr
[0m21:57:36.763292 [info ] [Thread-1  ]: 17 of 47 START model model dbt_anomaly_detection.derived_models_1mon_4hr ....... [RUN]
[0m21:57:36.763865 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_4hr"
[0m21:57:36.764048 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_4hr
[0m21:57:36.764217 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_4hr
[0m21:57:36.770135 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m21:57:36.771013 [debug] [Thread-1  ]: finished collecting timing info
[0m21:57:36.771160 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_4hr
[0m21:57:36.774205 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m21:57:36.774651 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:57:36.774812 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m21:57:47.715125 [debug] [Thread-1  ]: finished collecting timing info
[0m21:57:47.716263 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059b5130>]}
[0m21:57:47.716825 [info ] [Thread-1  ]: 17 of 47 OK created model model dbt_anomaly_detection.derived_models_1mon_4hr .. [[32mOK[0m in 10.95s]
[0m21:57:47.717399 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_4hr
[0m21:57:47.717672 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_8hr
[0m21:57:47.718153 [info ] [Thread-1  ]: 18 of 47 START model model dbt_anomaly_detection.derived_models_1mon_8hr ....... [RUN]
[0m21:57:47.718993 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_8hr"
[0m21:57:47.719219 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_8hr
[0m21:57:47.719423 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_8hr
[0m21:57:47.738830 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m21:57:47.739376 [debug] [Thread-1  ]: finished collecting timing info
[0m21:57:47.739508 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_8hr
[0m21:57:47.743288 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m21:57:47.743680 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:57:47.743827 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH  __dbt__cte__dev_ml_model_constraints as (

-- This materialization is used to constrain the model during dev due to lack of data. Current Date is set to latest_date

WITH latest_earliest AS (
SELECT 
EXTRACT(DATE FROM (SELECT MAX(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS latest_date, -- always sep 21
EXTRACT(DATE FROM (SELECT MIN(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS earliest_date -- always sep 17
)
SELECT *,
-- CASE WHEN DATE_DIFF(latest_date, earliest_date, DAY) >= 35 
-- THEN 35 
     DATE_DIFF(latest_date, earliest_date, DAY) -- always 4
     AS training_interval
FROM latest_earliest
),date_control as (
SELECT 

(SELECT latest_date FROM __dbt__cte__dev_ml_model_constraints) AS cur_date

),
interval_control as (
SELECT 

(SELECT training_interval FROM __dbt__cte__dev_ml_model_constraints) AS training_interval

),
lookback as (
SELECT 

training_interval - 3 AS lookback_interval

FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30"
    );
    
[0m21:57:59.733422 [debug] [Thread-1  ]: finished collecting timing info
[0m21:57:59.734425 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059e0760>]}
[0m21:57:59.734971 [info ] [Thread-1  ]: 18 of 47 OK created model model dbt_anomaly_detection.derived_models_1mon_8hr .. [[32mOK[0m in 12.02s]
[0m21:57:59.735481 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_8hr
[0m21:57:59.735715 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_4hr
[0m21:57:59.736144 [info ] [Thread-1  ]: 19 of 47 START model model dbt_anomaly_detection.derived_models_2mon_4hr ....... [RUN]
[0m21:57:59.736693 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_4hr"
[0m21:57:59.736871 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_4hr
[0m21:57:59.737043 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_4hr
[0m21:57:59.753693 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m21:57:59.754224 [debug] [Thread-1  ]: finished collecting timing info
[0m21:57:59.754338 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_4hr
[0m21:57:59.756710 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m21:57:59.757015 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:57:59.757151 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH  __dbt__cte__dev_ml_model_constraints as (

-- This materialization is used to constrain the model during dev due to lack of data. Current Date is set to latest_date

WITH latest_earliest AS (
SELECT 
EXTRACT(DATE FROM (SELECT MAX(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS latest_date, -- always sep 21
EXTRACT(DATE FROM (SELECT MIN(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS earliest_date -- always sep 17
)
SELECT *,
-- CASE WHEN DATE_DIFF(latest_date, earliest_date, DAY) >= 35 
-- THEN 35 
     DATE_DIFF(latest_date, earliest_date, DAY) -- always 4
     AS training_interval
FROM latest_earliest
),date_control as (
SELECT 

(SELECT latest_date FROM __dbt__cte__dev_ml_model_constraints) AS cur_date

),
interval_control as (
SELECT 

(SELECT training_interval FROM __dbt__cte__dev_ml_model_constraints) AS training_interval

),
lookback as (
SELECT 

training_interval - 3 AS lookback_interval

FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30"
    );
    
[0m21:58:12.030349 [debug] [Thread-1  ]: finished collecting timing info
[0m21:58:12.031985 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10589bfa0>]}
[0m21:58:12.032670 [info ] [Thread-1  ]: 19 of 47 OK created model model dbt_anomaly_detection.derived_models_2mon_4hr .. [[32mOK[0m in 12.30s]
[0m21:58:12.033205 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_4hr
[0m21:58:12.033435 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_8hr
[0m21:58:12.033842 [info ] [Thread-1  ]: 20 of 47 START model model dbt_anomaly_detection.derived_models_2mon_8hr ....... [RUN]
[0m21:58:12.034410 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_8hr"
[0m21:58:12.034599 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_8hr
[0m21:58:12.034788 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_8hr
[0m21:58:12.052293 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m21:58:12.052820 [debug] [Thread-1  ]: finished collecting timing info
[0m21:58:12.052942 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_8hr
[0m21:58:12.055326 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m21:58:12.055903 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:58:12.056074 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH  __dbt__cte__dev_ml_model_constraints as (

-- This materialization is used to constrain the model during dev due to lack of data. Current Date is set to latest_date

WITH latest_earliest AS (
SELECT 
EXTRACT(DATE FROM (SELECT MAX(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS latest_date, -- always sep 21
EXTRACT(DATE FROM (SELECT MIN(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS earliest_date -- always sep 17
)
SELECT *,
-- CASE WHEN DATE_DIFF(latest_date, earliest_date, DAY) >= 35 
-- THEN 35 
     DATE_DIFF(latest_date, earliest_date, DAY) -- always 4
     AS training_interval
FROM latest_earliest
),date_control as (
SELECT 

(SELECT latest_date FROM __dbt__cte__dev_ml_model_constraints) AS cur_date

),
interval_control as (
SELECT 

(SELECT training_interval FROM __dbt__cte__dev_ml_model_constraints) AS training_interval

),
lookback as (
SELECT 

training_interval - 3 AS lookback_interval

FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30"
    );
    
[0m21:58:22.578318 [debug] [Thread-1  ]: finished collecting timing info
[0m21:58:22.579453 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10586b340>]}
[0m21:58:22.580053 [info ] [Thread-1  ]: 20 of 47 OK created model model dbt_anomaly_detection.derived_models_2mon_8hr .. [[32mOK[0m in 10.55s]
[0m21:58:22.580641 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_8hr
[0m21:58:22.580907 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_12hr
[0m21:58:22.581399 [info ] [Thread-1  ]: 21 of 47 START model model dbt_anomaly_detection.derived_models_05mon_12hr ..... [RUN]
[0m21:58:22.582083 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_12hr"
[0m21:58:22.582310 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_12hr
[0m21:58:22.582517 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_12hr
[0m21:58:22.605878 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m21:58:22.606373 [debug] [Thread-1  ]: finished collecting timing info
[0m21:58:22.606489 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_12hr
[0m21:58:22.608918 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m21:58:22.609347 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:58:22.609489 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH  __dbt__cte__dev_ml_model_constraints as (

-- This materialization is used to constrain the model during dev due to lack of data. Current Date is set to latest_date

WITH latest_earliest AS (
SELECT 
EXTRACT(DATE FROM (SELECT MAX(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS latest_date, -- always sep 21
EXTRACT(DATE FROM (SELECT MIN(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS earliest_date -- always sep 17
)
SELECT *,
-- CASE WHEN DATE_DIFF(latest_date, earliest_date, DAY) >= 35 
-- THEN 35 
     DATE_DIFF(latest_date, earliest_date, DAY) -- always 4
     AS training_interval
FROM latest_earliest
),date_control as (
SELECT 

(SELECT latest_date FROM __dbt__cte__dev_ml_model_constraints) AS cur_date

),
interval_control as (
SELECT 

(SELECT training_interval FROM __dbt__cte__dev_ml_model_constraints) AS training_interval

),
lookback as (
SELECT 

training_interval - 3 AS lookback_interval

FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30"
    );
    
[0m21:58:39.247802 [debug] [Thread-1  ]: finished collecting timing info
[0m21:58:39.249378 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105896370>]}
[0m21:58:39.250026 [info ] [Thread-1  ]: 21 of 47 OK created model model dbt_anomaly_detection.derived_models_05mon_12hr  [[32mOK[0m in 16.67s]
[0m21:58:39.250523 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_12hr
[0m21:58:39.250732 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_24hr
[0m21:58:39.251036 [info ] [Thread-1  ]: 22 of 47 START model model dbt_anomaly_detection.derived_models_05mon_24hr ..... [RUN]
[0m21:58:39.252723 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_24hr"
[0m21:58:39.253034 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_24hr
[0m21:58:39.253242 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_24hr
[0m21:58:39.272262 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m21:58:39.273702 [debug] [Thread-1  ]: finished collecting timing info
[0m21:58:39.273897 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_24hr
[0m21:58:39.276571 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m21:58:39.277248 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:58:39.277414 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH  __dbt__cte__dev_ml_model_constraints as (

-- This materialization is used to constrain the model during dev due to lack of data. Current Date is set to latest_date

WITH latest_earliest AS (
SELECT 
EXTRACT(DATE FROM (SELECT MAX(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS latest_date, -- always sep 21
EXTRACT(DATE FROM (SELECT MIN(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS earliest_date -- always sep 17
)
SELECT *,
-- CASE WHEN DATE_DIFF(latest_date, earliest_date, DAY) >= 35 
-- THEN 35 
     DATE_DIFF(latest_date, earliest_date, DAY) -- always 4
     AS training_interval
FROM latest_earliest
),date_control as (
SELECT 

(SELECT latest_date FROM __dbt__cte__dev_ml_model_constraints) AS cur_date

),
interval_control as (
SELECT 

(SELECT training_interval FROM __dbt__cte__dev_ml_model_constraints) AS training_interval

),
lookback as (
SELECT 

training_interval - 3 AS lookback_interval

FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30"
    );
    
[0m21:58:50.308211 [debug] [Thread-1  ]: finished collecting timing info
[0m21:58:50.309522 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105965c10>]}
[0m21:58:50.310095 [info ] [Thread-1  ]: 22 of 47 OK created model model dbt_anomaly_detection.derived_models_05mon_24hr  [[32mOK[0m in 11.06s]
[0m21:58:50.310680 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_24hr
[0m21:58:50.310947 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_12hr
[0m21:58:50.311443 [info ] [Thread-1  ]: 23 of 47 START model model dbt_anomaly_detection.derived_models_1mon_12hr ...... [RUN]
[0m21:58:50.312098 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_12hr"
[0m21:58:50.312315 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_12hr
[0m21:58:50.312512 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_12hr
[0m21:58:50.332783 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m21:58:50.333307 [debug] [Thread-1  ]: finished collecting timing info
[0m21:58:50.333433 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_12hr
[0m21:58:50.336983 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m21:58:50.337283 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:58:50.337381 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH  __dbt__cte__dev_ml_model_constraints as (

-- This materialization is used to constrain the model during dev due to lack of data. Current Date is set to latest_date

WITH latest_earliest AS (
SELECT 
EXTRACT(DATE FROM (SELECT MAX(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS latest_date, -- always sep 21
EXTRACT(DATE FROM (SELECT MIN(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS earliest_date -- always sep 17
)
SELECT *,
-- CASE WHEN DATE_DIFF(latest_date, earliest_date, DAY) >= 35 
-- THEN 35 
     DATE_DIFF(latest_date, earliest_date, DAY) -- always 4
     AS training_interval
FROM latest_earliest
),date_control as (
SELECT 

(SELECT latest_date FROM __dbt__cte__dev_ml_model_constraints) AS cur_date

),
interval_control as (
SELECT 

(SELECT training_interval FROM __dbt__cte__dev_ml_model_constraints) AS training_interval

),
lookback as (
SELECT 

training_interval - 3 AS lookback_interval

FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30"
    );
    
[0m21:59:02.017220 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:02.018358 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105966880>]}
[0m21:59:02.018923 [info ] [Thread-1  ]: 23 of 47 OK created model model dbt_anomaly_detection.derived_models_1mon_12hr . [[32mOK[0m in 11.71s]
[0m21:59:02.019430 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_12hr
[0m21:59:02.019655 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_24hr
[0m21:59:02.020049 [info ] [Thread-1  ]: 24 of 47 START model model dbt_anomaly_detection.derived_models_1mon_24hr ...... [RUN]
[0m21:59:02.020610 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_24hr"
[0m21:59:02.020805 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_24hr
[0m21:59:02.020980 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_24hr
[0m21:59:02.028836 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m21:59:02.029534 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:02.029691 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_24hr
[0m21:59:02.032756 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m21:59:02.033210 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:59:02.033377 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m21:59:13.093753 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:13.096200 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105966970>]}
[0m21:59:13.096717 [info ] [Thread-1  ]: 24 of 47 OK created model model dbt_anomaly_detection.derived_models_1mon_24hr . [[32mOK[0m in 11.08s]
[0m21:59:13.097180 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_24hr
[0m21:59:13.097378 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_12hr
[0m21:59:13.097719 [info ] [Thread-1  ]: 25 of 47 START model model dbt_anomaly_detection.derived_models_2mon_12hr ...... [RUN]
[0m21:59:13.098167 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_12hr"
[0m21:59:13.098303 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_12hr
[0m21:59:13.098436 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_12hr
[0m21:59:13.113152 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m21:59:13.113609 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:13.113715 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_12hr
[0m21:59:13.115929 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m21:59:13.116223 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:59:13.116346 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH  __dbt__cte__dev_ml_model_constraints as (

-- This materialization is used to constrain the model during dev due to lack of data. Current Date is set to latest_date

WITH latest_earliest AS (
SELECT 
EXTRACT(DATE FROM (SELECT MAX(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS latest_date, -- always sep 21
EXTRACT(DATE FROM (SELECT MIN(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS earliest_date -- always sep 17
)
SELECT *,
-- CASE WHEN DATE_DIFF(latest_date, earliest_date, DAY) >= 35 
-- THEN 35 
     DATE_DIFF(latest_date, earliest_date, DAY) -- always 4
     AS training_interval
FROM latest_earliest
),date_control as (
SELECT 

(SELECT latest_date FROM __dbt__cte__dev_ml_model_constraints) AS cur_date

),
interval_control as (
SELECT 

(SELECT training_interval FROM __dbt__cte__dev_ml_model_constraints) AS training_interval

),
lookback as (
SELECT 

training_interval - 3 AS lookback_interval

FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30"
    );
    
[0m21:59:23.986798 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:23.988626 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a623a0>]}
[0m21:59:24.507695 [info ] [Thread-1  ]: 25 of 47 OK created model model dbt_anomaly_detection.derived_models_2mon_12hr . [[32mOK[0m in 10.89s]
[0m21:59:24.508718 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_12hr
[0m21:59:24.509040 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_24hr
[0m21:59:24.509345 [info ] [Thread-1  ]: 26 of 47 START model model dbt_anomaly_detection.derived_models_2mon_24hr ...... [RUN]
[0m21:59:24.510509 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_24hr"
[0m21:59:24.510880 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_24hr
[0m21:59:24.511100 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_24hr
[0m21:59:24.534747 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m21:59:24.535221 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:24.535321 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_24hr
[0m21:59:24.537067 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m21:59:24.537308 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:59:24.537415 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH  __dbt__cte__dev_ml_model_constraints as (

-- This materialization is used to constrain the model during dev due to lack of data. Current Date is set to latest_date

WITH latest_earliest AS (
SELECT 
EXTRACT(DATE FROM (SELECT MAX(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS latest_date, -- always sep 21
EXTRACT(DATE FROM (SELECT MIN(time_stamps) FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`)) AS earliest_date -- always sep 17
)
SELECT *,
-- CASE WHEN DATE_DIFF(latest_date, earliest_date, DAY) >= 35 
-- THEN 35 
     DATE_DIFF(latest_date, earliest_date, DAY) -- always 4
     AS training_interval
FROM latest_earliest
),date_control as (
SELECT 

(SELECT latest_date FROM __dbt__cte__dev_ml_model_constraints) AS cur_date

),
interval_control as (
SELECT 

(SELECT training_interval FROM __dbt__cte__dev_ml_model_constraints) AS training_interval

),
lookback as (
SELECT 

training_interval - 3 AS lookback_interval

FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30"
    );
    
[0m21:59:36.620861 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:36.621786 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059f3280>]}
[0m21:59:36.622346 [info ] [Thread-1  ]: 26 of 47 OK created model model dbt_anomaly_detection.derived_models_2mon_24hr . [[32mOK[0m in 12.11s]
[0m21:59:36.622941 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_24hr
[0m21:59:36.623213 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ml_detect
[0m21:59:36.623654 [info ] [Thread-1  ]: 27 of 47 START table model dbt_anomaly_detection.derived_ml_detect ............. [RUN]
[0m21:59:36.624588 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_ml_detect"
[0m21:59:36.624788 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_ml_detect
[0m21:59:36.624965 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_ml_detect
[0m21:59:36.633862 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_ml_detect"
[0m21:59:36.634439 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:36.634604 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_ml_detect
[0m21:59:36.637864 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_ml_detect"
[0m21:59:36.638317 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:59:36.638488 [debug] [Thread-1  ]: On model.anomaly_detection.derived_ml_detect: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_ml_detect"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_ml_detect`
  
  
  OPTIONS()
  as (
    -- 2 periods of training * 2 probabality threshold * 4 aggregation levels 


  WITH test_set AS (
  SELECT
  time_stamps,
        event_count,
        app_event,
        LoB,
        agg_tag
      FROM
        `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`  
      WHERE (agg_tag = "4hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 10 DAY)) 
      OR (agg_tag = "8hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 10 DAY))
      OR (agg_tag = "12hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 15 DAY))
      OR (agg_tag = "24hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 15 DAY))
  )

  
  

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
     

  
  UNION ALL
  



    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
     

  
  ORDER BY
  agg_tag,
  time_stamps,
  app_event,
  LoB,
  prob_threshold,
  training_period
  


  );
  
[0m21:59:39.188207 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:39.189253 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059b3130>]}
[0m21:59:39.189906 [info ] [Thread-1  ]: 27 of 47 OK created table model dbt_anomaly_detection.derived_ml_detect ........ [[32mCREATE TABLE (240.0 rows, 141.8 KB processed)[0m in 2.57s]
[0m21:59:39.190472 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ml_detect
[0m21:59:39.191276 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ml_detect_tweaked
[0m21:59:39.191810 [info ] [Thread-1  ]: 28 of 47 START table model dbt_anomaly_detection.ml_detect_tweaked ............. [RUN]
[0m21:59:39.192442 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ml_detect_tweaked"
[0m21:59:39.192652 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ml_detect_tweaked
[0m21:59:39.192828 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ml_detect_tweaked
[0m21:59:39.198138 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ml_detect_tweaked"
[0m21:59:39.198770 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:39.198942 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ml_detect_tweaked
[0m21:59:39.204137 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.ml_detect_tweaked"
[0m21:59:39.204620 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:59:39.204797 [debug] [Thread-1  ]: On model.anomaly_detection.ml_detect_tweaked: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.ml_detect_tweaked"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked`
  
  
  OPTIONS()
  as (
     
  
  with neg_bound_reset as (
SELECT app_event, LoB, agg_tag, time_stamps, prob_threshold, training_period, event_count, 
  IF(lower_bound<0, 2, 0.77*lower_bound) AS lower_bound, 1.3*upper_bound AS upper_bound, anomaly_probability, is_anomaly
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ml_detect` )

SELECT app_event, LoB, agg_tag, time_stamps, prob_threshold, training_period, event_count, 
  lower_bound, upper_bound, anomaly_probability,
  (upper_bound < event_count OR event_count < lower_bound) AS is_anomaly
FROM neg_bound_reset
  );
  
[0m21:59:41.622993 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:41.624989 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058d52e0>]}
[0m21:59:41.626228 [info ] [Thread-1  ]: 28 of 47 OK created table model dbt_anomaly_detection.ml_detect_tweaked ........ [[32mCREATE TABLE (240.0 rows, 23.7 KB processed)[0m in 2.43s]
[0m21:59:41.627132 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ml_detect_tweaked
[0m21:59:41.628265 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_model_features_dbt
[0m21:59:41.628668 [info ] [Thread-1  ]: 29 of 47 START table model dbt_anomaly_detection.derived_model_features_dbt .... [RUN]
[0m21:59:41.629304 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_model_features_dbt"
[0m21:59:41.629515 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_model_features_dbt
[0m21:59:41.629708 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_model_features_dbt
[0m21:59:41.635005 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_model_features_dbt"
[0m21:59:41.635461 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:41.635558 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_model_features_dbt
[0m21:59:41.638157 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_model_features_dbt"
[0m21:59:41.638632 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:59:41.638811 [debug] [Thread-1  ]: On model.anomaly_detection.derived_model_features_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_model_features_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_model_features_dbt`
  
  
  OPTIONS()
  as (
    
  
  SELECT
    app_event, LoB, 
    CONCAT(agg_tag, '_', prob_threshold, "threshold", '_', RTRIM(LTRIM(training_period, "derived_models_"), CONCAT('_', agg_tag))) AS control_config,
    SUM(CASE WHEN is_anomaly = TRUE THEN 1 ELSE 0 END) AS anomalies,
    SQRT(AVG( POWER(upper_bound - lower_bound, 2) ) ) / AVG(lower_bound) AS RMSD_prcnt,
    SUM(CASE WHEN lower_bound < 0 THEN 1 ELSE 0 END) AS neg_lower
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked` AS all_configs
  GROUP BY
    app_event, 
    LoB,
    control_config
  ORDER BY
    control_config,
    app_event,
    LoB
  );
  
[0m21:59:44.143016 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:44.144292 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10584f3a0>]}
[0m21:59:44.144760 [info ] [Thread-1  ]: 29 of 47 OK created table model dbt_anomaly_detection.derived_model_features_dbt  [[32mCREATE TABLE (4.0 rows, 18.3 KB processed)[0m in 2.52s]
[0m21:59:44.145204 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_model_features_dbt
[0m21:59:44.145754 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_model_features_dbt
[0m21:59:44.146168 [info ] [Thread-1  ]: 30 of 47 START table model dbt_anomaly_detection.filtered_model_features_dbt ... [RUN]
[0m21:59:44.146747 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_model_features_dbt"
[0m21:59:44.146947 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_model_features_dbt
[0m21:59:44.147151 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_model_features_dbt
[0m21:59:44.152094 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_model_features_dbt"
[0m21:59:44.153198 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:44.153401 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_model_features_dbt
[0m21:59:44.157304 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.filtered_model_features_dbt"
[0m21:59:44.157925 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:59:44.158136 [debug] [Thread-1  ]: On model.anomaly_detection.filtered_model_features_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.filtered_model_features_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
  
  
  OPTIONS()
  as (
    
SELECT features.app_event, features.LoB, control_config, anomalies, RMSD_prcnt, neg_lower
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events` AS non_recent
INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`derived_model_features_dbt` AS features
  ON non_recent.app_event = features.app_event
  AND non_recent.LoB = features.LoB
  );
  
[0m21:59:46.993929 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:46.996165 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105aa83d0>]}
[0m21:59:46.996655 [info ] [Thread-1  ]: 30 of 47 OK created table model dbt_anomaly_detection.filtered_model_features_dbt  [[32mCREATE TABLE (4.0 rows, 336.0 Bytes processed)[0m in 2.85s]
[0m21:59:46.997137 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_model_features_dbt
[0m21:59:46.998091 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ref_distinct_tuples
[0m21:59:46.998533 [info ] [Thread-1  ]: 31 of 47 START table model dbt_anomaly_detection.ref_distinct_tuples ........... [RUN]
[0m21:59:46.999089 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ref_distinct_tuples"
[0m21:59:46.999269 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ref_distinct_tuples
[0m21:59:46.999447 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ref_distinct_tuples
[0m21:59:47.006090 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ref_distinct_tuples"
[0m21:59:47.006951 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:47.007135 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ref_distinct_tuples
[0m21:59:47.013126 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.ref_distinct_tuples"
[0m21:59:47.013616 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:59:47.013774 [debug] [Thread-1  ]: On model.anomaly_detection.ref_distinct_tuples: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.ref_distinct_tuples"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`ref_distinct_tuples`
  
  
  OPTIONS()
  as (
    
SELECT DISTINCT app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
WHERE app_event LIKE '%add%'
UNION ALL
SELECT DISTINCT app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
WHERE app_event NOT LIKE '%ad%'
ORDER BY app_event
  );
  
[0m21:59:49.564143 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:49.565162 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105adddc0>]}
[0m21:59:49.565740 [info ] [Thread-1  ]: 31 of 47 OK created table model dbt_anomaly_detection.ref_distinct_tuples ...... [[32mCREATE TABLE (2.0 rows, 60.0 Bytes processed)[0m in 2.57s]
[0m21:59:49.566298 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ref_distinct_tuples
[0m21:59:49.566567 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_phase
[0m21:59:49.566844 [info ] [Thread-1  ]: 32 of 47 START table model dbt_anomaly_detection.trade_off_phase ............... [RUN]
[0m21:59:49.567583 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.trade_off_phase"
[0m21:59:49.567781 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.trade_off_phase
[0m21:59:49.567978 [debug] [Thread-1  ]: Compiling model.anomaly_detection.trade_off_phase
[0m21:59:49.574775 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.trade_off_phase"
[0m21:59:49.575666 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:49.575836 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.trade_off_phase
[0m21:59:49.579457 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.trade_off_phase"
[0m21:59:49.579934 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:59:49.580112 [debug] [Thread-1  ]: On model.anomaly_detection.trade_off_phase: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.trade_off_phase"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_phase`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
where anomalies = 0
and RMSD_prcnt < 5
  );
  
[0m21:59:52.123571 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:52.124556 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ae1cd0>]}
[0m21:59:52.125108 [info ] [Thread-1  ]: 32 of 47 OK created table model dbt_anomaly_detection.trade_off_phase .......... [[32mCREATE TABLE (0.0 rows, 292.0 Bytes processed)[0m in 2.56s]
[0m21:59:52.125672 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_phase
[0m21:59:52.126458 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_min_anomalies
[0m21:59:52.127194 [info ] [Thread-1  ]: 33 of 47 START table model dbt_anomaly_detection.trade_off_min_anomalies ....... [RUN]
[0m21:59:52.127795 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.trade_off_min_anomalies"
[0m21:59:52.127992 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.trade_off_min_anomalies
[0m21:59:52.128180 [debug] [Thread-1  ]: Compiling model.anomaly_detection.trade_off_min_anomalies
[0m21:59:52.133091 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.trade_off_min_anomalies"
[0m21:59:52.134026 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:52.134220 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.trade_off_min_anomalies
[0m21:59:52.137931 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.trade_off_min_anomalies"
[0m21:59:52.138522 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:59:52.138729 [debug] [Thread-1  ]: On model.anomaly_detection.trade_off_min_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.trade_off_min_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_min_anomalies`
  
  
  OPTIONS()
  as (
    

 SELECT app_event, LoB, MIN(anomalies) AS anomalies 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_phase`
  GROUP BY app_event, LoB
  ORDER BY app_event, LoB
  );
  
[0m21:59:54.550269 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:54.551773 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105869a90>]}
[0m21:59:54.552537 [info ] [Thread-1  ]: 33 of 47 OK created table model dbt_anomaly_detection.trade_off_min_anomalies .. [[32mCREATE TABLE (0.0 rows, 0 processed)[0m in 2.42s]
[0m21:59:54.553152 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_min_anomalies
[0m21:59:54.554177 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_min_anomalies_results
[0m21:59:54.554809 [info ] [Thread-1  ]: 34 of 47 START table model dbt_anomaly_detection.trade_off_min_anomalies_results  [RUN]
[0m21:59:54.555460 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.trade_off_min_anomalies_results"
[0m21:59:54.555645 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.trade_off_min_anomalies_results
[0m21:59:54.555819 [debug] [Thread-1  ]: Compiling model.anomaly_detection.trade_off_min_anomalies_results
[0m21:59:54.565234 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.trade_off_min_anomalies_results"
[0m21:59:54.566110 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:54.566277 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.trade_off_min_anomalies_results
[0m21:59:54.570630 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.trade_off_min_anomalies_results"
[0m21:59:54.571266 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:59:54.571485 [debug] [Thread-1  ]: On model.anomaly_detection.trade_off_min_anomalies_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.trade_off_min_anomalies_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_min_anomalies_results`
  
  
  OPTIONS()
  as (
    

  SELECT neg_lower_criteria_res.app_event, neg_lower_criteria_res.LoB, neg_lower_criteria_res.control_config, 
    neg_lower_criteria_res.anomalies, neg_lower_criteria_res.RMSD_prcnt, neg_lower_criteria_res.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_phase` AS neg_lower_criteria_res
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_min_anomalies` AS min_neg_lower_min_anomalies
    ON neg_lower_criteria_res.anomalies = min_neg_lower_min_anomalies.anomalies
      AND neg_lower_criteria_res.app_event = min_neg_lower_min_anomalies.app_event
      AND neg_lower_criteria_res.LoB = min_neg_lower_min_anomalies.LoB
  ORDER BY neg_lower_criteria_res.app_event, neg_lower_criteria_res.LoB, RMSD_prcnt DESC
  );
  
[0m21:59:57.300206 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:57.301939 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10589e7f0>]}
[0m21:59:57.302553 [info ] [Thread-1  ]: 34 of 47 OK created table model dbt_anomaly_detection.trade_off_min_anomalies_results  [[32mCREATE TABLE (0.0 rows, 0 processed)[0m in 2.75s]
[0m21:59:57.303032 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_min_anomalies_results
[0m21:59:57.303784 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_max_RMSD
[0m21:59:57.304302 [info ] [Thread-1  ]: 35 of 47 START table model dbt_anomaly_detection.trade_off_max_RMSD ............ [RUN]
[0m21:59:57.304783 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.trade_off_max_RMSD"
[0m21:59:57.304929 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.trade_off_max_RMSD
[0m21:59:57.305078 [debug] [Thread-1  ]: Compiling model.anomaly_detection.trade_off_max_RMSD
[0m21:59:57.310620 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.trade_off_max_RMSD"
[0m21:59:57.312082 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:57.312259 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.trade_off_max_RMSD
[0m21:59:57.314947 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.trade_off_max_RMSD"
[0m21:59:57.315311 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:59:57.315456 [debug] [Thread-1  ]: On model.anomaly_detection.trade_off_max_RMSD: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.trade_off_max_RMSD"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_max_RMSD`
  
  
  OPTIONS()
  as (
    

SELECT app_event, LoB, MAX(RMSD_prcnt) AS RMSD_prcnt 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_min_anomalies_results`
  GROUP BY app_event, LoB
  ORDER BY app_event, LoB
  );
  
[0m21:59:59.797229 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:59.798186 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105acbbe0>]}
[0m21:59:59.798636 [info ] [Thread-1  ]: 35 of 47 OK created table model dbt_anomaly_detection.trade_off_max_RMSD ....... [[32mCREATE TABLE (0.0 rows, 0 processed)[0m in 2.49s]
[0m21:59:59.799035 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_max_RMSD
[0m21:59:59.800088 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_max_RMSD_results
[0m21:59:59.800645 [info ] [Thread-1  ]: 36 of 47 START table model dbt_anomaly_detection.trade_off_max_RMSD_results .... [RUN]
[0m21:59:59.801237 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.trade_off_max_RMSD_results"
[0m21:59:59.801430 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.trade_off_max_RMSD_results
[0m21:59:59.801612 [debug] [Thread-1  ]: Compiling model.anomaly_detection.trade_off_max_RMSD_results
[0m21:59:59.806865 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.trade_off_max_RMSD_results"
[0m21:59:59.807622 [debug] [Thread-1  ]: finished collecting timing info
[0m21:59:59.807775 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.trade_off_max_RMSD_results
[0m21:59:59.811276 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.trade_off_max_RMSD_results"
[0m21:59:59.811862 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:59:59.812063 [debug] [Thread-1  ]: On model.anomaly_detection.trade_off_max_RMSD_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.trade_off_max_RMSD_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_max_RMSD_results`
  
  
  OPTIONS()
  as (
    

SELECT anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.LoB, anomalies_criteria_res_dbt.control_config, 
    anomalies_criteria_res_dbt.anomalies, anomalies_criteria_res_dbt.RMSD_prcnt, anomalies_criteria_res_dbt.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_min_anomalies_results` AS anomalies_criteria_res_dbt
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_max_RMSD` AS min_anomalies_max_RMSD
    ON anomalies_criteria_res_dbt.RMSD_prcnt = min_anomalies_max_RMSD.RMSD_prcnt
      AND anomalies_criteria_res_dbt.app_event = min_anomalies_max_RMSD.app_event
      AND anomalies_criteria_res_dbt.LoB = min_anomalies_max_RMSD.LoB
  ORDER BY anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.LoB, control_config
  );
  
[0m22:00:02.279061 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:02.279986 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ace580>]}
[0m22:00:02.280539 [info ] [Thread-1  ]: 36 of 47 OK created table model dbt_anomaly_detection.trade_off_max_RMSD_results  [[32mCREATE TABLE (0.0 rows, 0 processed)[0m in 2.48s]
[0m22:00:02.281077 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_max_RMSD_results
[0m22:00:02.281816 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events
[0m22:00:02.282413 [info ] [Thread-1  ]: 37 of 47 START table model dbt_anomaly_detection.remaining_events .............. [RUN]
[0m22:00:02.283093 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events"
[0m22:00:02.283272 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events
[0m22:00:02.283434 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events
[0m22:00:02.293404 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events"
[0m22:00:02.295286 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:02.295464 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events
[0m22:00:02.298768 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events"
[0m22:00:02.299146 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:00:02.299304 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events`
  
  
  OPTIONS()
  as (
    

select all_events.app_event as app_event, LoB, control_config, anomalies, RMSD_prcnt, neg_lower
from `ld-snowplow`.`dbt_anomaly_detection`.`ref_distinct_tuples` as all_events
left join `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_max_RMSD_results` as results 
on all_events.app_event = results.app_event 
where control_config is null
  );
  
[0m22:00:05.208037 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:05.209340 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10586e4f0>]}
[0m22:00:05.210076 [info ] [Thread-1  ]: 37 of 47 OK created table model dbt_anomaly_detection.remaining_events ......... [[32mCREATE TABLE (2.0 rows, 30.0 Bytes processed)[0m in 2.93s]
[0m22:00:05.210718 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events
[0m22:00:05.211459 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_features
[0m22:00:05.212114 [info ] [Thread-1  ]: 38 of 47 START table model dbt_anomaly_detection.remaining_events_features ..... [RUN]
[0m22:00:05.212705 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_features"
[0m22:00:05.212903 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_features
[0m22:00:05.213090 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_features
[0m22:00:05.220359 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_features"
[0m22:00:05.221223 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:05.221401 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_features
[0m22:00:05.224942 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_features"
[0m22:00:05.225434 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:00:05.225603 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_features: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_features"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features`
  
  
  OPTIONS()
  as (
    

select remaning_events.app_event as app_event, all_features.LoB as LoB, all_features.control_config as control_config, all_features.anomalies as anomalies, all_features.RMSD_prcnt as RMSD_prcnt, all_features.neg_lower as neg_lower
from `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events` as remaning_events
inner join `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt` as all_features
on remaning_events.app_event = all_features.app_event
  );
  
[0m22:00:07.753737 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:07.754456 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a789d0>]}
[0m22:00:07.754825 [info ] [Thread-1  ]: 38 of 47 OK created table model dbt_anomaly_detection.remaining_events_features  [[32mCREATE TABLE (4.0 rows, 322.0 Bytes processed)[0m in 2.54s]
[0m22:00:07.755212 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_features
[0m22:00:07.756509 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_features_null_filtered
[0m22:00:07.756914 [info ] [Thread-1  ]: 39 of 47 START table model dbt_anomaly_detection.remaining_events_features_null_filtered  [RUN]
[0m22:00:07.757299 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_features_null_filtered"
[0m22:00:07.757424 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_features_null_filtered
[0m22:00:07.757548 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_features_null_filtered
[0m22:00:07.761087 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_features_null_filtered"
[0m22:00:07.761887 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:07.762027 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_features_null_filtered
[0m22:00:07.766357 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_features_null_filtered"
[0m22:00:07.766834 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:00:07.766984 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_features_null_filtered: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_features_null_filtered"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features`
where RMSD_prcnt is not null
  );
  
[0m22:00:09.944770 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:09.945796 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a896d0>]}
[0m22:00:09.946350 [info ] [Thread-1  ]: 39 of 47 OK created table model dbt_anomaly_detection.remaining_events_features_null_filtered  [[32mCREATE TABLE (4.0 rows, 292.0 Bytes processed)[0m in 2.19s]
[0m22:00:09.946905 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_features_null_filtered
[0m22:00:09.947863 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies
[0m22:00:09.948488 [info ] [Thread-1  ]: 40 of 47 START table model dbt_anomaly_detection.remaining_events_min_anomalies  [RUN]
[0m22:00:09.949181 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies"
[0m22:00:09.949397 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies
[0m22:00:09.949599 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies
[0m22:00:09.957210 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies"
[0m22:00:09.958024 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:09.958180 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies
[0m22:00:09.961687 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_anomalies"
[0m22:00:09.962187 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:00:09.962368 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies`
  
  
  OPTIONS()
  as (
    

SELECT app_event, LoB, MIN(anomalies) AS anomalies 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered`
  GROUP BY app_event, LoB
  ORDER BY app_event, LoB
  );
  
[0m22:00:13.131129 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:13.131500 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058aaf40>]}
[0m22:00:13.131719 [info ] [Thread-1  ]: 40 of 47 OK created table model dbt_anomaly_detection.remaining_events_min_anomalies  [[32mCREATE TABLE (2.0 rows, 120.0 Bytes processed)[0m in 3.18s]
[0m22:00:13.131948 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies
[0m22:00:13.132301 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m22:00:13.132525 [info ] [Thread-1  ]: 41 of 47 START table model dbt_anomaly_detection.remaining_events_min_anomalies_results  [RUN]
[0m22:00:13.132796 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m22:00:13.132884 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies_results
[0m22:00:13.132965 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies_results
[0m22:00:13.136328 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m22:00:13.136773 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:13.136894 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies_results
[0m22:00:13.139068 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m22:00:13.139400 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:00:13.139525 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_anomalies_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_anomalies_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results`
  
  
  OPTIONS()
  as (
    

SELECT neg_lower_criteria_res.app_event, neg_lower_criteria_res.LoB, neg_lower_criteria_res.control_config, 
    neg_lower_criteria_res.anomalies, neg_lower_criteria_res.RMSD_prcnt, neg_lower_criteria_res.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered` AS neg_lower_criteria_res
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies` AS min_neg_lower_min_anomalies
    ON neg_lower_criteria_res.anomalies = min_neg_lower_min_anomalies.anomalies
      AND neg_lower_criteria_res.app_event = min_neg_lower_min_anomalies.app_event
      AND neg_lower_criteria_res.LoB = min_neg_lower_min_anomalies.LoB
  ORDER BY neg_lower_criteria_res.app_event, neg_lower_criteria_res.LoB, RMSD_prcnt DESC
  );
  
[0m22:00:15.761736 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:15.762860 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058931c0>]}
[0m22:00:15.763431 [info ] [Thread-1  ]: 41 of 47 OK created table model dbt_anomaly_detection.remaining_events_min_anomalies_results  [[32mCREATE TABLE (3.0 rows, 352.0 Bytes processed)[0m in 2.63s]
[0m22:00:15.763993 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m22:00:15.764901 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD
[0m22:00:15.765279 [info ] [Thread-1  ]: 42 of 47 START table model dbt_anomaly_detection.remaining_events_min_RMSD ..... [RUN]
[0m22:00:15.765762 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD"
[0m22:00:15.765928 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD
[0m22:00:15.766092 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD
[0m22:00:15.770936 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD"
[0m22:00:15.771913 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:15.772070 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD
[0m22:00:15.779033 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_RMSD"
[0m22:00:15.779617 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:00:15.779790 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_RMSD: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_RMSD"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD`
  
  
  OPTIONS()
  as (
    

SELECT app_event, LoB, MIN(RMSD_prcnt) AS RMSD_prcnt 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results`
  GROUP BY app_event, LoB
  ORDER BY app_event, LoB
  );
  
[0m22:00:18.238604 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:18.239001 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ac5e80>]}
[0m22:00:18.239234 [info ] [Thread-1  ]: 42 of 47 OK created table model dbt_anomaly_detection.remaining_events_min_RMSD  [[32mCREATE TABLE (2.0 rows, 90.0 Bytes processed)[0m in 2.47s]
[0m22:00:18.239485 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD
[0m22:00:18.239759 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m22:00:18.239966 [info ] [Thread-1  ]: 43 of 47 START table model dbt_anomaly_detection.remaining_events_min_RMSD_results  [RUN]
[0m22:00:18.240291 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m22:00:18.240412 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD_results
[0m22:00:18.240511 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD_results
[0m22:00:18.244028 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m22:00:18.244480 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:18.244589 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD_results
[0m22:00:18.246957 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m22:00:18.247255 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:00:18.247375 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_RMSD_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_RMSD_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
  
  
  OPTIONS()
  as (
    

SELECT anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.LoB, anomalies_criteria_res_dbt.control_config, 
    anomalies_criteria_res_dbt.anomalies, anomalies_criteria_res_dbt.RMSD_prcnt, anomalies_criteria_res_dbt.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results` AS anomalies_criteria_res_dbt
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD` AS min_anomalies_max_RMSD
    ON anomalies_criteria_res_dbt.RMSD_prcnt = min_anomalies_max_RMSD.RMSD_prcnt
      AND anomalies_criteria_res_dbt.app_event = min_anomalies_max_RMSD.app_event
      AND anomalies_criteria_res_dbt.LoB = min_anomalies_max_RMSD.LoB
  ORDER BY anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.LoB, control_config
  );
  
[0m22:00:21.140819 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:21.141787 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059eb6d0>]}
[0m22:00:21.142358 [info ] [Thread-1  ]: 43 of 47 OK created table model dbt_anomaly_detection.remaining_events_min_RMSD_results  [[32mCREATE TABLE (2.0 rows, 280.0 Bytes processed)[0m in 2.90s]
[0m22:00:21.142945 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m22:00:21.143626 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_models
[0m22:00:21.144224 [info ] [Thread-1  ]: 44 of 47 START table model dbt_anomaly_detection.all_models .................... [RUN]
[0m22:00:21.144849 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_models"
[0m22:00:21.145060 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_models
[0m22:00:21.145254 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_models
[0m22:00:21.152231 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_models"
[0m22:00:21.153090 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:21.153274 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_models
[0m22:00:21.156792 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_models"
[0m22:00:21.157254 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:00:21.157438 [debug] [Thread-1  ]: On model.anomaly_detection.all_models: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_models"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_models`
  
  
  OPTIONS()
  as (
    

     select *
     from `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
     union all
     select *
     from `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_max_RMSD_results`
     order by app_event, LoB
  );
  
[0m22:00:23.708332 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:23.709304 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b56430>]}
[0m22:00:23.709863 [info ] [Thread-1  ]: 44 of 47 OK created table model dbt_anomaly_detection.all_models ............... [[32mCREATE TABLE (2.0 rows, 146.0 Bytes processed)[0m in 2.56s]
[0m22:00:23.710437 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_models
[0m22:00:23.711179 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_all_models
[0m22:00:23.711639 [info ] [Thread-1  ]: 45 of 47 START table model dbt_anomaly_detection.filtered_all_models ........... [RUN]
[0m22:00:23.712105 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_all_models"
[0m22:00:23.712267 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_all_models
[0m22:00:23.712427 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_all_models
[0m22:00:23.720075 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_all_models"
[0m22:00:23.720911 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:23.721068 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_all_models
[0m22:00:23.725581 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.filtered_all_models"
[0m22:00:23.726077 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:00:23.726243 [debug] [Thread-1  ]: On model.anomaly_detection.filtered_all_models: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.filtered_all_models"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`filtered_all_models`
  
  
  OPTIONS()
  as (
    
SELECT app_event, LoB, control_config, anomalies, RMSD_prcnt, neg_lower, 
  CASE WHEN (neg_lower = 0 AND anomalies < 5 AND RMSD_prcnt > 0.40 AND RMSD_prcnt < 5.00 ) THEN "ideal_model" ELSE "non_ideal_model" END AS model_quality_tag,
FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_models`
WHERE app_event LIKE '%add%'
UNION ALL 
SELECT app_event, LoB, control_config, anomalies, RMSD_prcnt, neg_lower, 
  CASE WHEN (neg_lower = 0 AND anomalies < 5 AND RMSD_prcnt > 0.40 AND RMSD_prcnt < 5.00 ) THEN "ideal_model" ELSE "non_ideal_model" END AS model_quality_tag,
FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_models`
WHERE app_event NOT LIKE '%ad%'
ORDER BY app_event, LoB
  );
  
[0m22:00:25.952993 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:25.953946 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ae1b50>]}
[0m22:00:25.954511 [info ] [Thread-1  ]: 45 of 47 OK created table model dbt_anomaly_detection.filtered_all_models ...... [[32mCREATE TABLE (2.0 rows, 146.0 Bytes processed)[0m in 2.24s]
[0m22:00:25.955065 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_all_models
[0m22:00:25.956005 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m22:00:25.956520 [info ] [Thread-1  ]: 46 of 47 START table model dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [RUN]
[0m22:00:25.957043 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m22:00:25.957207 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m22:00:25.957367 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m22:00:25.964035 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m22:00:25.964778 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:25.964938 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m22:00:25.968456 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m22:00:25.968967 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:00:25.969178 [debug] [Thread-1  ]: On model.anomaly_detection.derived_alerts_fired_automation_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_alerts_fired_automation_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_alerts_fired_automation_dbt`
  
  
  OPTIONS()
  as (
    
  
  WITH ml_detect_updated AS (
    SELECT app_event, LoB, 
      CONCAT(agg_tag, '_', prob_threshold, "threshold", '_', RTRIM(LTRIM(training_period, "derived_models_"), CONCAT('_', agg_tag))) AS control_config,
      time_stamps, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked`
  )
  SELECT time_stamps, all_configs.app_event AS app_event, all_configs.LoB AS LoB, control_table.control_config AS control_config, 
    anomalies, RMSD_prcnt, neg_lower, model_quality_tag,
    event_count, lower_bound, upper_bound, anomaly_probability, is_anomaly
  FROM ml_detect_updated AS all_configs 
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`filtered_all_models` AS control_table 
    ON all_configs.app_event = control_table.app_event
      AND all_configs.LoB = control_table.LoB
      AND all_configs.control_config = control_table.control_config
  ORDER BY all_configs.app_event, all_configs.LoB, time_stamps
  );
  
[0m22:00:29.019451 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:29.020577 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a8ed90>]}
[0m22:00:29.021170 [info ] [Thread-1  ]: 46 of 47 OK created table model dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [[32mCREATE TABLE (120.0 rows, 24.1 KB processed)[0m in 3.06s]
[0m22:00:29.021761 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m22:00:29.022840 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_events_with_anomalies
[0m22:00:29.023186 [info ] [Thread-1  ]: 47 of 47 START table model dbt_anomaly_detection.derived_events_with_anomalies . [RUN]
[0m22:00:29.023764 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_events_with_anomalies"
[0m22:00:29.023962 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_events_with_anomalies
[0m22:00:29.024151 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_events_with_anomalies
[0m22:00:29.030635 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_events_with_anomalies"
[0m22:00:29.031420 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:29.031573 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_events_with_anomalies
[0m22:00:29.035505 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_events_with_anomalies"
[0m22:00:29.036218 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:00:29.036455 [debug] [Thread-1  ]: On model.anomaly_detection.derived_events_with_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_events_with_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_events_with_anomalies`
  
  
  OPTIONS()
  as (
    
SELECT time_stamps, app_event, LoB, event_count AS event_count_anomalous_yesterday
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_alerts_fired_automation_dbt`
WHERE DATE(time_stamps) = CURRENT_DATE() - 1 
  AND is_anomaly
  );
  
[0m22:00:31.767880 [debug] [Thread-1  ]: finished collecting timing info
[0m22:00:31.769284 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55747c7f-57e7-4d4e-a2a5-d950768aacc9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ba0c40>]}
[0m22:00:31.769975 [info ] [Thread-1  ]: 47 of 47 OK created table model dbt_anomaly_detection.derived_events_with_anomalies  [[32mCREATE TABLE (6.0 rows, 4.6 KB processed)[0m in 2.75s]
[0m22:00:31.770575 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_events_with_anomalies
[0m22:00:31.772336 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m22:00:31.773064 [info ] [MainThread]: 
[0m22:00:31.773405 [info ] [MainThread]: Finished running 35 table models, 12 model models in 0 hours 4 minutes and 2.17 seconds (242.17s).
[0m22:00:31.773706 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:00:31.773861 [debug] [MainThread]: Connection 'model.anomaly_detection.derived_events_with_anomalies' was properly closed.
[0m22:00:31.793771 [info ] [MainThread]: 
[0m22:00:31.794033 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:00:31.794268 [info ] [MainThread]: 
[0m22:00:31.794415 [info ] [MainThread]: Done. PASS=47 WARN=0 ERROR=0 SKIP=0 TOTAL=47
[0m22:00:31.794652 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b22b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053b43d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10545aaf0>]}
[0m22:00:31.794819 [debug] [MainThread]: Flushing usage events


============================== 2023-02-09 22:07:10.238301 | b27177a3-1f51-4bdc-a540-c63609dca2eb ==============================
[0m22:07:10.238320 [info ] [MainThread]: Running with dbt=1.2.1
[0m22:07:10.238943 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:07:10.239107 [debug] [MainThread]: Tracking: tracking
[0m22:07:10.247967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056e5b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056e56d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056e5970>]}
[0m22:07:10.307906 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 11 files changed.
[0m22:07:10.308286 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_05mon_12hr.sql
[0m22:07:10.308417 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_1mon_12hr.sql
[0m22:07:10.308536 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_2mon_24hr.sql
[0m22:07:10.308640 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_05mon_4hr.sql
[0m22:07:10.308742 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_2mon_8hr.sql
[0m22:07:10.308848 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_05mon_8hr.sql
[0m22:07:10.308953 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_2mon_12hr.sql
[0m22:07:10.309056 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_1mon_8hr.sql
[0m22:07:10.309164 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_ml_detect.sql
[0m22:07:10.309264 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_05mon_24hr.sql
[0m22:07:10.309365 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_2mon_4hr.sql
[0m22:07:10.317451 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_12hr.sql
[0m22:07:10.324496 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_12hr.sql
[0m22:07:10.325115 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_12hr.sql
[0m22:07:10.327780 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_12hr.sql
[0m22:07:10.328396 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_24hr.sql
[0m22:07:10.332537 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_24hr.sql
[0m22:07:10.333109 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_4hr.sql
[0m22:07:10.336055 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_4hr.sql
[0m22:07:10.336724 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_8hr.sql
[0m22:07:10.340162 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_8hr.sql
[0m22:07:10.340724 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_8hr.sql
[0m22:07:10.343555 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_8hr.sql
[0m22:07:10.344176 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_12hr.sql
[0m22:07:10.347092 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_12hr.sql
[0m22:07:10.347697 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_8hr.sql
[0m22:07:10.350566 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_8hr.sql
[0m22:07:10.351314 [debug] [MainThread]: 1603: static parser failed on example/derived_ml_detect.sql
[0m22:07:10.358993 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_ml_detect.sql
[0m22:07:10.359574 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_24hr.sql
[0m22:07:10.362927 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_24hr.sql
[0m22:07:10.363600 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_4hr.sql
[0m22:07:10.395156 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_4hr.sql
[0m22:07:10.411721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10633b0d0>]}
[0m22:07:10.419287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062a9be0>]}
[0m22:07:10.419428 [info ] [MainThread]: Found 48 models, 0 tests, 0 snapshots, 0 analyses, 540 macros, 0 operations, 0 seed files, 2 sources, 0 exposures, 0 metrics
[0m22:07:10.419549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062a9b80>]}
[0m22:07:10.420930 [info ] [MainThread]: 
[0m22:07:10.421170 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m22:07:10.422688 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow"
[0m22:07:10.422816 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:07:11.934881 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow_dbt_anomaly_detection"
[0m22:07:11.935572 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:07:12.367372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062ad490>]}
[0m22:07:12.368682 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:07:12.369072 [info ] [MainThread]: 
[0m22:07:12.376383 [debug] [Thread-1  ]: Began running node model.anomaly_detection.reference_derived
[0m22:07:12.376903 [info ] [Thread-1  ]: 1 of 47 START table model dbt_anomaly_detection.reference_derived .............. [RUN]
[0m22:07:12.377506 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.reference_derived"
[0m22:07:12.377703 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.reference_derived
[0m22:07:12.377988 [debug] [Thread-1  ]: Compiling model.anomaly_detection.reference_derived
[0m22:07:12.383409 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.reference_derived"
[0m22:07:12.384101 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:12.384277 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.reference_derived
[0m22:07:12.401401 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:07:12.853240 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.reference_derived"
[0m22:07:12.854470 [debug] [Thread-1  ]: On model.anomaly_detection.reference_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.reference_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived`
  
  
  OPTIONS()
  as (
    
SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`web_purchase_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY) AND DATE(collector_tstamp) < CURRENT_DATE()

UNION ALL

SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`pco_web_apply_filter_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY) AND DATE(collector_tstamp) < CURRENT_DATE()
  );
  
[0m22:07:17.588422 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:17.588996 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1063c1b20>]}
[0m22:07:17.589290 [info ] [Thread-1  ]: 1 of 47 OK created table model dbt_anomaly_detection.reference_derived ......... [[32mCREATE TABLE (1.1m rows, 34.8 MB processed)[0m in 5.21s]
[0m22:07:17.589607 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.reference_derived
[0m22:07:17.590190 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ref_including_LoBs
[0m22:07:17.590630 [info ] [Thread-1  ]: 2 of 47 START table model dbt_anomaly_detection.derived_ref_including_LoBs ..... [RUN]
[0m22:07:17.591004 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_ref_including_LoBs"
[0m22:07:17.591114 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_ref_including_LoBs
[0m22:07:17.591221 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_ref_including_LoBs
[0m22:07:17.594030 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_ref_including_LoBs"
[0m22:07:17.594644 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:17.594759 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_ref_including_LoBs
[0m22:07:17.596751 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:07:18.040906 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_ref_including_LoBs"
[0m22:07:18.043010 [debug] [Thread-1  ]: On model.anomaly_detection.derived_ref_including_LoBs: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_ref_including_LoBs"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs`
  
  
  OPTIONS()
  as (
    

SELECT collector_tstamp, event, user_event_name, app_id,
    CASE
    WHEN LOWER(app_id) LIKE '%pcexpress%' OR LOWER(app_id) LIKE '%pcx%' THEN "PCX"
    WHEN LOWER(app_id) LIKE '%sdm%' OR LOWER(app_id) LIKE '%beauty%' THEN "SDM Shop"
    WHEN LOWER(app_id) LIKE '%drx%' THEN "SDM Drx" 
    -- WHEN LOWER(page_urlhost) like '%pcoptimum.ca%' and (LOWER(app_id) like '%ios%' or LOWER(app_id) like '%android%') then 'PC Optimum' 
    -- not recommended to use page_urlhost for mobile apps
    WHEN LOWER(app_id) like '%pco%' THEN 'PC Optimum'
    -- WHEN LOWER(page_urlhost) like '%presidentschoice.ca%' THEN 'PC' -- placeholder for after the launch 
    -- WHEN LOWER(page_urlhost) like '%joefresh.com%' and LOWER(app_id) like '%ios%' then 'JF' -- placeholder for after the launch 
    WHEN LOWER(app_id) like '%jf%' THEN 'JF'
    ELSE app_id END AS LoB,
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived`
  );
  
[0m22:07:23.064228 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:23.065944 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106600d30>]}
[0m22:07:23.066505 [info ] [Thread-1  ]: 2 of 47 OK created table model dbt_anomaly_detection.derived_ref_including_LoBs  [[32mCREATE TABLE (1.1m rows, 34.8 MB processed)[0m in 5.47s]
[0m22:07:23.067036 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ref_including_LoBs
[0m22:07:23.067872 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived
[0m22:07:23.068326 [info ] [Thread-1  ]: 3 of 47 START table model dbt_anomaly_detection.all_agg_derived ................ [RUN]
[0m22:07:23.068800 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived"
[0m22:07:23.068946 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived
[0m22:07:23.069089 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived
[0m22:07:23.076346 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived"
[0m22:07:23.077165 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:23.077309 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived
[0m22:07:23.079592 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:07:23.325600 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived"
[0m22:07:23.326682 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
  
  
  OPTIONS()
  as (
    


SELECT "4hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_4hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/4)*4 AS _4hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _4hr_trunc,
    app_id,
    LoB,
    event_type)

UNION ALL


SELECT "8hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_8hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/8)*8 AS _8hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _8hr_trunc,
    app_id,
    LoB,
    event_type)

UNION ALL


SELECT "12hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_12hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/12)*12 AS _12hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _12hr_trunc,
    app_id,
    LoB,
    event_type)

UNION ALL


SELECT "24hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_24hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/24)*24 AS _24hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _24hr_trunc,
    app_id,
    LoB,
    event_type)

ORDER BY
  agg_tag,
  time_stamps,
  app_id,
  LoB,
  event_type


  );
  
[0m22:07:28.150889 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:28.151839 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10661e910>]}
[0m22:07:28.152292 [info ] [Thread-1  ]: 3 of 47 OK created table model dbt_anomaly_detection.all_agg_derived ........... [[32mCREATE TABLE (2.2k rows, 42.0 MB processed)[0m in 5.08s]
[0m22:07:28.153111 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived
[0m22:07:28.154078 [debug] [Thread-1  ]: Began running node model.anomaly_detection.count_cutoff
[0m22:07:28.154577 [info ] [Thread-1  ]: 4 of 47 START table model dbt_anomaly_detection.count_cutoff ................... [RUN]
[0m22:07:28.155156 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.count_cutoff"
[0m22:07:28.155323 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.count_cutoff
[0m22:07:28.155478 [debug] [Thread-1  ]: Compiling model.anomaly_detection.count_cutoff
[0m22:07:28.160498 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.count_cutoff"
[0m22:07:28.161138 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:28.161292 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.count_cutoff
[0m22:07:28.163970 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:07:28.577306 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.count_cutoff"
[0m22:07:28.578634 [debug] [Thread-1  ]: On model.anomaly_detection.count_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.count_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, LoB, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select agg_tag, app_event, LoB, min(time_stamps) as strt_time
from pairs
where event_count > 50
group by app_event, LoB, agg_tag 
order by app_event, LoB, agg_tag
  );
  
[0m22:07:31.034155 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:31.036000 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066049d0>]}
[0m22:07:31.036504 [info ] [Thread-1  ]: 4 of 47 OK created table model dbt_anomaly_detection.count_cutoff .............. [[32mCREATE TABLE (8.0 rows, 93.3 KB processed)[0m in 2.88s]
[0m22:07:31.037006 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.count_cutoff
[0m22:07:31.037241 [debug] [Thread-1  ]: Began running node model.anomaly_detection.dev_ml_model_constraints
[0m22:07:31.037754 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.dev_ml_model_constraints"
[0m22:07:31.038053 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.dev_ml_model_constraints
[0m22:07:31.038199 [debug] [Thread-1  ]: Compiling model.anomaly_detection.dev_ml_model_constraints
[0m22:07:31.044583 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.dev_ml_model_constraints"
[0m22:07:31.045354 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:31.045750 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.dev_ml_model_constraints
[0m22:07:31.045903 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff
[0m22:07:31.046247 [info ] [Thread-1  ]: 5 of 47 START table model dbt_anomaly_detection.all_agg_derived_cutoff ......... [RUN]
[0m22:07:31.046752 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff"
[0m22:07:31.046884 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff
[0m22:07:31.046996 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff
[0m22:07:31.050044 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m22:07:31.050446 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:31.050571 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff
[0m22:07:31.052557 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:07:31.452347 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m22:07:31.453690 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, LoB, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select time_stamps, app_event, LoB, agg_tag, event_count
from(
select time_stamps, strt_time, pairs.app_event, pairs.LoB, pairs.agg_tag, event_count
from pairs
inner join `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff` as cutoff
on pairs.agg_tag = cutoff.agg_tag
and pairs.app_event = cutoff.app_event
and pairs.LoB = cutoff.LoB
order by pairs.app_event, pairs.LoB, pairs.agg_tag, time_stamps)
where time_stamps >= strt_time
  );
  
[0m22:07:37.312929 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:37.314876 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062b0520>]}
[0m22:07:37.315611 [info ] [Thread-1  ]: 5 of 47 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff .... [[32mCREATE TABLE (2.2k rows, 93.6 KB processed)[0m in 6.27s]
[0m22:07:37.316117 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff
[0m22:07:37.316954 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m22:07:37.317316 [info ] [Thread-1  ]: 6 of 47 START table model dbt_anomaly_detection.all_agg_derived_cutoff_long .... [RUN]
[0m22:07:37.317829 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m22:07:37.317976 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_long
[0m22:07:37.318112 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_long
[0m22:07:37.324268 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m22:07:37.325561 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:37.325694 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_long
[0m22:07:37.327657 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:07:37.734229 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m22:07:37.736499 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB(CURRENT_DATE(), INTERVAL 10 DAY)
  );
  
[0m22:07:39.934372 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:39.937472 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106513070>]}
[0m22:07:39.938120 [info ] [Thread-1  ]: 6 of 47 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_long  [[32mCREATE TABLE (1.9k rows, 91.2 KB processed)[0m in 2.62s]
[0m22:07:39.938640 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m22:07:39.938865 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m22:07:39.939105 [info ] [Thread-1  ]: 7 of 47 START table model dbt_anomaly_detection.all_agg_derived_cutoff_short ... [RUN]
[0m22:07:39.940219 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m22:07:39.940535 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_short
[0m22:07:39.940731 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_short
[0m22:07:39.944874 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m22:07:39.946073 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:39.946420 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_short
[0m22:07:39.949389 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:07:40.341069 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m22:07:40.341954 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB(CURRENT_DATE(), INTERVAL 15 DAY)
  );
  
[0m22:07:42.633412 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:42.634458 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066028e0>]}
[0m22:07:42.634952 [info ] [Thread-1  ]: 7 of 47 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_short  [[32mCREATE TABLE (1.8k rows, 91.2 KB processed)[0m in 2.69s]
[0m22:07:42.635416 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m22:07:42.635620 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_nonrecent_events
[0m22:07:42.636092 [info ] [Thread-1  ]: 8 of 47 START table model dbt_anomaly_detection.derived_nonrecent_events ....... [RUN]
[0m22:07:42.636880 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_nonrecent_events"
[0m22:07:42.637121 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_nonrecent_events
[0m22:07:42.637294 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_nonrecent_events
[0m22:07:42.644462 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m22:07:42.645282 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:42.645477 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_nonrecent_events
[0m22:07:42.647856 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:07:43.043359 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m22:07:43.044382 [debug] [Thread-1  ]: On model.anomaly_detection.derived_nonrecent_events: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_nonrecent_events"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events`
  
  
  OPTIONS()
  as (
    

SELECT MIN(time_stamps) AS strt_time, app_event, LoB 
FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
GROUP BY app_event, LoB
HAVING DATE(MIN(time_stamps)) < DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
ORDER BY strt_time
  );
  
[0m22:07:45.128759 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:45.129476 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065f52e0>]}
[0m22:07:45.129829 [info ] [Thread-1  ]: 8 of 47 OK created table model dbt_anomaly_detection.derived_nonrecent_events .. [[32mCREATE TABLE (2.0 rows, 63.3 KB processed)[0m in 2.49s]
[0m22:07:45.130185 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_nonrecent_events
[0m22:07:45.130345 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_long
[0m22:07:45.130520 [info ] [Thread-1  ]: 9 of 47 START table model dbt_anomaly_detection.aggregation_quartiles_long ..... [RUN]
[0m22:07:45.131001 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_long"
[0m22:07:45.131152 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_long
[0m22:07:45.131280 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_long
[0m22:07:45.135899 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m22:07:45.137002 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:45.137168 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_long
[0m22:07:45.139208 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:07:45.538634 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m22:07:45.539685 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m22:07:47.938831 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:47.939907 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065ec190>]}
[0m22:07:47.940461 [info ] [Thread-1  ]: 9 of 47 OK created table model dbt_anomaly_detection.aggregation_quartiles_long  [[32mCREATE TABLE (8.0 rows, 53.0 KB processed)[0m in 2.81s]
[0m22:07:47.941027 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_long
[0m22:07:47.941289 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_short
[0m22:07:47.941823 [info ] [Thread-1  ]: 10 of 47 START table model dbt_anomaly_detection.aggregation_quartiles_short ... [RUN]
[0m22:07:47.943089 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_short"
[0m22:07:47.943341 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_short
[0m22:07:47.943547 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_short
[0m22:07:47.948273 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m22:07:47.949085 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:47.949261 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_short
[0m22:07:47.951983 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:07:48.364370 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m22:07:48.365974 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m22:07:50.899410 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:50.900298 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106615130>]}
[0m22:07:50.900792 [info ] [Thread-1  ]: 10 of 47 OK created table model dbt_anomaly_detection.aggregation_quartiles_short  [[32mCREATE TABLE (8.0 rows, 49.7 KB processed)[0m in 2.96s]
[0m22:07:50.901266 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_short
[0m22:07:50.901493 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_long
[0m22:07:50.901903 [info ] [Thread-1  ]: 11 of 47 START table model dbt_anomaly_detection.aggregation_bounds_long ....... [RUN]
[0m22:07:50.902575 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_long"
[0m22:07:50.902742 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_long
[0m22:07:50.902896 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_long
[0m22:07:50.907090 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m22:07:50.907997 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:50.908148 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_long
[0m22:07:50.912987 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:07:51.320663 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m22:07:51.322029 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m22:07:53.229128 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:53.230238 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106644160>]}
[0m22:07:53.230968 [info ] [Thread-1  ]: 11 of 47 OK created table model dbt_anomaly_detection.aggregation_bounds_long .. [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.33s]
[0m22:07:53.231545 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_long
[0m22:07:53.231806 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_short
[0m22:07:53.232052 [info ] [Thread-1  ]: 12 of 47 START table model dbt_anomaly_detection.aggregation_bounds_short ...... [RUN]
[0m22:07:53.232554 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_short"
[0m22:07:53.232917 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_short
[0m22:07:53.233370 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_short
[0m22:07:53.237787 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m22:07:53.238469 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:53.238620 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_short
[0m22:07:53.241094 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:07:53.636472 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m22:07:53.637568 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m22:07:55.764322 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:55.765390 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106605af0>]}
[0m22:07:55.765956 [info ] [Thread-1  ]: 12 of 47 OK created table model dbt_anomaly_detection.aggregation_bounds_short . [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.53s]
[0m22:07:55.766514 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_short
[0m22:07:55.766778 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_long
[0m22:07:55.767072 [info ] [Thread-1  ]: 13 of 47 START table model dbt_anomaly_detection.aggregation_outliers_long ..... [RUN]
[0m22:07:55.767943 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_long"
[0m22:07:55.768142 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_long
[0m22:07:55.768337 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_long
[0m22:07:55.773328 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m22:07:55.775886 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:55.776145 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_long
[0m22:07:55.779268 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:07:56.222999 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m22:07:56.225662 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, LoB, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag, LoB,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m22:07:58.399817 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:58.400905 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10664c040>]}
[0m22:07:58.401463 [info ] [Thread-1  ]: 13 of 47 OK created table model dbt_anomaly_detection.aggregation_outliers_long  [[32mCREATE TABLE (1.9k rows, 81.4 KB processed)[0m in 2.63s]
[0m22:07:58.402033 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_long
[0m22:07:58.402298 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_short
[0m22:07:58.403061 [info ] [Thread-1  ]: 14 of 47 START table model dbt_anomaly_detection.aggregation_outliers_short .... [RUN]
[0m22:07:58.403558 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_short"
[0m22:07:58.403721 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_short
[0m22:07:58.403880 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_short
[0m22:07:58.412324 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m22:07:58.413161 [debug] [Thread-1  ]: finished collecting timing info
[0m22:07:58.413344 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_short
[0m22:07:58.418781 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:07:58.812358 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m22:07:58.813810 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, LoB, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag, LoB,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m22:08:01.522360 [debug] [Thread-1  ]: finished collecting timing info
[0m22:08:01.523422 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10665aaf0>]}
[0m22:08:01.523975 [info ] [Thread-1  ]: 14 of 47 OK created table model dbt_anomaly_detection.aggregation_outliers_short  [[32mCREATE TABLE (1.8k rows, 76.3 KB processed)[0m in 3.12s]
[0m22:08:01.524547 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_short
[0m22:08:01.524809 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_4hr
[0m22:08:01.525552 [info ] [Thread-1  ]: 15 of 47 START model model dbt_anomaly_detection.derived_models_05mon_4hr ...... [RUN]
[0m22:08:01.526240 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_4hr"
[0m22:08:01.526478 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_4hr
[0m22:08:01.526689 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_4hr
[0m22:08:01.532909 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m22:08:01.533683 [debug] [Thread-1  ]: finished collecting timing info
[0m22:08:01.533840 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_4hr
[0m22:08:01.550219 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m22:08:01.550759 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:08:01.550914 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:08:12.687879 [debug] [Thread-1  ]: finished collecting timing info
[0m22:08:12.690213 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106692f70>]}
[0m22:08:12.690769 [info ] [Thread-1  ]: 15 of 47 OK created model model dbt_anomaly_detection.derived_models_05mon_4hr . [[32mOK[0m in 11.16s]
[0m22:08:12.691334 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_4hr
[0m22:08:12.691606 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_8hr
[0m22:08:12.692327 [info ] [Thread-1  ]: 16 of 47 START model model dbt_anomaly_detection.derived_models_05mon_8hr ...... [RUN]
[0m22:08:12.693136 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_8hr"
[0m22:08:12.693324 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_8hr
[0m22:08:12.693491 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_8hr
[0m22:08:12.701864 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m22:08:12.702753 [debug] [Thread-1  ]: finished collecting timing info
[0m22:08:12.702931 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_8hr
[0m22:08:12.706341 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m22:08:12.706826 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:08:12.707026 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:08:24.606836 [debug] [Thread-1  ]: finished collecting timing info
[0m22:08:24.607895 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106591e80>]}
[0m22:08:24.608482 [info ] [Thread-1  ]: 16 of 47 OK created model model dbt_anomaly_detection.derived_models_05mon_8hr . [[32mOK[0m in 11.91s]
[0m22:08:24.609063 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_8hr
[0m22:08:24.609335 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_4hr
[0m22:08:24.609774 [info ] [Thread-1  ]: 17 of 47 START model model dbt_anomaly_detection.derived_models_1mon_4hr ....... [RUN]
[0m22:08:24.610330 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_4hr"
[0m22:08:24.610513 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_4hr
[0m22:08:24.610687 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_4hr
[0m22:08:24.621894 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m22:08:24.622596 [debug] [Thread-1  ]: finished collecting timing info
[0m22:08:24.622740 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_4hr
[0m22:08:24.625503 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m22:08:24.625888 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:08:24.626057 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:08:36.338123 [debug] [Thread-1  ]: finished collecting timing info
[0m22:08:36.340511 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106609a90>]}
[0m22:08:36.341110 [info ] [Thread-1  ]: 17 of 47 OK created model model dbt_anomaly_detection.derived_models_1mon_4hr .. [[32mOK[0m in 11.73s]
[0m22:08:36.341706 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_4hr
[0m22:08:36.341988 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_8hr
[0m22:08:36.342274 [info ] [Thread-1  ]: 18 of 47 START model model dbt_anomaly_detection.derived_models_1mon_8hr ....... [RUN]
[0m22:08:36.343269 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_8hr"
[0m22:08:36.343467 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_8hr
[0m22:08:36.343629 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_8hr
[0m22:08:36.351847 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m22:08:36.352521 [debug] [Thread-1  ]: finished collecting timing info
[0m22:08:36.352680 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_8hr
[0m22:08:36.355815 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m22:08:36.356299 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:08:36.356473 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:08:49.059893 [debug] [Thread-1  ]: finished collecting timing info
[0m22:08:49.060888 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065ff790>]}
[0m22:08:49.061341 [info ] [Thread-1  ]: 18 of 47 OK created model model dbt_anomaly_detection.derived_models_1mon_8hr .. [[32mOK[0m in 12.72s]
[0m22:08:49.061878 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_8hr
[0m22:08:49.062110 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_4hr
[0m22:08:49.062374 [info ] [Thread-1  ]: 19 of 47 START model model dbt_anomaly_detection.derived_models_2mon_4hr ....... [RUN]
[0m22:08:49.062885 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_4hr"
[0m22:08:49.063092 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_4hr
[0m22:08:49.063521 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_4hr
[0m22:08:49.072408 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m22:08:49.073375 [debug] [Thread-1  ]: finished collecting timing info
[0m22:08:49.073577 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_4hr
[0m22:08:49.078465 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m22:08:49.078969 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:08:49.079105 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:09:02.789733 [debug] [Thread-1  ]: finished collecting timing info
[0m22:09:02.792176 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065f1430>]}
[0m22:09:02.792846 [info ] [Thread-1  ]: 19 of 47 OK created model model dbt_anomaly_detection.derived_models_2mon_4hr .. [[32mOK[0m in 13.73s]
[0m22:09:02.793649 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_4hr
[0m22:09:02.794009 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_8hr
[0m22:09:02.794428 [info ] [Thread-1  ]: 20 of 47 START model model dbt_anomaly_detection.derived_models_2mon_8hr ....... [RUN]
[0m22:09:02.795405 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_8hr"
[0m22:09:02.795638 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_8hr
[0m22:09:02.795870 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_8hr
[0m22:09:02.816645 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m22:09:02.817133 [debug] [Thread-1  ]: finished collecting timing info
[0m22:09:02.817252 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_8hr
[0m22:09:02.819601 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m22:09:02.820293 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:09:02.820528 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:09:15.762192 [debug] [Thread-1  ]: finished collecting timing info
[0m22:09:15.765226 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065f2520>]}
[0m22:09:15.765711 [info ] [Thread-1  ]: 20 of 47 OK created model model dbt_anomaly_detection.derived_models_2mon_8hr .. [[32mOK[0m in 12.97s]
[0m22:09:15.766104 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_8hr
[0m22:09:15.766283 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_12hr
[0m22:09:15.766581 [info ] [Thread-1  ]: 21 of 47 START model model dbt_anomaly_detection.derived_models_05mon_12hr ..... [RUN]
[0m22:09:15.767019 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_12hr"
[0m22:09:15.767156 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_12hr
[0m22:09:15.767293 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_12hr
[0m22:09:15.773377 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m22:09:15.773856 [debug] [Thread-1  ]: finished collecting timing info
[0m22:09:15.773988 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_12hr
[0m22:09:15.776364 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m22:09:15.776724 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:09:15.776856 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:09:27.176892 [debug] [Thread-1  ]: finished collecting timing info
[0m22:09:27.178078 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1063c25e0>]}
[0m22:09:27.178689 [info ] [Thread-1  ]: 21 of 47 OK created model model dbt_anomaly_detection.derived_models_05mon_12hr  [[32mOK[0m in 11.41s]
[0m22:09:27.179270 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_12hr
[0m22:09:27.179537 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_24hr
[0m22:09:27.180019 [info ] [Thread-1  ]: 22 of 47 START model model dbt_anomaly_detection.derived_models_05mon_24hr ..... [RUN]
[0m22:09:27.180662 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_24hr"
[0m22:09:27.180875 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_24hr
[0m22:09:27.181072 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_24hr
[0m22:09:27.189317 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m22:09:27.190934 [debug] [Thread-1  ]: finished collecting timing info
[0m22:09:27.191154 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_24hr
[0m22:09:27.195912 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m22:09:27.196605 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:09:27.196773 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:09:39.644788 [debug] [Thread-1  ]: finished collecting timing info
[0m22:09:39.646285 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106690be0>]}
[0m22:09:39.646854 [info ] [Thread-1  ]: 22 of 47 OK created model model dbt_anomaly_detection.derived_models_05mon_24hr  [[32mOK[0m in 12.47s]
[0m22:09:39.647322 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_24hr
[0m22:09:39.647525 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_12hr
[0m22:09:39.647917 [info ] [Thread-1  ]: 23 of 47 START model model dbt_anomaly_detection.derived_models_1mon_12hr ...... [RUN]
[0m22:09:39.648393 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_12hr"
[0m22:09:39.648562 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_12hr
[0m22:09:39.648716 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_12hr
[0m22:09:39.653795 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m22:09:39.654447 [debug] [Thread-1  ]: finished collecting timing info
[0m22:09:39.654588 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_12hr
[0m22:09:39.657463 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m22:09:39.657905 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:09:39.658049 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:09:52.685914 [debug] [Thread-1  ]: finished collecting timing info
[0m22:09:52.686869 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066d4b80>]}
[0m22:09:52.687196 [info ] [Thread-1  ]: 23 of 47 OK created model model dbt_anomaly_detection.derived_models_1mon_12hr . [[32mOK[0m in 13.04s]
[0m22:09:52.687425 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_12hr
[0m22:09:52.687534 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_24hr
[0m22:09:52.687638 [info ] [Thread-1  ]: 24 of 47 START model model dbt_anomaly_detection.derived_models_1mon_24hr ...... [RUN]
[0m22:09:52.688038 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_24hr"
[0m22:09:52.688135 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_24hr
[0m22:09:52.688213 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_24hr
[0m22:09:52.691660 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m22:09:52.692149 [debug] [Thread-1  ]: finished collecting timing info
[0m22:09:52.692595 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_24hr
[0m22:09:52.695294 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m22:09:52.696262 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:09:52.696403 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:10:05.922948 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:05.925149 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106523d00>]}
[0m22:10:05.925599 [info ] [Thread-1  ]: 24 of 47 OK created model model dbt_anomaly_detection.derived_models_1mon_24hr . [[32mOK[0m in 13.24s]
[0m22:10:05.926042 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_24hr
[0m22:10:05.926237 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_12hr
[0m22:10:05.926585 [info ] [Thread-1  ]: 25 of 47 START model model dbt_anomaly_detection.derived_models_2mon_12hr ...... [RUN]
[0m22:10:05.927044 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_12hr"
[0m22:10:05.927183 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_12hr
[0m22:10:05.927317 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_12hr
[0m22:10:05.933686 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m22:10:05.934387 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:05.934542 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_12hr
[0m22:10:05.939303 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m22:10:05.939813 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:10:05.940073 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:10:18.130837 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:18.133231 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066a4d30>]}
[0m22:10:18.695383 [info ] [Thread-1  ]: 25 of 47 OK created model model dbt_anomaly_detection.derived_models_2mon_12hr . [[32mOK[0m in 12.21s]
[0m22:10:18.696356 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_12hr
[0m22:10:18.696674 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_24hr
[0m22:10:18.697192 [info ] [Thread-1  ]: 26 of 47 START model model dbt_anomaly_detection.derived_models_2mon_24hr ...... [RUN]
[0m22:10:18.697986 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_24hr"
[0m22:10:18.698217 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_24hr
[0m22:10:18.698434 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_24hr
[0m22:10:18.704831 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m22:10:18.705723 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:18.705909 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_24hr
[0m22:10:18.709917 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m22:10:18.710790 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:10:18.711017 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:10:32.572811 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:32.575115 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066a49a0>]}
[0m22:10:32.575617 [info ] [Thread-1  ]: 26 of 47 OK created model model dbt_anomaly_detection.derived_models_2mon_24hr . [[32mOK[0m in 13.88s]
[0m22:10:32.576029 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_24hr
[0m22:10:32.576993 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ml_detect
[0m22:10:32.577308 [info ] [Thread-1  ]: 27 of 47 START table model dbt_anomaly_detection.derived_ml_detect ............. [RUN]
[0m22:10:32.577681 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_ml_detect"
[0m22:10:32.577809 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_ml_detect
[0m22:10:32.577944 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_ml_detect
[0m22:10:32.591152 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_ml_detect"
[0m22:10:32.591604 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:32.591713 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_ml_detect
[0m22:10:32.593521 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:10:33.017354 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_ml_detect"
[0m22:10:33.018708 [debug] [Thread-1  ]: On model.anomaly_detection.derived_ml_detect: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_ml_detect"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_ml_detect`
  
  
  OPTIONS()
  as (
    -- 2 periods of training * 2 probabality threshold * 4 aggregation levels 


  WITH test_set AS (
  SELECT
  time_stamps,
        event_count,
        app_event,
        LoB,
        agg_tag
      FROM
        `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`  
      WHERE (agg_tag = "4hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 10 DAY)) 
      OR (agg_tag = "8hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 10 DAY))
      OR (agg_tag = "12hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 15 DAY))
      OR (agg_tag = "24hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 15 DAY))
  )

  
  

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
       

  
  UNION ALL
  



    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
       

  
  ORDER BY
  agg_tag,
  time_stamps,
  app_event,
  LoB,
  prob_threshold,
  training_period
  


  );
  
[0m22:10:37.237960 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:37.238821 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072387c0>]}
[0m22:10:37.239306 [info ] [Thread-1  ]: 27 of 47 OK created table model dbt_anomaly_detection.derived_ml_detect ........ [[32mCREATE TABLE (1.6k rows, 488.7 KB processed)[0m in 4.66s]
[0m22:10:37.239785 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ml_detect
[0m22:10:37.240478 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ml_detect_tweaked
[0m22:10:37.240814 [info ] [Thread-1  ]: 28 of 47 START table model dbt_anomaly_detection.ml_detect_tweaked ............. [RUN]
[0m22:10:37.241208 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ml_detect_tweaked"
[0m22:10:37.241349 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ml_detect_tweaked
[0m22:10:37.241492 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ml_detect_tweaked
[0m22:10:37.246610 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ml_detect_tweaked"
[0m22:10:37.247749 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:37.247908 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ml_detect_tweaked
[0m22:10:37.250275 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:10:37.694225 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.ml_detect_tweaked"
[0m22:10:37.696838 [debug] [Thread-1  ]: On model.anomaly_detection.ml_detect_tweaked: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.ml_detect_tweaked"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked`
  
  
  OPTIONS()
  as (
     
  
  with neg_bound_reset as (
SELECT app_event, LoB, agg_tag, time_stamps, prob_threshold, training_period, event_count, 
  IF(lower_bound<0, 2, 0.77*lower_bound) AS lower_bound, 1.3*upper_bound AS upper_bound, anomaly_probability, is_anomaly
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ml_detect` )

SELECT app_event, LoB, agg_tag, time_stamps, prob_threshold, training_period, event_count, 
  lower_bound, upper_bound, anomaly_probability,
  (upper_bound < event_count OR event_count < lower_bound) AS is_anomaly
FROM neg_bound_reset
  );
  
[0m22:10:40.271024 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:40.272483 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065e79a0>]}
[0m22:10:40.273171 [info ] [Thread-1  ]: 28 of 47 OK created table model dbt_anomaly_detection.ml_detect_tweaked ........ [[32mCREATE TABLE (1.6k rows, 161.4 KB processed)[0m in 3.03s]
[0m22:10:40.273770 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ml_detect_tweaked
[0m22:10:40.274866 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_model_features_dbt
[0m22:10:40.275210 [info ] [Thread-1  ]: 29 of 47 START table model dbt_anomaly_detection.derived_model_features_dbt .... [RUN]
[0m22:10:40.275720 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_model_features_dbt"
[0m22:10:40.275886 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_model_features_dbt
[0m22:10:40.276050 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_model_features_dbt
[0m22:10:40.280960 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_model_features_dbt"
[0m22:10:40.281901 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:40.282065 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_model_features_dbt
[0m22:10:40.285202 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:10:40.668859 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_model_features_dbt"
[0m22:10:40.670086 [debug] [Thread-1  ]: On model.anomaly_detection.derived_model_features_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_model_features_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_model_features_dbt`
  
  
  OPTIONS()
  as (
    
  
  SELECT
    app_event, LoB, 
    CONCAT(agg_tag, '_', prob_threshold, "threshold", '_', RTRIM(LTRIM(training_period, "derived_models_"), CONCAT('_', agg_tag))) AS control_config,
    SUM(CASE WHEN is_anomaly = TRUE THEN 1 ELSE 0 END) AS anomalies,
    SQRT(AVG( POWER(upper_bound - lower_bound, 2) ) ) / AVG(lower_bound) AS RMSD_prcnt,
    SUM(CASE WHEN lower_bound < 0 THEN 1 ELSE 0 END) AS neg_lower
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked` AS all_configs
  GROUP BY
    app_event, 
    LoB,
    control_config
  ORDER BY
    control_config,
    app_event,
    LoB
  );
  
[0m22:10:43.355088 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:43.357238 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065a4160>]}
[0m22:10:43.358020 [info ] [Thread-1  ]: 29 of 47 OK created table model dbt_anomaly_detection.derived_model_features_dbt  [[32mCREATE TABLE (48.0 rows, 125.0 KB processed)[0m in 3.08s]
[0m22:10:43.358548 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_model_features_dbt
[0m22:10:43.359288 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_model_features_dbt
[0m22:10:43.359834 [info ] [Thread-1  ]: 30 of 47 START table model dbt_anomaly_detection.filtered_model_features_dbt ... [RUN]
[0m22:10:43.360436 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_model_features_dbt"
[0m22:10:43.360596 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_model_features_dbt
[0m22:10:43.360749 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_model_features_dbt
[0m22:10:43.366495 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_model_features_dbt"
[0m22:10:43.367075 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:43.367216 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_model_features_dbt
[0m22:10:43.371125 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:10:43.815680 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.filtered_model_features_dbt"
[0m22:10:43.817263 [debug] [Thread-1  ]: On model.anomaly_detection.filtered_model_features_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.filtered_model_features_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
  
  
  OPTIONS()
  as (
    
SELECT features.app_event, features.LoB, control_config, anomalies, RMSD_prcnt, neg_lower
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events` AS non_recent
INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`derived_model_features_dbt` AS features
  ON non_recent.app_event = features.app_event
  AND non_recent.LoB = features.LoB
  );
  
[0m22:10:46.617643 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:46.618572 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066e7cd0>]}
[0m22:10:46.619056 [info ] [Thread-1  ]: 30 of 47 OK created table model dbt_anomaly_detection.filtered_model_features_dbt  [[32mCREATE TABLE (48.0 rows, 3.5 KB processed)[0m in 3.26s]
[0m22:10:46.619541 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_model_features_dbt
[0m22:10:46.620362 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ref_distinct_tuples
[0m22:10:46.620715 [info ] [Thread-1  ]: 31 of 47 START table model dbt_anomaly_detection.ref_distinct_tuples ........... [RUN]
[0m22:10:46.621169 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ref_distinct_tuples"
[0m22:10:46.621328 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ref_distinct_tuples
[0m22:10:46.621489 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ref_distinct_tuples
[0m22:10:46.626400 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ref_distinct_tuples"
[0m22:10:46.627363 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:46.627531 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ref_distinct_tuples
[0m22:10:46.630339 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:10:47.188148 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.ref_distinct_tuples"
[0m22:10:47.189498 [debug] [Thread-1  ]: On model.anomaly_detection.ref_distinct_tuples: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.ref_distinct_tuples"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`ref_distinct_tuples`
  
  
  OPTIONS()
  as (
    
SELECT DISTINCT app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
WHERE app_event LIKE '%add%'
UNION ALL
SELECT DISTINCT app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
WHERE app_event NOT LIKE '%ad%'
ORDER BY app_event
  );
  
[0m22:10:49.787556 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:49.788193 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106523850>]}
[0m22:10:49.788490 [info ] [Thread-1  ]: 31 of 47 OK created table model dbt_anomaly_detection.ref_distinct_tuples ...... [[32mCREATE TABLE (2.0 rows, 720.0 Bytes processed)[0m in 3.17s]
[0m22:10:49.788797 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ref_distinct_tuples
[0m22:10:49.788954 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_phase
[0m22:10:49.789094 [info ] [Thread-1  ]: 32 of 47 START table model dbt_anomaly_detection.trade_off_phase ............... [RUN]
[0m22:10:49.789473 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.trade_off_phase"
[0m22:10:49.789610 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.trade_off_phase
[0m22:10:49.789725 [debug] [Thread-1  ]: Compiling model.anomaly_detection.trade_off_phase
[0m22:10:49.792255 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.trade_off_phase"
[0m22:10:49.792662 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:49.792766 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.trade_off_phase
[0m22:10:49.794608 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:10:50.216231 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.trade_off_phase"
[0m22:10:50.217069 [debug] [Thread-1  ]: On model.anomaly_detection.trade_off_phase: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.trade_off_phase"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_phase`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
where anomalies = 0
and RMSD_prcnt < 5
  );
  
[0m22:10:52.644704 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:52.645833 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072557f0>]}
[0m22:10:52.646453 [info ] [Thread-1  ]: 32 of 47 OK created table model dbt_anomaly_detection.trade_off_phase .......... [[32mCREATE TABLE (11.0 rows, 3.5 KB processed)[0m in 2.86s]
[0m22:10:52.647046 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_phase
[0m22:10:52.647915 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_min_anomalies
[0m22:10:52.648359 [info ] [Thread-1  ]: 33 of 47 START table model dbt_anomaly_detection.trade_off_min_anomalies ....... [RUN]
[0m22:10:52.648938 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.trade_off_min_anomalies"
[0m22:10:52.649145 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.trade_off_min_anomalies
[0m22:10:52.649340 [debug] [Thread-1  ]: Compiling model.anomaly_detection.trade_off_min_anomalies
[0m22:10:52.654105 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.trade_off_min_anomalies"
[0m22:10:52.655098 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:52.655277 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.trade_off_min_anomalies
[0m22:10:52.665012 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:10:53.093577 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.trade_off_min_anomalies"
[0m22:10:53.096308 [debug] [Thread-1  ]: On model.anomaly_detection.trade_off_min_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.trade_off_min_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_min_anomalies`
  
  
  OPTIONS()
  as (
    

 SELECT app_event, LoB, MIN(anomalies) AS anomalies 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_phase`
  GROUP BY app_event, LoB
  ORDER BY app_event, LoB
  );
  
[0m22:10:55.211973 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:55.213241 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107289a90>]}
[0m22:10:55.214069 [info ] [Thread-1  ]: 33 of 47 OK created table model dbt_anomaly_detection.trade_off_min_anomalies .. [[32mCREATE TABLE (1.0 rows, 330.0 Bytes processed)[0m in 2.56s]
[0m22:10:55.214995 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_min_anomalies
[0m22:10:55.216009 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_min_anomalies_results
[0m22:10:55.216376 [info ] [Thread-1  ]: 34 of 47 START table model dbt_anomaly_detection.trade_off_min_anomalies_results  [RUN]
[0m22:10:55.216898 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.trade_off_min_anomalies_results"
[0m22:10:55.217111 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.trade_off_min_anomalies_results
[0m22:10:55.217291 [debug] [Thread-1  ]: Compiling model.anomaly_detection.trade_off_min_anomalies_results
[0m22:10:55.222598 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.trade_off_min_anomalies_results"
[0m22:10:55.223281 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:55.223487 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.trade_off_min_anomalies_results
[0m22:10:55.225682 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:10:55.669306 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.trade_off_min_anomalies_results"
[0m22:10:55.670736 [debug] [Thread-1  ]: On model.anomaly_detection.trade_off_min_anomalies_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.trade_off_min_anomalies_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_min_anomalies_results`
  
  
  OPTIONS()
  as (
    

  SELECT neg_lower_criteria_res.app_event, neg_lower_criteria_res.LoB, neg_lower_criteria_res.control_config, 
    neg_lower_criteria_res.anomalies, neg_lower_criteria_res.RMSD_prcnt, neg_lower_criteria_res.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_phase` AS neg_lower_criteria_res
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_min_anomalies` AS min_neg_lower_min_anomalies
    ON neg_lower_criteria_res.anomalies = min_neg_lower_min_anomalies.anomalies
      AND neg_lower_criteria_res.app_event = min_neg_lower_min_anomalies.app_event
      AND neg_lower_criteria_res.LoB = min_neg_lower_min_anomalies.LoB
  ORDER BY neg_lower_criteria_res.app_event, neg_lower_criteria_res.LoB, RMSD_prcnt DESC
  );
  
[0m22:10:58.061689 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:58.063534 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106523700>]}
[0m22:10:58.063986 [info ] [Thread-1  ]: 34 of 47 OK created table model dbt_anomaly_detection.trade_off_min_anomalies_results  [[32mCREATE TABLE (11.0 rows, 847.0 Bytes processed)[0m in 2.85s]
[0m22:10:58.064421 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_min_anomalies_results
[0m22:10:58.065215 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_max_RMSD
[0m22:10:58.065825 [info ] [Thread-1  ]: 35 of 47 START table model dbt_anomaly_detection.trade_off_max_RMSD ............ [RUN]
[0m22:10:58.066313 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.trade_off_max_RMSD"
[0m22:10:58.066460 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.trade_off_max_RMSD
[0m22:10:58.066601 [debug] [Thread-1  ]: Compiling model.anomaly_detection.trade_off_max_RMSD
[0m22:10:58.070414 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.trade_off_max_RMSD"
[0m22:10:58.071245 [debug] [Thread-1  ]: finished collecting timing info
[0m22:10:58.071429 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.trade_off_max_RMSD
[0m22:10:58.073768 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:10:58.447215 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.trade_off_max_RMSD"
[0m22:10:58.450029 [debug] [Thread-1  ]: On model.anomaly_detection.trade_off_max_RMSD: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.trade_off_max_RMSD"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_max_RMSD`
  
  
  OPTIONS()
  as (
    

SELECT app_event, LoB, MAX(RMSD_prcnt) AS RMSD_prcnt 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_min_anomalies_results`
  GROUP BY app_event, LoB
  ORDER BY app_event, LoB
  );
  
[0m22:11:00.858946 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:00.860745 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107232cd0>]}
[0m22:11:00.861557 [info ] [Thread-1  ]: 35 of 47 OK created table model dbt_anomaly_detection.trade_off_max_RMSD ....... [[32mCREATE TABLE (1.0 rows, 330.0 Bytes processed)[0m in 2.79s]
[0m22:11:00.862214 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_max_RMSD
[0m22:11:00.862835 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_max_RMSD_results
[0m22:11:00.863210 [info ] [Thread-1  ]: 36 of 47 START table model dbt_anomaly_detection.trade_off_max_RMSD_results .... [RUN]
[0m22:11:00.863804 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.trade_off_max_RMSD_results"
[0m22:11:00.863955 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.trade_off_max_RMSD_results
[0m22:11:00.864102 [debug] [Thread-1  ]: Compiling model.anomaly_detection.trade_off_max_RMSD_results
[0m22:11:00.873724 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.trade_off_max_RMSD_results"
[0m22:11:00.874304 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:00.874464 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.trade_off_max_RMSD_results
[0m22:11:00.876567 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:11:01.370144 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.trade_off_max_RMSD_results"
[0m22:11:01.371532 [debug] [Thread-1  ]: On model.anomaly_detection.trade_off_max_RMSD_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.trade_off_max_RMSD_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_max_RMSD_results`
  
  
  OPTIONS()
  as (
    

SELECT anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.LoB, anomalies_criteria_res_dbt.control_config, 
    anomalies_criteria_res_dbt.anomalies, anomalies_criteria_res_dbt.RMSD_prcnt, anomalies_criteria_res_dbt.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_min_anomalies_results` AS anomalies_criteria_res_dbt
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_max_RMSD` AS min_anomalies_max_RMSD
    ON anomalies_criteria_res_dbt.RMSD_prcnt = min_anomalies_max_RMSD.RMSD_prcnt
      AND anomalies_criteria_res_dbt.app_event = min_anomalies_max_RMSD.app_event
      AND anomalies_criteria_res_dbt.LoB = min_anomalies_max_RMSD.LoB
  ORDER BY anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.LoB, control_config
  );
  
[0m22:11:03.592610 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:03.593508 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065e7610>]}
[0m22:11:03.594039 [info ] [Thread-1  ]: 36 of 47 OK created table model dbt_anomaly_detection.trade_off_max_RMSD_results  [[32mCREATE TABLE (1.0 rows, 847.0 Bytes processed)[0m in 2.73s]
[0m22:11:03.594599 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_max_RMSD_results
[0m22:11:03.595447 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events
[0m22:11:03.595916 [info ] [Thread-1  ]: 37 of 47 START table model dbt_anomaly_detection.remaining_events .............. [RUN]
[0m22:11:03.596451 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events"
[0m22:11:03.596620 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events
[0m22:11:03.596783 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events
[0m22:11:03.601736 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events"
[0m22:11:03.602731 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:03.602891 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events
[0m22:11:03.605919 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:11:04.048778 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events"
[0m22:11:04.051079 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events`
  
  
  OPTIONS()
  as (
    

select all_events.app_event as app_event, LoB, control_config, anomalies, RMSD_prcnt, neg_lower
from `ld-snowplow`.`dbt_anomaly_detection`.`ref_distinct_tuples` as all_events
left join `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_max_RMSD_results` as results 
on all_events.app_event = results.app_event 
where control_config is null
  );
  
[0m22:11:06.419095 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:06.420088 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072befd0>]}
[0m22:11:06.420751 [info ] [Thread-1  ]: 37 of 47 OK created table model dbt_anomaly_detection.remaining_events ......... [[32mCREATE TABLE (1.0 rows, 103.0 Bytes processed)[0m in 2.82s]
[0m22:11:06.421314 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events
[0m22:11:06.421994 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_features
[0m22:11:06.422403 [info ] [Thread-1  ]: 38 of 47 START table model dbt_anomaly_detection.remaining_events_features ..... [RUN]
[0m22:11:06.423421 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_features"
[0m22:11:06.423653 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_features
[0m22:11:06.423853 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_features
[0m22:11:06.428241 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_features"
[0m22:11:06.429029 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:06.429158 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_features
[0m22:11:06.431235 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:11:06.823979 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_features"
[0m22:11:06.824889 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_features: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_features"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features`
  
  
  OPTIONS()
  as (
    

select remaning_events.app_event as app_event, all_features.LoB as LoB, all_features.control_config as control_config, all_features.anomalies as anomalies, all_features.RMSD_prcnt as RMSD_prcnt, all_features.neg_lower as neg_lower
from `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events` as remaning_events
inner join `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt` as all_features
on remaning_events.app_event = all_features.app_event
  );
  
[0m22:11:09.102093 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:09.102975 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072dc4f0>]}
[0m22:11:09.103439 [info ] [Thread-1  ]: 38 of 47 OK created table model dbt_anomaly_detection.remaining_events_features  [[32mCREATE TABLE (24.0 rows, 3.5 KB processed)[0m in 2.68s]
[0m22:11:09.103850 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_features
[0m22:11:09.104474 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_features_null_filtered
[0m22:11:09.105037 [info ] [Thread-1  ]: 39 of 47 START table model dbt_anomaly_detection.remaining_events_features_null_filtered  [RUN]
[0m22:11:09.105725 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_features_null_filtered"
[0m22:11:09.105894 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_features_null_filtered
[0m22:11:09.106037 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_features_null_filtered
[0m22:11:09.112038 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_features_null_filtered"
[0m22:11:09.112711 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:09.112856 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_features_null_filtered
[0m22:11:09.115220 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:11:09.497504 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_features_null_filtered"
[0m22:11:09.498107 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_features_null_filtered: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_features_null_filtered"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features`
where RMSD_prcnt is not null
  );
  
[0m22:11:11.855561 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:11.857390 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065a2e50>]}
[0m22:11:11.858082 [info ] [Thread-1  ]: 39 of 47 OK created table model dbt_anomaly_detection.remaining_events_features_null_filtered  [[32mCREATE TABLE (24.0 rows, 1.7 KB processed)[0m in 2.75s]
[0m22:11:11.858681 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_features_null_filtered
[0m22:11:11.859636 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies
[0m22:11:11.860334 [info ] [Thread-1  ]: 40 of 47 START table model dbt_anomaly_detection.remaining_events_min_anomalies  [RUN]
[0m22:11:11.860958 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies"
[0m22:11:11.861150 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies
[0m22:11:11.861322 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies
[0m22:11:11.866234 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies"
[0m22:11:11.867167 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:11.867336 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies
[0m22:11:11.870188 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:11:12.379943 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_anomalies"
[0m22:11:12.381374 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies`
  
  
  OPTIONS()
  as (
    

SELECT app_event, LoB, MIN(anomalies) AS anomalies 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered`
  GROUP BY app_event, LoB
  ORDER BY app_event, LoB
  );
  
[0m22:11:14.537175 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:14.537608 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10664cb50>]}
[0m22:11:14.537884 [info ] [Thread-1  ]: 40 of 47 OK created table model dbt_anomaly_detection.remaining_events_min_anomalies  [[32mCREATE TABLE (1.0 rows, 720.0 Bytes processed)[0m in 2.68s]
[0m22:11:14.538184 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies
[0m22:11:14.538558 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m22:11:14.538935 [info ] [Thread-1  ]: 41 of 47 START table model dbt_anomaly_detection.remaining_events_min_anomalies_results  [RUN]
[0m22:11:14.539312 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m22:11:14.539424 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies_results
[0m22:11:14.539529 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies_results
[0m22:11:14.542538 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m22:11:14.543029 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:14.543151 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies_results
[0m22:11:14.546952 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:11:14.950822 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m22:11:14.952824 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_anomalies_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_anomalies_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results`
  
  
  OPTIONS()
  as (
    

SELECT neg_lower_criteria_res.app_event, neg_lower_criteria_res.LoB, neg_lower_criteria_res.control_config, 
    neg_lower_criteria_res.anomalies, neg_lower_criteria_res.RMSD_prcnt, neg_lower_criteria_res.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered` AS neg_lower_criteria_res
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies` AS min_neg_lower_min_anomalies
    ON neg_lower_criteria_res.anomalies = min_neg_lower_min_anomalies.anomalies
      AND neg_lower_criteria_res.app_event = min_neg_lower_min_anomalies.app_event
      AND neg_lower_criteria_res.LoB = min_neg_lower_min_anomalies.LoB
  ORDER BY neg_lower_criteria_res.app_event, neg_lower_criteria_res.LoB, RMSD_prcnt DESC
  );
  
[0m22:11:17.455384 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:17.456225 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072ef6a0>]}
[0m22:11:17.456739 [info ] [Thread-1  ]: 41 of 47 OK created table model dbt_anomaly_detection.remaining_events_min_anomalies_results  [[32mCREATE TABLE (7.0 rows, 1.8 KB processed)[0m in 2.92s]
[0m22:11:17.457261 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m22:11:17.457814 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD
[0m22:11:17.458256 [info ] [Thread-1  ]: 42 of 47 START table model dbt_anomaly_detection.remaining_events_min_RMSD ..... [RUN]
[0m22:11:17.458780 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD"
[0m22:11:17.458954 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD
[0m22:11:17.459123 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD
[0m22:11:17.463583 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD"
[0m22:11:17.464436 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:17.464591 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD
[0m22:11:17.467516 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:11:17.880801 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_RMSD"
[0m22:11:17.882173 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_RMSD: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_RMSD"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD`
  
  
  OPTIONS()
  as (
    

SELECT app_event, LoB, MIN(RMSD_prcnt) AS RMSD_prcnt 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results`
  GROUP BY app_event, LoB
  ORDER BY app_event, LoB
  );
  
[0m22:11:20.450354 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:20.451264 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106622550>]}
[0m22:11:20.451726 [info ] [Thread-1  ]: 42 of 47 OK created table model dbt_anomaly_detection.remaining_events_min_RMSD  [[32mCREATE TABLE (1.0 rows, 210.0 Bytes processed)[0m in 2.99s]
[0m22:11:20.452191 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD
[0m22:11:20.453159 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m22:11:20.453459 [info ] [Thread-1  ]: 43 of 47 START table model dbt_anomaly_detection.remaining_events_min_RMSD_results  [RUN]
[0m22:11:20.453911 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m22:11:20.454064 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD_results
[0m22:11:20.454206 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD_results
[0m22:11:20.460516 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m22:11:20.461360 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:20.461524 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD_results
[0m22:11:20.464518 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:11:20.851793 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m22:11:20.854451 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_RMSD_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_RMSD_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
  
  
  OPTIONS()
  as (
    

SELECT anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.LoB, anomalies_criteria_res_dbt.control_config, 
    anomalies_criteria_res_dbt.anomalies, anomalies_criteria_res_dbt.RMSD_prcnt, anomalies_criteria_res_dbt.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results` AS anomalies_criteria_res_dbt
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD` AS min_anomalies_max_RMSD
    ON anomalies_criteria_res_dbt.RMSD_prcnt = min_anomalies_max_RMSD.RMSD_prcnt
      AND anomalies_criteria_res_dbt.app_event = min_anomalies_max_RMSD.app_event
      AND anomalies_criteria_res_dbt.LoB = min_anomalies_max_RMSD.LoB
  ORDER BY anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.LoB, control_config
  );
  
[0m22:11:22.897260 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:22.898244 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107222e80>]}
[0m22:11:22.898793 [info ] [Thread-1  ]: 43 of 47 OK created table model dbt_anomaly_detection.remaining_events_min_RMSD_results  [[32mCREATE TABLE (1.0 rows, 546.0 Bytes processed)[0m in 2.44s]
[0m22:11:22.899355 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m22:11:22.900041 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_models
[0m22:11:22.900684 [info ] [Thread-1  ]: 44 of 47 START table model dbt_anomaly_detection.all_models .................... [RUN]
[0m22:11:22.901150 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_models"
[0m22:11:22.901303 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_models
[0m22:11:22.901465 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_models
[0m22:11:22.909522 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_models"
[0m22:11:22.910351 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:22.910524 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_models
[0m22:11:22.915164 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:11:23.297957 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_models"
[0m22:11:23.300380 [debug] [Thread-1  ]: On model.anomaly_detection.all_models: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_models"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_models`
  
  
  OPTIONS()
  as (
    

     select *
     from `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
     union all
     select *
     from `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_max_RMSD_results`
     order by app_event, LoB
  );
  
[0m22:11:25.428171 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:25.429162 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106620ac0>]}
[0m22:11:25.429714 [info ] [Thread-1  ]: 44 of 47 OK created table model dbt_anomaly_detection.all_models ............... [[32mCREATE TABLE (2.0 rows, 147.0 Bytes processed)[0m in 2.53s]
[0m22:11:25.430280 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_models
[0m22:11:25.431226 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_all_models
[0m22:11:25.431644 [info ] [Thread-1  ]: 45 of 47 START table model dbt_anomaly_detection.filtered_all_models ........... [RUN]
[0m22:11:25.432181 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_all_models"
[0m22:11:25.432357 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_all_models
[0m22:11:25.432529 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_all_models
[0m22:11:25.437673 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_all_models"
[0m22:11:25.438445 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:25.438603 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_all_models
[0m22:11:25.441435 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:11:25.886262 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.filtered_all_models"
[0m22:11:25.887650 [debug] [Thread-1  ]: On model.anomaly_detection.filtered_all_models: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.filtered_all_models"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`filtered_all_models`
  
  
  OPTIONS()
  as (
    
SELECT app_event, LoB, control_config, anomalies, RMSD_prcnt, neg_lower, 
  CASE WHEN (neg_lower = 0 AND anomalies < 5 AND RMSD_prcnt > 0.40 AND RMSD_prcnt < 5.00 ) THEN "ideal_model" ELSE "non_ideal_model" END AS model_quality_tag,
FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_models`
WHERE app_event LIKE '%add%'
UNION ALL 
SELECT app_event, LoB, control_config, anomalies, RMSD_prcnt, neg_lower, 
  CASE WHEN (neg_lower = 0 AND anomalies < 5 AND RMSD_prcnt > 0.40 AND RMSD_prcnt < 5.00 ) THEN "ideal_model" ELSE "non_ideal_model" END AS model_quality_tag,
FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_models`
WHERE app_event NOT LIKE '%ad%'
ORDER BY app_event, LoB
  );
  
[0m22:11:28.175413 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:28.177534 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107232cd0>]}
[0m22:11:28.178607 [info ] [Thread-1  ]: 45 of 47 OK created table model dbt_anomaly_detection.filtered_all_models ...... [[32mCREATE TABLE (2.0 rows, 147.0 Bytes processed)[0m in 2.75s]
[0m22:11:28.179417 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_all_models
[0m22:11:28.180087 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m22:11:28.180336 [info ] [Thread-1  ]: 46 of 47 START table model dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [RUN]
[0m22:11:28.180841 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m22:11:28.181251 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m22:11:28.181710 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m22:11:28.187927 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m22:11:28.188698 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:28.188860 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m22:11:28.191377 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:11:28.628180 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m22:11:28.628703 [debug] [Thread-1  ]: On model.anomaly_detection.derived_alerts_fired_automation_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_alerts_fired_automation_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_alerts_fired_automation_dbt`
  
  
  OPTIONS()
  as (
    
  
  WITH ml_detect_updated AS (
    SELECT app_event, LoB, 
      CONCAT(agg_tag, '_', prob_threshold, "threshold", '_', RTRIM(LTRIM(training_period, "derived_models_"), CONCAT('_', agg_tag))) AS control_config,
      time_stamps, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked`
  )
  SELECT time_stamps, all_configs.app_event AS app_event, all_configs.LoB AS LoB, control_table.control_config AS control_config, 
    anomalies, RMSD_prcnt, neg_lower, model_quality_tag,
    event_count, lower_bound, upper_bound, anomaly_probability, is_anomaly
  FROM ml_detect_updated AS all_configs 
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`filtered_all_models` AS control_table 
    ON all_configs.app_event = control_table.app_event
      AND all_configs.LoB = control_table.LoB
      AND all_configs.control_config = control_table.control_config
  ORDER BY all_configs.app_event, all_configs.LoB, time_stamps
  );
  
[0m22:11:31.218968 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:31.219833 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107448fa0>]}
[0m22:11:31.220288 [info ] [Thread-1  ]: 46 of 47 OK created table model dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [[32mCREATE TABLE (60.0 rows, 163.1 KB processed)[0m in 3.04s]
[0m22:11:31.220754 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m22:11:31.221508 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_events_with_anomalies
[0m22:11:31.221881 [info ] [Thread-1  ]: 47 of 47 START table model dbt_anomaly_detection.derived_events_with_anomalies . [RUN]
[0m22:11:31.222396 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_events_with_anomalies"
[0m22:11:31.222557 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_events_with_anomalies
[0m22:11:31.222703 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_events_with_anomalies
[0m22:11:31.231955 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_events_with_anomalies"
[0m22:11:31.232523 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:31.232670 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_events_with_anomalies
[0m22:11:31.235122 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:11:31.641751 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_events_with_anomalies"
[0m22:11:31.643039 [debug] [Thread-1  ]: On model.anomaly_detection.derived_events_with_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_events_with_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_events_with_anomalies`
  
  
  OPTIONS()
  as (
    
SELECT time_stamps, app_event, LoB, event_count AS event_count_anomalous_yesterday
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_alerts_fired_automation_dbt`
WHERE DATE(time_stamps) = CURRENT_DATE() - 1 
  AND is_anomaly
  );
  
[0m22:11:33.951748 [debug] [Thread-1  ]: finished collecting timing info
[0m22:11:33.953367 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b27177a3-1f51-4bdc-a540-c63609dca2eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065e7d30>]}
[0m22:11:33.954109 [info ] [Thread-1  ]: 47 of 47 OK created table model dbt_anomaly_detection.derived_events_with_anomalies  [[32mCREATE TABLE (0.0 rows, 2.3 KB processed)[0m in 2.73s]
[0m22:11:33.954842 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_events_with_anomalies
[0m22:11:33.957049 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m22:11:33.957890 [info ] [MainThread]: 
[0m22:11:33.958250 [info ] [MainThread]: Finished running 35 table models, 12 model models in 0 hours 4 minutes and 23.54 seconds (263.54s).
[0m22:11:33.958510 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:11:33.958643 [debug] [MainThread]: Connection 'model.anomaly_detection.derived_events_with_anomalies' was properly closed.
[0m22:11:33.974939 [info ] [MainThread]: 
[0m22:11:33.975172 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:11:33.975395 [info ] [MainThread]: 
[0m22:11:33.975597 [info ] [MainThread]: Done. PASS=47 WARN=0 ERROR=0 SKIP=0 TOTAL=47
[0m22:11:33.975939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062a2a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10566fb80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065ecb80>]}
[0m22:11:33.976131 [debug] [MainThread]: Flushing usage events


============================== 2023-02-10 18:20:46.148683 | 4b87a9c8-5c0f-47f0-bfc6-42a180fd027a ==============================
[0m18:20:46.148711 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:20:46.149344 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m18:20:46.149438 [debug] [MainThread]: Tracking: tracking
[0m18:20:46.157523 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105265e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105256a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105256d30>]}
[0m18:20:46.213029 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m18:20:46.213397 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/remaining_events_features_null_filtered.sql
[0m18:20:46.213523 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/filtered_all_models.sql
[0m18:20:46.213632 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/remaining_events_min_anomalies.sql
[0m18:20:46.221368 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_features_null_filtered.sql
[0m18:20:46.227007 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_all_models.sql
[0m18:20:46.228527 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies.sql
[0m18:20:46.244680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055d30d0>]}
[0m18:20:46.280125 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055161f0>]}
[0m18:20:46.280326 [info ] [MainThread]: Found 48 models, 0 tests, 0 snapshots, 0 analyses, 540 macros, 0 operations, 0 seed files, 2 sources, 0 exposures, 0 metrics
[0m18:20:46.280448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105516220>]}
[0m18:20:46.281830 [info ] [MainThread]: 
[0m18:20:46.282080 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m18:20:46.283599 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow"
[0m18:20:46.283779 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:20:48.414666 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow_dbt_anomaly_detection"
[0m18:20:48.415428 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:20:48.813389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104006d30>]}
[0m18:20:48.814583 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:20:48.814979 [info ] [MainThread]: 
[0m18:20:48.822086 [debug] [Thread-1  ]: Began running node model.anomaly_detection.reference_derived
[0m18:20:48.822692 [info ] [Thread-1  ]: 1 of 47 START table model dbt_anomaly_detection.reference_derived .............. [RUN]
[0m18:20:48.823265 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.reference_derived"
[0m18:20:48.823433 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.reference_derived
[0m18:20:48.823645 [debug] [Thread-1  ]: Compiling model.anomaly_detection.reference_derived
[0m18:20:48.831990 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.reference_derived"
[0m18:20:48.832819 [debug] [Thread-1  ]: finished collecting timing info
[0m18:20:48.832925 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.reference_derived
[0m18:20:48.845288 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:20:49.342154 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.reference_derived"
[0m18:20:49.343710 [debug] [Thread-1  ]: On model.anomaly_detection.reference_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.reference_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived`
  
  
  OPTIONS()
  as (
    
SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`web_purchase_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY) AND DATE(collector_tstamp) < CURRENT_DATE()

UNION ALL

SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`pco_web_apply_filter_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY) AND DATE(collector_tstamp) < CURRENT_DATE()
  );
  
[0m18:20:54.176130 [debug] [Thread-1  ]: finished collecting timing info
[0m18:20:54.176943 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10522a4c0>]}
[0m18:20:54.177452 [info ] [Thread-1  ]: 1 of 47 OK created table model dbt_anomaly_detection.reference_derived ......... [[32mCREATE TABLE (1.1m rows, 34.8 MB processed)[0m in 5.35s]
[0m18:20:54.178106 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.reference_derived
[0m18:20:54.178876 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ref_including_LoBs
[0m18:20:54.179270 [info ] [Thread-1  ]: 2 of 47 START table model dbt_anomaly_detection.derived_ref_including_LoBs ..... [RUN]
[0m18:20:54.179715 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_ref_including_LoBs"
[0m18:20:54.179856 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_ref_including_LoBs
[0m18:20:54.179985 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_ref_including_LoBs
[0m18:20:54.183380 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_ref_including_LoBs"
[0m18:20:54.183952 [debug] [Thread-1  ]: finished collecting timing info
[0m18:20:54.184078 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_ref_including_LoBs
[0m18:20:54.186230 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:20:54.614790 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_ref_including_LoBs"
[0m18:20:54.616043 [debug] [Thread-1  ]: On model.anomaly_detection.derived_ref_including_LoBs: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_ref_including_LoBs"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs`
  
  
  OPTIONS()
  as (
    

SELECT collector_tstamp, event, user_event_name, app_id,
    CASE
    WHEN LOWER(app_id) LIKE '%pcexpress%' OR LOWER(app_id) LIKE '%pcx%' THEN "PCX"
    WHEN LOWER(app_id) LIKE '%sdm%' OR LOWER(app_id) LIKE '%beauty%' THEN "SDM Shop"
    WHEN LOWER(app_id) LIKE '%drx%' THEN "SDM Drx" 
    -- WHEN LOWER(page_urlhost) like '%pcoptimum.ca%' and (LOWER(app_id) like '%ios%' or LOWER(app_id) like '%android%') then 'PC Optimum' 
    -- not recommended to use page_urlhost for mobile apps
    WHEN LOWER(app_id) like '%pco%' THEN 'PC Optimum'
    -- WHEN LOWER(page_urlhost) like '%presidentschoice.ca%' THEN 'PC' -- placeholder for after the launch 
    -- WHEN LOWER(page_urlhost) like '%joefresh.com%' and LOWER(app_id) like '%ios%' then 'JF' -- placeholder for after the launch 
    WHEN LOWER(app_id) like '%jf%' THEN 'JF'
    ELSE app_id END AS LoB,
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived`
  );
  
[0m18:20:59.554128 [debug] [Thread-1  ]: finished collecting timing info
[0m18:20:59.555793 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056facd0>]}
[0m18:20:59.556530 [info ] [Thread-1  ]: 2 of 47 OK created table model dbt_anomaly_detection.derived_ref_including_LoBs  [[32mCREATE TABLE (1.1m rows, 34.4 MB processed)[0m in 5.38s]
[0m18:20:59.557231 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ref_including_LoBs
[0m18:20:59.558201 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived
[0m18:20:59.558758 [info ] [Thread-1  ]: 3 of 47 START table model dbt_anomaly_detection.all_agg_derived ................ [RUN]
[0m18:20:59.559407 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived"
[0m18:20:59.559613 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived
[0m18:20:59.559811 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived
[0m18:20:59.568067 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived"
[0m18:20:59.570930 [debug] [Thread-1  ]: finished collecting timing info
[0m18:20:59.571107 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived
[0m18:20:59.575950 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:21:00.036013 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived"
[0m18:21:00.037360 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
  
  
  OPTIONS()
  as (
    


SELECT "4hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_4hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/4)*4 AS _4hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _4hr_trunc,
    app_id,
    LoB,
    event_type)

UNION ALL


SELECT "8hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_8hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/8)*8 AS _8hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _8hr_trunc,
    app_id,
    LoB,
    event_type)

UNION ALL


SELECT "12hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_12hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/12)*12 AS _12hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _12hr_trunc,
    app_id,
    LoB,
    event_type)

UNION ALL


SELECT "24hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_24hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/24)*24 AS _24hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _24hr_trunc,
    app_id,
    LoB,
    event_type)

ORDER BY
  agg_tag,
  time_stamps,
  app_id,
  LoB,
  event_type


  );
  
[0m18:21:04.497320 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:04.498878 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105707190>]}
[0m18:21:04.499449 [info ] [Thread-1  ]: 3 of 47 OK created table model dbt_anomaly_detection.all_agg_derived ........... [[32mCREATE TABLE (2.1k rows, 41.5 MB processed)[0m in 4.94s]
[0m18:21:04.499923 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived
[0m18:21:04.500644 [debug] [Thread-1  ]: Began running node model.anomaly_detection.count_cutoff
[0m18:21:04.501085 [info ] [Thread-1  ]: 4 of 47 START table model dbt_anomaly_detection.count_cutoff ................... [RUN]
[0m18:21:04.501510 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.count_cutoff"
[0m18:21:04.501653 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.count_cutoff
[0m18:21:04.501798 [debug] [Thread-1  ]: Compiling model.anomaly_detection.count_cutoff
[0m18:21:04.505960 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.count_cutoff"
[0m18:21:04.506699 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:04.506825 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.count_cutoff
[0m18:21:04.509058 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:21:04.935961 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.count_cutoff"
[0m18:21:04.938376 [debug] [Thread-1  ]: On model.anomaly_detection.count_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.count_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, LoB, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select agg_tag, app_event, LoB, min(time_stamps) as strt_time
from pairs
where event_count > 50
group by app_event, LoB, agg_tag 
order by app_event, LoB, agg_tag
  );
  
[0m18:21:07.299102 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:07.300869 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105617a30>]}
[0m18:21:07.301545 [info ] [Thread-1  ]: 4 of 47 OK created table model dbt_anomaly_detection.count_cutoff .............. [[32mCREATE TABLE (8.0 rows, 92.3 KB processed)[0m in 2.80s]
[0m18:21:07.302064 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.count_cutoff
[0m18:21:07.302299 [debug] [Thread-1  ]: Began running node model.anomaly_detection.dev_ml_model_constraints
[0m18:21:07.302723 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.dev_ml_model_constraints"
[0m18:21:07.303350 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.dev_ml_model_constraints
[0m18:21:07.303583 [debug] [Thread-1  ]: Compiling model.anomaly_detection.dev_ml_model_constraints
[0m18:21:07.308756 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.dev_ml_model_constraints"
[0m18:21:07.309704 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:07.310236 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.dev_ml_model_constraints
[0m18:21:07.310400 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff
[0m18:21:07.310720 [info ] [Thread-1  ]: 5 of 47 START table model dbt_anomaly_detection.all_agg_derived_cutoff ......... [RUN]
[0m18:21:07.311207 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff"
[0m18:21:07.311358 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff
[0m18:21:07.311502 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff
[0m18:21:07.315470 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m18:21:07.316142 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:07.316280 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff
[0m18:21:07.320948 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:21:07.777871 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m18:21:07.780544 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, LoB, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select time_stamps, app_event, LoB, agg_tag, event_count
from(
select time_stamps, strt_time, pairs.app_event, pairs.LoB, pairs.agg_tag, event_count
from pairs
inner join `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff` as cutoff
on pairs.agg_tag = cutoff.agg_tag
and pairs.app_event = cutoff.app_event
and pairs.LoB = cutoff.LoB
order by pairs.app_event, pairs.LoB, pairs.agg_tag, time_stamps)
where time_stamps >= strt_time
  );
  
[0m18:21:10.389180 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:10.391246 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10570e5b0>]}
[0m18:21:10.392414 [info ] [Thread-1  ]: 5 of 47 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff .... [[32mCREATE TABLE (2.1k rows, 92.6 KB processed)[0m in 3.08s]
[0m18:21:10.393575 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff
[0m18:21:10.395286 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m18:21:10.395715 [info ] [Thread-1  ]: 6 of 47 START table model dbt_anomaly_detection.all_agg_derived_cutoff_long .... [RUN]
[0m18:21:10.399515 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m18:21:10.399946 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_long
[0m18:21:10.400172 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_long
[0m18:21:10.404761 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m18:21:10.405597 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:10.405780 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_long
[0m18:21:10.408934 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:21:10.809942 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m18:21:10.811226 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB(CURRENT_DATE(), INTERVAL 10 DAY)
  );
  
[0m18:21:13.386129 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:13.388393 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10570dd90>]}
[0m18:21:13.389616 [info ] [Thread-1  ]: 6 of 47 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_long  [[32mCREATE TABLE (1.9k rows, 90.2 KB processed)[0m in 2.99s]
[0m18:21:13.390670 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m18:21:13.391121 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m18:21:13.391962 [info ] [Thread-1  ]: 7 of 47 START table model dbt_anomaly_detection.all_agg_derived_cutoff_short ... [RUN]
[0m18:21:13.392753 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m18:21:13.392968 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_short
[0m18:21:13.393182 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_short
[0m18:21:13.398228 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m18:21:13.399170 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:13.399354 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_short
[0m18:21:13.402938 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:21:13.802784 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m18:21:13.804563 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB(CURRENT_DATE(), INTERVAL 15 DAY)
  );
  
[0m18:21:16.082757 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:16.083503 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056f30d0>]}
[0m18:21:16.083842 [info ] [Thread-1  ]: 7 of 47 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_short  [[32mCREATE TABLE (1.8k rows, 90.2 KB processed)[0m in 2.69s]
[0m18:21:16.084173 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m18:21:16.084318 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_nonrecent_events
[0m18:21:16.084634 [info ] [Thread-1  ]: 8 of 47 START table model dbt_anomaly_detection.derived_nonrecent_events ....... [RUN]
[0m18:21:16.085185 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_nonrecent_events"
[0m18:21:16.085310 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_nonrecent_events
[0m18:21:16.085428 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_nonrecent_events
[0m18:21:16.088836 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m18:21:16.089463 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:16.089584 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_nonrecent_events
[0m18:21:16.094849 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:21:16.507614 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m18:21:16.510224 [debug] [Thread-1  ]: On model.anomaly_detection.derived_nonrecent_events: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_nonrecent_events"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events`
  
  
  OPTIONS()
  as (
    

SELECT MIN(time_stamps) AS strt_time, app_event, LoB 
FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
GROUP BY app_event, LoB
HAVING DATE(MIN(time_stamps)) < DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
ORDER BY strt_time
  );
  
[0m18:21:18.837229 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:18.839433 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056de820>]}
[0m18:21:18.840623 [info ] [Thread-1  ]: 8 of 47 OK created table model dbt_anomaly_detection.derived_nonrecent_events .. [[32mCREATE TABLE (2.0 rows, 62.6 KB processed)[0m in 2.75s]
[0m18:21:18.841933 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_nonrecent_events
[0m18:21:18.842312 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_long
[0m18:21:18.842640 [info ] [Thread-1  ]: 9 of 47 START table model dbt_anomaly_detection.aggregation_quartiles_long ..... [RUN]
[0m18:21:18.843974 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_long"
[0m18:21:18.844636 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_long
[0m18:21:18.844931 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_long
[0m18:21:18.849975 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m18:21:18.850858 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:18.851042 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_long
[0m18:21:18.854227 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:21:19.285025 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m18:21:19.287720 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m18:21:21.840729 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:21.842943 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056cd070>]}
[0m18:21:21.843621 [info ] [Thread-1  ]: 9 of 47 OK created table model dbt_anomaly_detection.aggregation_quartiles_long  [[32mCREATE TABLE (8.0 rows, 53.0 KB processed)[0m in 3.00s]
[0m18:21:21.844137 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_long
[0m18:21:21.844371 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_short
[0m18:21:21.844998 [info ] [Thread-1  ]: 10 of 47 START table model dbt_anomaly_detection.aggregation_quartiles_short ... [RUN]
[0m18:21:21.845562 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_short"
[0m18:21:21.845734 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_short
[0m18:21:21.845899 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_short
[0m18:21:21.850300 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m18:21:21.851561 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:21.851724 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_short
[0m18:21:21.854259 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:21:22.285511 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m18:21:22.288166 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m18:21:24.631067 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:24.631791 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056eaf10>]}
[0m18:21:24.632186 [info ] [Thread-1  ]: 10 of 47 OK created table model dbt_anomaly_detection.aggregation_quartiles_short  [[32mCREATE TABLE (8.0 rows, 49.7 KB processed)[0m in 2.79s]
[0m18:21:24.632592 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_short
[0m18:21:24.632749 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_long
[0m18:21:24.633226 [info ] [Thread-1  ]: 11 of 47 START table model dbt_anomaly_detection.aggregation_bounds_long ....... [RUN]
[0m18:21:24.633960 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_long"
[0m18:21:24.634116 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_long
[0m18:21:24.634255 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_long
[0m18:21:24.640446 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m18:21:24.640999 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:24.641132 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_long
[0m18:21:24.644937 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:21:25.055951 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m18:21:25.057174 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m18:21:27.241587 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:27.242657 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10570d640>]}
[0m18:21:27.243225 [info ] [Thread-1  ]: 11 of 47 OK created table model dbt_anomaly_detection.aggregation_bounds_long .. [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.61s]
[0m18:21:27.243809 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_long
[0m18:21:27.244070 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_short
[0m18:21:27.244356 [info ] [Thread-1  ]: 12 of 47 START table model dbt_anomaly_detection.aggregation_bounds_short ...... [RUN]
[0m18:21:27.245168 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_short"
[0m18:21:27.245680 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_short
[0m18:21:27.245949 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_short
[0m18:21:27.250265 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m18:21:27.250942 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:27.251120 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_short
[0m18:21:27.253803 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:21:27.686977 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m18:21:27.688007 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m18:21:29.920427 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:29.921242 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10570d9d0>]}
[0m18:21:29.921760 [info ] [Thread-1  ]: 12 of 47 OK created table model dbt_anomaly_detection.aggregation_bounds_short . [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.68s]
[0m18:21:29.922295 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_short
[0m18:21:29.922544 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_long
[0m18:21:29.923267 [info ] [Thread-1  ]: 13 of 47 START table model dbt_anomaly_detection.aggregation_outliers_long ..... [RUN]
[0m18:21:29.923973 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_long"
[0m18:21:29.924188 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_long
[0m18:21:29.924384 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_long
[0m18:21:29.929275 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m18:21:29.930878 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:29.931045 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_long
[0m18:21:29.933743 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:21:30.366681 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m18:21:30.367811 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, LoB, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag, LoB,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m18:21:32.787763 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:32.788134 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10575ea60>]}
[0m18:21:32.788385 [info ] [Thread-1  ]: 13 of 47 OK created table model dbt_anomaly_detection.aggregation_outliers_long  [[32mCREATE TABLE (1.9k rows, 81.4 KB processed)[0m in 2.86s]
[0m18:21:32.788653 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_long
[0m18:21:32.788780 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_short
[0m18:21:32.789133 [info ] [Thread-1  ]: 14 of 47 START table model dbt_anomaly_detection.aggregation_outliers_short .... [RUN]
[0m18:21:32.791188 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_short"
[0m18:21:32.791312 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_short
[0m18:21:32.791406 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_short
[0m18:21:32.794988 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m18:21:32.795422 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:32.795521 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_short
[0m18:21:32.797200 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:21:33.025538 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m18:21:33.028788 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, LoB, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag, LoB,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m18:21:35.230158 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:35.232387 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105782fd0>]}
[0m18:21:35.233512 [info ] [Thread-1  ]: 14 of 47 OK created table model dbt_anomaly_detection.aggregation_outliers_short  [[32mCREATE TABLE (1.8k rows, 76.3 KB processed)[0m in 2.44s]
[0m18:21:35.234675 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_short
[0m18:21:35.235269 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_4hr
[0m18:21:35.236540 [info ] [Thread-1  ]: 15 of 47 START model model dbt_anomaly_detection.derived_models_05mon_4hr ...... [RUN]
[0m18:21:35.237382 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_4hr"
[0m18:21:35.237605 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_4hr
[0m18:21:35.237807 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_4hr
[0m18:21:35.244463 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m18:21:35.246593 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:35.246940 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_4hr
[0m18:21:35.264215 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m18:21:35.264927 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:21:35.265086 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:21:46.861571 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:46.863800 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056ebac0>]}
[0m18:21:46.864958 [info ] [Thread-1  ]: 15 of 47 OK created model model dbt_anomaly_detection.derived_models_05mon_4hr . [[32mOK[0m in 11.63s]
[0m18:21:46.866326 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_4hr
[0m18:21:46.866895 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_8hr
[0m18:21:46.867439 [info ] [Thread-1  ]: 16 of 47 START model model dbt_anomaly_detection.derived_models_05mon_8hr ...... [RUN]
[0m18:21:46.868785 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_8hr"
[0m18:21:46.869035 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_8hr
[0m18:21:46.869248 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_8hr
[0m18:21:46.877598 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m18:21:46.878508 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:46.878848 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_8hr
[0m18:21:46.886380 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m18:21:46.886955 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:21:46.887172 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:21:57.343253 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:57.344299 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10562ef10>]}
[0m18:21:57.344795 [info ] [Thread-1  ]: 16 of 47 OK created model model dbt_anomaly_detection.derived_models_05mon_8hr . [[32mOK[0m in 10.48s]
[0m18:21:57.345298 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_8hr
[0m18:21:57.345511 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_4hr
[0m18:21:57.345921 [info ] [Thread-1  ]: 17 of 47 START model model dbt_anomaly_detection.derived_models_1mon_4hr ....... [RUN]
[0m18:21:57.346499 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_4hr"
[0m18:21:57.346670 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_4hr
[0m18:21:57.346823 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_4hr
[0m18:21:57.353992 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m18:21:57.354735 [debug] [Thread-1  ]: finished collecting timing info
[0m18:21:57.354876 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_4hr
[0m18:21:57.357877 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m18:21:57.358532 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:21:57.358725 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:22:09.595786 [debug] [Thread-1  ]: finished collecting timing info
[0m18:22:09.596527 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056ea100>]}
[0m18:22:09.596891 [info ] [Thread-1  ]: 17 of 47 OK created model model dbt_anomaly_detection.derived_models_1mon_4hr .. [[32mOK[0m in 12.25s]
[0m18:22:09.597279 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_4hr
[0m18:22:09.597459 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_8hr
[0m18:22:09.597750 [info ] [Thread-1  ]: 18 of 47 START model model dbt_anomaly_detection.derived_models_1mon_8hr ....... [RUN]
[0m18:22:09.598625 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_8hr"
[0m18:22:09.598960 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_8hr
[0m18:22:09.599168 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_8hr
[0m18:22:09.606928 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m18:22:09.607599 [debug] [Thread-1  ]: finished collecting timing info
[0m18:22:09.607876 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_8hr
[0m18:22:09.611173 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m18:22:09.611703 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:22:09.611911 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:22:20.991906 [debug] [Thread-1  ]: finished collecting timing info
[0m18:22:20.992337 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105782460>]}
[0m18:22:20.992659 [info ] [Thread-1  ]: 18 of 47 OK created model model dbt_anomaly_detection.derived_models_1mon_8hr .. [[32mOK[0m in 11.39s]
[0m18:22:20.993059 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_8hr
[0m18:22:20.993169 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_4hr
[0m18:22:20.993378 [info ] [Thread-1  ]: 19 of 47 START model model dbt_anomaly_detection.derived_models_2mon_4hr ....... [RUN]
[0m18:22:20.993652 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_4hr"
[0m18:22:20.993730 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_4hr
[0m18:22:20.993801 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_4hr
[0m18:22:20.997422 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m18:22:20.998862 [debug] [Thread-1  ]: finished collecting timing info
[0m18:22:20.999056 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_4hr
[0m18:22:21.001499 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m18:22:21.001833 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:22:21.001960 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:22:33.476485 [debug] [Thread-1  ]: finished collecting timing info
[0m18:22:33.477233 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055f2370>]}
[0m18:22:33.477609 [info ] [Thread-1  ]: 19 of 47 OK created model model dbt_anomaly_detection.derived_models_2mon_4hr .. [[32mOK[0m in 12.48s]
[0m18:22:33.477971 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_4hr
[0m18:22:33.478153 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_8hr
[0m18:22:33.478474 [info ] [Thread-1  ]: 20 of 47 START model model dbt_anomaly_detection.derived_models_2mon_8hr ....... [RUN]
[0m18:22:33.478821 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_8hr"
[0m18:22:33.478944 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_8hr
[0m18:22:33.479073 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_8hr
[0m18:22:33.483371 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m18:22:33.484211 [debug] [Thread-1  ]: finished collecting timing info
[0m18:22:33.484378 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_8hr
[0m18:22:33.487188 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m18:22:33.487704 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:22:33.487869 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:22:44.582371 [debug] [Thread-1  ]: finished collecting timing info
[0m18:22:44.583513 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055f2550>]}
[0m18:22:44.584067 [info ] [Thread-1  ]: 20 of 47 OK created model model dbt_anomaly_detection.derived_models_2mon_8hr .. [[32mOK[0m in 11.10s]
[0m18:22:44.584627 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_8hr
[0m18:22:44.584865 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_12hr
[0m18:22:44.585259 [info ] [Thread-1  ]: 21 of 47 START model model dbt_anomaly_detection.derived_models_05mon_12hr ..... [RUN]
[0m18:22:44.585826 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_12hr"
[0m18:22:44.586014 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_12hr
[0m18:22:44.586195 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_12hr
[0m18:22:44.593961 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m18:22:44.594688 [debug] [Thread-1  ]: finished collecting timing info
[0m18:22:44.594840 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_12hr
[0m18:22:44.598213 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m18:22:44.598686 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:22:44.598861 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:22:55.079047 [debug] [Thread-1  ]: finished collecting timing info
[0m18:22:55.080123 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10578b490>]}
[0m18:22:55.080664 [info ] [Thread-1  ]: 21 of 47 OK created model model dbt_anomaly_detection.derived_models_05mon_12hr  [[32mOK[0m in 10.49s]
[0m18:22:55.081231 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_12hr
[0m18:22:55.081484 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_24hr
[0m18:22:55.081757 [info ] [Thread-1  ]: 22 of 47 START model model dbt_anomaly_detection.derived_models_05mon_24hr ..... [RUN]
[0m18:22:55.082855 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_24hr"
[0m18:22:55.083182 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_24hr
[0m18:22:55.083393 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_24hr
[0m18:22:55.093678 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m18:22:55.095734 [debug] [Thread-1  ]: finished collecting timing info
[0m18:22:55.095921 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_24hr
[0m18:22:55.100458 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m18:22:55.101121 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:22:55.101319 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:23:05.457814 [debug] [Thread-1  ]: finished collecting timing info
[0m18:23:05.459024 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056e7eb0>]}
[0m18:23:05.459624 [info ] [Thread-1  ]: 22 of 47 OK created model model dbt_anomaly_detection.derived_models_05mon_24hr  [[32mOK[0m in 10.38s]
[0m18:23:05.460171 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_24hr
[0m18:23:05.460396 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_12hr
[0m18:23:05.460628 [info ] [Thread-1  ]: 23 of 47 START model model dbt_anomaly_detection.derived_models_1mon_12hr ...... [RUN]
[0m18:23:05.461490 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_12hr"
[0m18:23:05.461782 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_12hr
[0m18:23:05.461968 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_12hr
[0m18:23:05.469393 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m18:23:05.470087 [debug] [Thread-1  ]: finished collecting timing info
[0m18:23:05.470239 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_12hr
[0m18:23:05.474043 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m18:23:05.474778 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:23:05.475002 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:23:18.866922 [debug] [Thread-1  ]: finished collecting timing info
[0m18:23:18.867965 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056e78e0>]}
[0m18:23:18.868546 [info ] [Thread-1  ]: 23 of 47 OK created model model dbt_anomaly_detection.derived_models_1mon_12hr . [[32mOK[0m in 13.41s]
[0m18:23:18.869111 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_12hr
[0m18:23:18.869377 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_24hr
[0m18:23:18.870071 [info ] [Thread-1  ]: 24 of 47 START model model dbt_anomaly_detection.derived_models_1mon_24hr ...... [RUN]
[0m18:23:18.870922 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_24hr"
[0m18:23:18.871119 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_24hr
[0m18:23:18.871289 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_24hr
[0m18:23:18.879119 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m18:23:18.880126 [debug] [Thread-1  ]: finished collecting timing info
[0m18:23:18.880469 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_24hr
[0m18:23:18.884209 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m18:23:18.885161 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:23:18.885405 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:23:29.392570 [debug] [Thread-1  ]: finished collecting timing info
[0m18:23:29.394553 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056eb460>]}
[0m18:23:29.395324 [info ] [Thread-1  ]: 24 of 47 OK created model model dbt_anomaly_detection.derived_models_1mon_24hr . [[32mOK[0m in 10.52s]
[0m18:23:29.396017 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_24hr
[0m18:23:29.396481 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_12hr
[0m18:23:29.397105 [info ] [Thread-1  ]: 25 of 47 START model model dbt_anomaly_detection.derived_models_2mon_12hr ...... [RUN]
[0m18:23:29.397844 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_12hr"
[0m18:23:29.398066 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_12hr
[0m18:23:29.398270 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_12hr
[0m18:23:29.408539 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m18:23:29.411024 [debug] [Thread-1  ]: finished collecting timing info
[0m18:23:29.411210 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_12hr
[0m18:23:29.414618 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m18:23:29.415096 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:23:29.415296 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:23:40.435681 [debug] [Thread-1  ]: finished collecting timing info
[0m18:23:40.436629 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10575de20>]}
[0m18:23:40.945098 [info ] [Thread-1  ]: 25 of 47 OK created model model dbt_anomaly_detection.derived_models_2mon_12hr . [[32mOK[0m in 11.04s]
[0m18:23:40.946039 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_12hr
[0m18:23:40.946317 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_24hr
[0m18:23:40.946818 [info ] [Thread-1  ]: 26 of 47 START model model dbt_anomaly_detection.derived_models_2mon_24hr ...... [RUN]
[0m18:23:40.947593 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_24hr"
[0m18:23:40.947816 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_24hr
[0m18:23:40.948014 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_24hr
[0m18:23:40.955993 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m18:23:40.958241 [debug] [Thread-1  ]: finished collecting timing info
[0m18:23:40.958417 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_24hr
[0m18:23:40.961837 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m18:23:40.962347 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:23:40.962555 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:23:51.670969 [debug] [Thread-1  ]: finished collecting timing info
[0m18:23:51.672688 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10576be50>]}
[0m18:23:51.673414 [info ] [Thread-1  ]: 26 of 47 OK created model model dbt_anomaly_detection.derived_models_2mon_24hr . [[32mOK[0m in 10.73s]
[0m18:23:51.674476 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_24hr
[0m18:23:51.675654 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ml_detect
[0m18:23:51.676074 [info ] [Thread-1  ]: 27 of 47 START table model dbt_anomaly_detection.derived_ml_detect ............. [RUN]
[0m18:23:51.676665 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_ml_detect"
[0m18:23:51.676846 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_ml_detect
[0m18:23:51.677021 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_ml_detect
[0m18:23:51.694297 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_ml_detect"
[0m18:23:51.695759 [debug] [Thread-1  ]: finished collecting timing info
[0m18:23:51.695919 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_ml_detect
[0m18:23:51.698120 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:23:52.136882 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_ml_detect"
[0m18:23:52.139881 [debug] [Thread-1  ]: On model.anomaly_detection.derived_ml_detect: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_ml_detect"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_ml_detect`
  
  
  OPTIONS()
  as (
    -- 2 periods of training * 2 probabality threshold * 4 aggregation levels 


  WITH test_set AS (
  SELECT
  time_stamps,
        event_count,
        app_event,
        LoB,
        agg_tag
      FROM
        `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`  
      WHERE (agg_tag = "4hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 10 DAY)) 
      OR (agg_tag = "8hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 10 DAY))
      OR (agg_tag = "12hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 15 DAY))
      OR (agg_tag = "24hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 15 DAY))
  )

  
  

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
       

  
  UNION ALL
  



    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
       

  
  ORDER BY
  agg_tag,
  time_stamps,
  app_event,
  LoB,
  prob_threshold,
  training_period
  


  );
  
[0m18:23:55.219793 [debug] [Thread-1  ]: finished collecting timing info
[0m18:23:55.220720 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10582afa0>]}
[0m18:23:55.221261 [info ] [Thread-1  ]: 27 of 47 OK created table model dbt_anomaly_detection.derived_ml_detect ........ [[32mCREATE TABLE (1.5k rows, 487.7 KB processed)[0m in 3.54s]
[0m18:23:55.221835 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ml_detect
[0m18:23:55.222747 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ml_detect_tweaked
[0m18:23:55.223285 [info ] [Thread-1  ]: 28 of 47 START table model dbt_anomaly_detection.ml_detect_tweaked ............. [RUN]
[0m18:23:55.224322 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ml_detect_tweaked"
[0m18:23:55.224529 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ml_detect_tweaked
[0m18:23:55.224704 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ml_detect_tweaked
[0m18:23:55.229288 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ml_detect_tweaked"
[0m18:23:55.230089 [debug] [Thread-1  ]: finished collecting timing info
[0m18:23:55.230251 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ml_detect_tweaked
[0m18:23:55.233120 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:23:55.673247 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.ml_detect_tweaked"
[0m18:23:55.674356 [debug] [Thread-1  ]: On model.anomaly_detection.ml_detect_tweaked: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.ml_detect_tweaked"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked`
  
  
  OPTIONS()
  as (
     
  
  with neg_bound_reset as (
SELECT app_event, LoB, agg_tag, time_stamps, prob_threshold, training_period, event_count, 
  IF(lower_bound<0, 2, 0.77*lower_bound) AS lower_bound, 1.3*upper_bound AS upper_bound, anomaly_probability, is_anomaly
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ml_detect` )

SELECT app_event, LoB, agg_tag, time_stamps, prob_threshold, training_period, event_count, 
  lower_bound, upper_bound, anomaly_probability,
  (upper_bound < event_count OR event_count < lower_bound) AS is_anomaly
FROM neg_bound_reset
  );
  
[0m18:23:57.805032 [debug] [Thread-1  ]: finished collecting timing info
[0m18:23:57.805614 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10575d070>]}
[0m18:23:57.805954 [info ] [Thread-1  ]: 28 of 47 OK created table model dbt_anomaly_detection.ml_detect_tweaked ........ [[32mCREATE TABLE (1.5k rows, 147.0 KB processed)[0m in 2.58s]
[0m18:23:57.806318 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ml_detect_tweaked
[0m18:23:57.806963 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_model_features_dbt
[0m18:23:57.807323 [info ] [Thread-1  ]: 29 of 47 START table model dbt_anomaly_detection.derived_model_features_dbt .... [RUN]
[0m18:23:57.807765 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_model_features_dbt"
[0m18:23:57.807898 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_model_features_dbt
[0m18:23:57.808018 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_model_features_dbt
[0m18:23:57.811305 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_model_features_dbt"
[0m18:23:57.811830 [debug] [Thread-1  ]: finished collecting timing info
[0m18:23:57.811953 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_model_features_dbt
[0m18:23:57.814130 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:23:58.099170 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_model_features_dbt"
[0m18:23:58.100091 [debug] [Thread-1  ]: On model.anomaly_detection.derived_model_features_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_model_features_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_model_features_dbt`
  
  
  OPTIONS()
  as (
    
  
  SELECT
    app_event, LoB, 
    CONCAT(agg_tag, '_', prob_threshold, "threshold", '_', RTRIM(LTRIM(training_period, "derived_models_"), CONCAT('_', agg_tag))) AS control_config,
    SUM(CASE WHEN is_anomaly = TRUE THEN 1 ELSE 0 END) AS anomalies,
    SQRT(AVG( POWER(upper_bound - lower_bound, 2) ) ) / AVG(lower_bound) AS RMSD_prcnt,
    SUM(CASE WHEN lower_bound < 0 THEN 1 ELSE 0 END) AS neg_lower
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked` AS all_configs
  GROUP BY
    app_event, 
    LoB,
    control_config
  ORDER BY
    control_config,
    app_event,
    LoB
  );
  
[0m18:24:00.407739 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:00.408563 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105833f70>]}
[0m18:24:00.409048 [info ] [Thread-1  ]: 29 of 47 OK created table model dbt_anomaly_detection.derived_model_features_dbt  [[32mCREATE TABLE (48.0 rows, 113.9 KB processed)[0m in 2.60s]
[0m18:24:00.409523 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_model_features_dbt
[0m18:24:00.410131 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_model_features_dbt
[0m18:24:00.410660 [info ] [Thread-1  ]: 30 of 47 START table model dbt_anomaly_detection.filtered_model_features_dbt ... [RUN]
[0m18:24:00.411221 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_model_features_dbt"
[0m18:24:00.411406 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_model_features_dbt
[0m18:24:00.411579 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_model_features_dbt
[0m18:24:00.416168 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_model_features_dbt"
[0m18:24:00.418236 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:00.418424 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_model_features_dbt
[0m18:24:00.425622 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:24:00.891183 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.filtered_model_features_dbt"
[0m18:24:00.892530 [debug] [Thread-1  ]: On model.anomaly_detection.filtered_model_features_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.filtered_model_features_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
  
  
  OPTIONS()
  as (
    
SELECT features.app_event, features.LoB, control_config, anomalies, RMSD_prcnt, neg_lower
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events` AS non_recent
INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`derived_model_features_dbt` AS features
  ON non_recent.app_event = features.app_event
  AND non_recent.LoB = features.LoB
  );
  
[0m18:24:03.109538 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:03.110362 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10583cfd0>]}
[0m18:24:03.110816 [info ] [Thread-1  ]: 30 of 47 OK created table model dbt_anomaly_detection.filtered_model_features_dbt  [[32mCREATE TABLE (48.0 rows, 3.5 KB processed)[0m in 2.70s]
[0m18:24:03.111290 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_model_features_dbt
[0m18:24:03.112225 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ref_distinct_tuples
[0m18:24:03.112733 [info ] [Thread-1  ]: 31 of 47 START table model dbt_anomaly_detection.ref_distinct_tuples ........... [RUN]
[0m18:24:03.113246 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ref_distinct_tuples"
[0m18:24:03.113410 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ref_distinct_tuples
[0m18:24:03.113576 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ref_distinct_tuples
[0m18:24:03.118273 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ref_distinct_tuples"
[0m18:24:03.119158 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:03.119320 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ref_distinct_tuples
[0m18:24:03.122019 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:24:03.379916 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.ref_distinct_tuples"
[0m18:24:03.380404 [debug] [Thread-1  ]: On model.anomaly_detection.ref_distinct_tuples: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.ref_distinct_tuples"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`ref_distinct_tuples`
  
  
  OPTIONS()
  as (
    
SELECT DISTINCT app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
WHERE app_event LIKE '%add%'
UNION ALL
SELECT DISTINCT app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
WHERE app_event NOT LIKE '%ad%'
ORDER BY app_event
  );
  
[0m18:24:05.517045 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:05.517917 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056b3790>]}
[0m18:24:05.518466 [info ] [Thread-1  ]: 31 of 47 OK created table model dbt_anomaly_detection.ref_distinct_tuples ...... [[32mCREATE TABLE (2.0 rows, 720.0 Bytes processed)[0m in 2.40s]
[0m18:24:05.519007 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ref_distinct_tuples
[0m18:24:05.519258 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_features_null_filtered
[0m18:24:05.519899 [info ] [Thread-1  ]: 32 of 47 START table model dbt_anomaly_detection.remaining_events_features_null_filtered  [RUN]
[0m18:24:05.520842 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_features_null_filtered"
[0m18:24:05.521034 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_features_null_filtered
[0m18:24:05.521209 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_features_null_filtered
[0m18:24:05.525786 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_features_null_filtered"
[0m18:24:05.527942 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:05.528217 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_features_null_filtered
[0m18:24:05.531450 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:24:05.941255 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_features_null_filtered"
[0m18:24:05.942720 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_features_null_filtered: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_features_null_filtered"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
where RMSD_prcnt is not null
  );
  
[0m18:24:08.288635 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:08.290394 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057660d0>]}
[0m18:24:08.291129 [info ] [Thread-1  ]: 32 of 47 OK created table model dbt_anomaly_detection.remaining_events_features_null_filtered  [[32mCREATE TABLE (48.0 rows, 3.5 KB processed)[0m in 2.77s]
[0m18:24:08.291731 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_features_null_filtered
[0m18:24:08.292006 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies
[0m18:24:08.292453 [info ] [Thread-1  ]: 33 of 47 START table model dbt_anomaly_detection.remaining_events_min_anomalies  [RUN]
[0m18:24:08.293603 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies"
[0m18:24:08.293856 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies
[0m18:24:08.294065 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies
[0m18:24:08.301698 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies"
[0m18:24:08.302495 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:08.302648 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies
[0m18:24:08.307602 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:24:08.738123 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_anomalies"
[0m18:24:08.739555 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies`
  
  
  OPTIONS()
  as (
    

SELECT app_event, LoB, MIN(anomalies) AS anomalies 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
  GROUP BY app_event, LoB
  ORDER BY app_event, LoB
  );
  
[0m18:24:10.881957 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:10.882838 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10586afa0>]}
[0m18:24:10.883368 [info ] [Thread-1  ]: 33 of 47 OK created table model dbt_anomaly_detection.remaining_events_min_anomalies  [[32mCREATE TABLE (2.0 rows, 1.4 KB processed)[0m in 2.59s]
[0m18:24:10.883914 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies
[0m18:24:10.884163 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_phase
[0m18:24:10.884445 [info ] [Thread-1  ]: 34 of 47 START table model dbt_anomaly_detection.trade_off_phase ............... [RUN]
[0m18:24:10.885410 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.trade_off_phase"
[0m18:24:10.885657 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.trade_off_phase
[0m18:24:10.885866 [debug] [Thread-1  ]: Compiling model.anomaly_detection.trade_off_phase
[0m18:24:10.890461 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.trade_off_phase"
[0m18:24:10.891356 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:10.891543 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.trade_off_phase
[0m18:24:10.894548 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:24:11.326793 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.trade_off_phase"
[0m18:24:11.329124 [debug] [Thread-1  ]: On model.anomaly_detection.trade_off_phase: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.trade_off_phase"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_phase`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
where anomalies = 0
and RMSD_prcnt < 5
  );
  
[0m18:24:13.603504 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:13.604675 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10564cb20>]}
[0m18:24:13.605272 [info ] [Thread-1  ]: 34 of 47 OK created table model dbt_anomaly_detection.trade_off_phase .......... [[32mCREATE TABLE (12.0 rows, 3.5 KB processed)[0m in 2.72s]
[0m18:24:13.605845 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_phase
[0m18:24:13.606097 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m18:24:13.606557 [info ] [Thread-1  ]: 35 of 47 START table model dbt_anomaly_detection.remaining_events_min_anomalies_results  [RUN]
[0m18:24:13.607116 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m18:24:13.607506 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies_results
[0m18:24:13.607791 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies_results
[0m18:24:13.612969 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m18:24:13.613805 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:13.613965 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies_results
[0m18:24:13.616915 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:24:14.066624 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m18:24:14.068654 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_anomalies_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_anomalies_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results`
  
  
  OPTIONS()
  as (
    

SELECT neg_lower_criteria_res.app_event, neg_lower_criteria_res.LoB, neg_lower_criteria_res.control_config, 
    neg_lower_criteria_res.anomalies, neg_lower_criteria_res.RMSD_prcnt, neg_lower_criteria_res.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered` AS neg_lower_criteria_res
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies` AS min_neg_lower_min_anomalies
    ON neg_lower_criteria_res.anomalies = min_neg_lower_min_anomalies.anomalies
      AND neg_lower_criteria_res.app_event = min_neg_lower_min_anomalies.app_event
      AND neg_lower_criteria_res.LoB = min_neg_lower_min_anomalies.LoB
  ORDER BY neg_lower_criteria_res.app_event, neg_lower_criteria_res.LoB, RMSD_prcnt DESC
  );
  
[0m18:24:16.333222 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:16.333965 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058abf40>]}
[0m18:24:16.334411 [info ] [Thread-1  ]: 35 of 47 OK created table model dbt_anomaly_detection.remaining_events_min_anomalies_results  [[32mCREATE TABLE (30.0 rows, 3.5 KB processed)[0m in 2.73s]
[0m18:24:16.334873 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m18:24:16.335076 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_min_anomalies
[0m18:24:16.335301 [info ] [Thread-1  ]: 36 of 47 START table model dbt_anomaly_detection.trade_off_min_anomalies ....... [RUN]
[0m18:24:16.336158 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.trade_off_min_anomalies"
[0m18:24:16.336373 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.trade_off_min_anomalies
[0m18:24:16.336555 [debug] [Thread-1  ]: Compiling model.anomaly_detection.trade_off_min_anomalies
[0m18:24:16.342859 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.trade_off_min_anomalies"
[0m18:24:16.343683 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:16.343844 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.trade_off_min_anomalies
[0m18:24:16.346464 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:24:16.749331 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.trade_off_min_anomalies"
[0m18:24:16.751287 [debug] [Thread-1  ]: On model.anomaly_detection.trade_off_min_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.trade_off_min_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_min_anomalies`
  
  
  OPTIONS()
  as (
    

 SELECT app_event, LoB, MIN(anomalies) AS anomalies 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_phase`
  GROUP BY app_event, LoB
  ORDER BY app_event, LoB
  );
  
[0m18:24:19.057815 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:19.058647 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058abbe0>]}
[0m18:24:19.059097 [info ] [Thread-1  ]: 36 of 47 OK created table model dbt_anomaly_detection.trade_off_min_anomalies .. [[32mCREATE TABLE (1.0 rows, 360.0 Bytes processed)[0m in 2.72s]
[0m18:24:19.059579 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_min_anomalies
[0m18:24:19.059787 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD
[0m18:24:19.060167 [info ] [Thread-1  ]: 37 of 47 START table model dbt_anomaly_detection.remaining_events_min_RMSD ..... [RUN]
[0m18:24:19.060901 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD"
[0m18:24:19.061069 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD
[0m18:24:19.061229 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD
[0m18:24:19.065389 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD"
[0m18:24:19.066223 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:19.066391 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD
[0m18:24:19.069031 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:24:19.448429 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_RMSD"
[0m18:24:19.449923 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_RMSD: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_RMSD"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD`
  
  
  OPTIONS()
  as (
    

SELECT app_event, LoB, MIN(RMSD_prcnt) AS RMSD_prcnt 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results`
  GROUP BY app_event, LoB
  ORDER BY app_event, LoB
  );
  
[0m18:24:21.597428 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:21.598314 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105771790>]}
[0m18:24:21.598823 [info ] [Thread-1  ]: 37 of 47 OK created table model dbt_anomaly_detection.remaining_events_min_RMSD  [[32mCREATE TABLE (2.0 rows, 900.0 Bytes processed)[0m in 2.54s]
[0m18:24:21.599341 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD
[0m18:24:21.599581 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_min_anomalies_results
[0m18:24:21.600000 [info ] [Thread-1  ]: 38 of 47 START table model dbt_anomaly_detection.trade_off_min_anomalies_results  [RUN]
[0m18:24:21.600460 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.trade_off_min_anomalies_results"
[0m18:24:21.600796 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.trade_off_min_anomalies_results
[0m18:24:21.600979 [debug] [Thread-1  ]: Compiling model.anomaly_detection.trade_off_min_anomalies_results
[0m18:24:21.607960 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.trade_off_min_anomalies_results"
[0m18:24:21.610177 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:21.610349 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.trade_off_min_anomalies_results
[0m18:24:21.615516 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:24:22.141855 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.trade_off_min_anomalies_results"
[0m18:24:22.143023 [debug] [Thread-1  ]: On model.anomaly_detection.trade_off_min_anomalies_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.trade_off_min_anomalies_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_min_anomalies_results`
  
  
  OPTIONS()
  as (
    

  SELECT neg_lower_criteria_res.app_event, neg_lower_criteria_res.LoB, neg_lower_criteria_res.control_config, 
    neg_lower_criteria_res.anomalies, neg_lower_criteria_res.RMSD_prcnt, neg_lower_criteria_res.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_phase` AS neg_lower_criteria_res
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_min_anomalies` AS min_neg_lower_min_anomalies
    ON neg_lower_criteria_res.anomalies = min_neg_lower_min_anomalies.anomalies
      AND neg_lower_criteria_res.app_event = min_neg_lower_min_anomalies.app_event
      AND neg_lower_criteria_res.LoB = min_neg_lower_min_anomalies.LoB
  ORDER BY neg_lower_criteria_res.app_event, neg_lower_criteria_res.LoB, RMSD_prcnt DESC
  );
  
[0m18:24:24.444018 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:24.445669 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058814f0>]}
[0m18:24:24.446475 [info ] [Thread-1  ]: 38 of 47 OK created table model dbt_anomaly_detection.trade_off_min_anomalies_results  [[32mCREATE TABLE (12.0 rows, 922.0 Bytes processed)[0m in 2.85s]
[0m18:24:24.447248 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_min_anomalies_results
[0m18:24:24.447547 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m18:24:24.448269 [info ] [Thread-1  ]: 39 of 47 START table model dbt_anomaly_detection.remaining_events_min_RMSD_results  [RUN]
[0m18:24:24.448950 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m18:24:24.449143 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD_results
[0m18:24:24.449321 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD_results
[0m18:24:24.454621 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m18:24:24.455423 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:24.455612 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD_results
[0m18:24:24.458469 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:24:24.843133 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m18:24:24.846565 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_RMSD_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_RMSD_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
  
  
  OPTIONS()
  as (
    

SELECT anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.LoB, anomalies_criteria_res_dbt.control_config, 
    anomalies_criteria_res_dbt.anomalies, anomalies_criteria_res_dbt.RMSD_prcnt, anomalies_criteria_res_dbt.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results` AS anomalies_criteria_res_dbt
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD` AS min_anomalies_max_RMSD
    ON anomalies_criteria_res_dbt.RMSD_prcnt = min_anomalies_max_RMSD.RMSD_prcnt
      AND anomalies_criteria_res_dbt.app_event = min_anomalies_max_RMSD.app_event
      AND anomalies_criteria_res_dbt.LoB = min_anomalies_max_RMSD.LoB
  ORDER BY anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.LoB, control_config
  );
  
[0m18:24:27.077521 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:27.078482 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10587c6a0>]}
[0m18:24:27.079062 [info ] [Thread-1  ]: 39 of 47 OK created table model dbt_anomaly_detection.remaining_events_min_RMSD_results  [[32mCREATE TABLE (2.0 rows, 2.2 KB processed)[0m in 2.63s]
[0m18:24:27.079626 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m18:24:27.079879 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_max_RMSD
[0m18:24:27.080164 [info ] [Thread-1  ]: 40 of 47 START table model dbt_anomaly_detection.trade_off_max_RMSD ............ [RUN]
[0m18:24:27.081250 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.trade_off_max_RMSD"
[0m18:24:27.081512 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.trade_off_max_RMSD
[0m18:24:27.081715 [debug] [Thread-1  ]: Compiling model.anomaly_detection.trade_off_max_RMSD
[0m18:24:27.086795 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.trade_off_max_RMSD"
[0m18:24:27.087817 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:27.087994 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.trade_off_max_RMSD
[0m18:24:27.091020 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:24:27.502721 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.trade_off_max_RMSD"
[0m18:24:27.505190 [debug] [Thread-1  ]: On model.anomaly_detection.trade_off_max_RMSD: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.trade_off_max_RMSD"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_max_RMSD`
  
  
  OPTIONS()
  as (
    

SELECT app_event, LoB, MAX(RMSD_prcnt) AS RMSD_prcnt 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_min_anomalies_results`
  GROUP BY app_event, LoB
  ORDER BY app_event, LoB
  );
  
[0m18:24:29.727852 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:29.729987 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105759fa0>]}
[0m18:24:29.730829 [info ] [Thread-1  ]: 40 of 47 OK created table model dbt_anomaly_detection.trade_off_max_RMSD ....... [[32mCREATE TABLE (1.0 rows, 360.0 Bytes processed)[0m in 2.65s]
[0m18:24:29.731453 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_max_RMSD
[0m18:24:29.731686 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_all_models
[0m18:24:29.732288 [info ] [Thread-1  ]: 41 of 47 START table model dbt_anomaly_detection.filtered_all_models ........... [RUN]
[0m18:24:29.732953 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_all_models"
[0m18:24:29.733158 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_all_models
[0m18:24:29.733338 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_all_models
[0m18:24:29.741264 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_all_models"
[0m18:24:29.742093 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:29.742251 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_all_models
[0m18:24:29.747379 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:24:30.179941 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.filtered_all_models"
[0m18:24:30.182248 [debug] [Thread-1  ]: On model.anomaly_detection.filtered_all_models: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.filtered_all_models"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`filtered_all_models`
  
  
  OPTIONS()
  as (
    
SELECT app_event, LoB, control_config, anomalies, RMSD_prcnt, neg_lower, 
  CASE WHEN (neg_lower = 0 AND anomalies < 5 AND RMSD_prcnt > 0.40 AND RMSD_prcnt < 5.00 ) THEN "ideal_model" ELSE "non_ideal_model" END AS model_quality_tag,
FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
WHERE app_event LIKE '%add%'
UNION ALL 
SELECT app_event, LoB, control_config, anomalies, RMSD_prcnt, neg_lower, 
  CASE WHEN (neg_lower = 0 AND anomalies < 5 AND RMSD_prcnt > 0.40 AND RMSD_prcnt < 5.00 ) THEN "ideal_model" ELSE "non_ideal_model" END AS model_quality_tag,
FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
WHERE app_event NOT LIKE '%ad%'
ORDER BY app_event, LoB
  );
  
[0m18:24:32.360351 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:32.361597 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058dd460>]}
[0m18:24:32.362093 [info ] [Thread-1  ]: 41 of 47 OK created table model dbt_anomaly_detection.filtered_all_models ...... [[32mCREATE TABLE (2.0 rows, 146.0 Bytes processed)[0m in 2.63s]
[0m18:24:32.362561 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_all_models
[0m18:24:32.362768 [debug] [Thread-1  ]: Began running node model.anomaly_detection.trade_off_max_RMSD_results
[0m18:24:32.362962 [info ] [Thread-1  ]: 42 of 47 START table model dbt_anomaly_detection.trade_off_max_RMSD_results .... [RUN]
[0m18:24:32.363830 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.trade_off_max_RMSD_results"
[0m18:24:32.364052 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.trade_off_max_RMSD_results
[0m18:24:32.364202 [debug] [Thread-1  ]: Compiling model.anomaly_detection.trade_off_max_RMSD_results
[0m18:24:32.368404 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.trade_off_max_RMSD_results"
[0m18:24:32.369247 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:32.369412 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.trade_off_max_RMSD_results
[0m18:24:32.371669 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:24:32.795802 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.trade_off_max_RMSD_results"
[0m18:24:32.796512 [debug] [Thread-1  ]: On model.anomaly_detection.trade_off_max_RMSD_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.trade_off_max_RMSD_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_max_RMSD_results`
  
  
  OPTIONS()
  as (
    

SELECT anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.LoB, anomalies_criteria_res_dbt.control_config, 
    anomalies_criteria_res_dbt.anomalies, anomalies_criteria_res_dbt.RMSD_prcnt, anomalies_criteria_res_dbt.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_min_anomalies_results` AS anomalies_criteria_res_dbt
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_max_RMSD` AS min_anomalies_max_RMSD
    ON anomalies_criteria_res_dbt.RMSD_prcnt = min_anomalies_max_RMSD.RMSD_prcnt
      AND anomalies_criteria_res_dbt.app_event = min_anomalies_max_RMSD.app_event
      AND anomalies_criteria_res_dbt.LoB = min_anomalies_max_RMSD.LoB
  ORDER BY anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.LoB, control_config
  );
  
[0m18:24:34.869855 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:34.871776 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058aef70>]}
[0m18:24:34.872386 [info ] [Thread-1  ]: 42 of 47 OK created table model dbt_anomaly_detection.trade_off_max_RMSD_results  [[32mCREATE TABLE (1.0 rows, 922.0 Bytes processed)[0m in 2.51s]
[0m18:24:34.872915 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.trade_off_max_RMSD_results
[0m18:24:34.873149 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m18:24:34.873666 [info ] [Thread-1  ]: 43 of 47 START table model dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [RUN]
[0m18:24:34.874490 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m18:24:34.874673 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m18:24:34.874848 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m18:24:34.881825 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m18:24:34.882644 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:34.882804 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m18:24:34.885604 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:24:35.278296 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m18:24:35.279354 [debug] [Thread-1  ]: On model.anomaly_detection.derived_alerts_fired_automation_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_alerts_fired_automation_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_alerts_fired_automation_dbt`
  
  
  OPTIONS()
  as (
    
  
  WITH ml_detect_updated AS (
    SELECT app_event, LoB, 
      CONCAT(agg_tag, '_', prob_threshold, "threshold", '_', RTRIM(LTRIM(training_period, "derived_models_"), CONCAT('_', agg_tag))) AS control_config,
      time_stamps, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked`
  )
  SELECT time_stamps, all_configs.app_event AS app_event, all_configs.LoB AS LoB, control_table.control_config AS control_config, 
    anomalies, RMSD_prcnt, neg_lower, model_quality_tag,
    event_count, lower_bound, upper_bound, anomaly_probability, is_anomaly
  FROM ml_detect_updated AS all_configs 
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`filtered_all_models` AS control_table 
    ON all_configs.app_event = control_table.app_event
      AND all_configs.LoB = control_table.LoB
      AND all_configs.control_config = control_table.control_config
  ORDER BY all_configs.app_event, all_configs.LoB, time_stamps
  );
  
[0m18:24:37.891085 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:37.892002 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105881430>]}
[0m18:24:37.892480 [info ] [Thread-1  ]: 43 of 47 OK created table model dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [[32mCREATE TABLE (55.0 rows, 148.7 KB processed)[0m in 3.02s]
[0m18:24:37.893033 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m18:24:37.893251 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_models
[0m18:24:37.893769 [info ] [Thread-1  ]: 44 of 47 START table model dbt_anomaly_detection.all_models .................... [RUN]
[0m18:24:37.894322 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_models"
[0m18:24:37.894507 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_models
[0m18:24:37.894678 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_models
[0m18:24:37.903998 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_models"
[0m18:24:37.904877 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:37.905029 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_models
[0m18:24:37.907468 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:24:38.290407 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_models"
[0m18:24:38.291638 [debug] [Thread-1  ]: On model.anomaly_detection.all_models: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_models"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_models`
  
  
  OPTIONS()
  as (
    

     select *
     from `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
     union all
     select *
     from `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_max_RMSD_results`
     order by app_event, LoB
  );
  
[0m18:24:40.461143 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:40.462709 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10591e430>]}
[0m18:24:40.463270 [info ] [Thread-1  ]: 44 of 47 OK created table model dbt_anomaly_detection.all_models ............... [[32mCREATE TABLE (3.0 rows, 221.0 Bytes processed)[0m in 2.57s]
[0m18:24:40.463739 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_models
[0m18:24:40.463940 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events
[0m18:24:40.464250 [info ] [Thread-1  ]: 45 of 47 START table model dbt_anomaly_detection.remaining_events .............. [RUN]
[0m18:24:40.464741 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events"
[0m18:24:40.464896 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events
[0m18:24:40.465053 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events
[0m18:24:40.469266 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events"
[0m18:24:40.470104 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:40.470258 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events
[0m18:24:40.472648 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:24:40.861149 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events"
[0m18:24:40.862548 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events`
  
  
  OPTIONS()
  as (
    

select all_events.app_event as app_event, LoB, control_config, anomalies, RMSD_prcnt, neg_lower
from `ld-snowplow`.`dbt_anomaly_detection`.`ref_distinct_tuples` as all_events
left join `ld-snowplow`.`dbt_anomaly_detection`.`trade_off_max_RMSD_results` as results 
on all_events.app_event = results.app_event 
where control_config is null
  );
  
[0m18:24:43.265752 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:43.266786 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10587ce50>]}
[0m18:24:43.267308 [info ] [Thread-1  ]: 45 of 47 OK created table model dbt_anomaly_detection.remaining_events ......... [[32mCREATE TABLE (1.0 rows, 105.0 Bytes processed)[0m in 2.80s]
[0m18:24:43.267809 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events
[0m18:24:43.268026 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_events_with_anomalies
[0m18:24:43.268415 [info ] [Thread-1  ]: 46 of 47 START table model dbt_anomaly_detection.derived_events_with_anomalies . [RUN]
[0m18:24:43.269257 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_events_with_anomalies"
[0m18:24:43.269442 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_events_with_anomalies
[0m18:24:43.269620 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_events_with_anomalies
[0m18:24:43.274124 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_events_with_anomalies"
[0m18:24:43.275002 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:43.275206 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_events_with_anomalies
[0m18:24:43.277964 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:24:43.738825 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_events_with_anomalies"
[0m18:24:43.739455 [debug] [Thread-1  ]: On model.anomaly_detection.derived_events_with_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_events_with_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_events_with_anomalies`
  
  
  OPTIONS()
  as (
    
SELECT time_stamps, app_event, LoB, event_count AS event_count_anomalous_yesterday
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_alerts_fired_automation_dbt`
WHERE DATE(time_stamps) = CURRENT_DATE() - 1 
  AND is_anomaly
  );
  
[0m18:24:46.061332 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:46.062798 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058c2220>]}
[0m18:24:46.063409 [info ] [Thread-1  ]: 46 of 47 OK created table model dbt_anomaly_detection.derived_events_with_anomalies  [[32mCREATE TABLE (0.0 rows, 2.1 KB processed)[0m in 2.79s]
[0m18:24:46.063933 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_events_with_anomalies
[0m18:24:46.064164 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_features
[0m18:24:46.064400 [info ] [Thread-1  ]: 47 of 47 START table model dbt_anomaly_detection.remaining_events_features ..... [RUN]
[0m18:24:46.064867 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_features"
[0m18:24:46.065202 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_features
[0m18:24:46.065437 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_features
[0m18:24:46.072417 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_features"
[0m18:24:46.073080 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:46.073243 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_features
[0m18:24:46.076043 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:24:46.484267 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_features"
[0m18:24:46.486886 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_features: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_features"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features`
  
  
  OPTIONS()
  as (
    

select remaning_events.app_event as app_event, all_features.LoB as LoB, all_features.control_config as control_config, all_features.anomalies as anomalies, all_features.RMSD_prcnt as RMSD_prcnt, all_features.neg_lower as neg_lower
from `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events` as remaning_events
inner join `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt` as all_features
on remaning_events.app_event = all_features.app_event
  );
  
[0m18:24:48.681335 [debug] [Thread-1  ]: finished collecting timing info
[0m18:24:48.682099 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b87a9c8-5c0f-47f0-bfc6-42a180fd027a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105848190>]}
[0m18:24:48.682557 [info ] [Thread-1  ]: 47 of 47 OK created table model dbt_anomaly_detection.remaining_events_features  [[32mCREATE TABLE (24.0 rows, 3.5 KB processed)[0m in 2.62s]
[0m18:24:48.683041 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_features
[0m18:24:48.684623 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m18:24:48.685263 [info ] [MainThread]: 
[0m18:24:48.685544 [info ] [MainThread]: Finished running 35 table models, 12 model models in 0 hours 4 minutes and 2.40 seconds (242.40s).
[0m18:24:48.685793 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:24:48.685943 [debug] [MainThread]: Connection 'model.anomaly_detection.remaining_events_features' was properly closed.
[0m18:24:48.700992 [info ] [MainThread]: 
[0m18:24:48.701261 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:24:48.701500 [info ] [MainThread]: 
[0m18:24:48.701667 [info ] [MainThread]: Done. PASS=47 WARN=0 ERROR=0 SKIP=0 TOTAL=47
[0m18:24:48.701903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10585cca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056dea30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10575dc10>]}
[0m18:24:48.702084 [debug] [MainThread]: Flushing usage events


============================== 2023-02-10 18:40:33.701137 | 2f865e4d-652e-4142-9e93-bd6f0be7e06b ==============================
[0m18:40:33.701166 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:40:33.701532 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m18:40:33.701612 [debug] [MainThread]: Tracking: tracking
[0m18:40:33.708519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109630e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109630040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109630550>]}
[0m18:40:33.754194 [debug] [MainThread]: Partial parsing enabled: 17 files deleted, 0 files added, 1 files changed.
[0m18:40:33.755623 [debug] [MainThread]: Partial parsing: deleted source source.anomaly_detection.derived_app_event_2.pco_web_apply_filter_exported
[0m18:40:33.755696 [debug] [MainThread]: Partial parsing: deleted source source.anomaly_detection.derived_app_event_1.web_purchase_exported
[0m18:40:33.755813 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/trade_off_max_RMSD.sql
[0m18:40:33.755875 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/trade_off_max_RMSD_results.sql
[0m18:40:33.755929 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/remaining_events.sql
[0m18:40:33.755979 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/trade_off_min_anomalies_results.sql
[0m18:40:33.756027 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/all_models.sql
[0m18:40:33.756076 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/trade_off_phase.sql
[0m18:40:33.756123 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/trade_off_min_anomalies.sql
[0m18:40:33.756170 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/remaining_events_features.sql
[0m18:40:33.756263 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/remaining_events_min_anomalies.sql
[0m18:40:33.763507 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_events_with_anomalies.sql
[0m18:40:33.769158 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_alerts_fired_automation_dbt.sql
[0m18:40:33.770446 [debug] [MainThread]: 1699: static parser successfully parsed example/ref_distinct_tuples.sql
[0m18:40:33.771595 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_all_models.sql
[0m18:40:33.773044 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_RMSD_results.sql
[0m18:40:33.774190 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_RMSD.sql
[0m18:40:33.775379 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies_results.sql
[0m18:40:33.776840 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_features_null_filtered.sql
[0m18:40:33.778213 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies.sql
[0m18:40:33.779327 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_model_features_dbt.sql
[0m18:40:33.780605 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_model_features_dbt.sql
[0m18:40:33.781855 [debug] [MainThread]: 1699: static parser successfully parsed example/ml_detect_tweaked.sql
[0m18:40:33.783412 [debug] [MainThread]: 1603: static parser failed on example/derived_ml_detect.sql
[0m18:40:33.791247 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_ml_detect.sql
[0m18:40:33.791864 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_4hr.sql
[0m18:40:33.794819 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_4hr.sql
[0m18:40:33.795422 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_8hr.sql
[0m18:40:33.798719 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_8hr.sql
[0m18:40:33.799332 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_4hr.sql
[0m18:40:33.801922 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_4hr.sql
[0m18:40:33.802515 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_8hr.sql
[0m18:40:33.805222 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_8hr.sql
[0m18:40:33.805810 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_4hr.sql
[0m18:40:33.808426 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_4hr.sql
[0m18:40:33.809011 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_8hr.sql
[0m18:40:33.812048 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_8hr.sql
[0m18:40:33.812692 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_long.sql
[0m18:40:33.813857 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_bounds_long.sql
[0m18:40:33.815018 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_quartiles_long.sql
[0m18:40:33.816369 [debug] [MainThread]: 1699: static parser successfully parsed example/all_agg_derived_cutoff_long.sql
[0m18:40:33.818340 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_12hr.sql
[0m18:40:33.821113 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_12hr.sql
[0m18:40:33.821687 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_24hr.sql
[0m18:40:33.853198 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_24hr.sql
[0m18:40:33.853948 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_12hr.sql
[0m18:40:33.856926 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_12hr.sql
[0m18:40:33.857529 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_24hr.sql
[0m18:40:33.860414 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_24hr.sql
[0m18:40:33.861003 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_12hr.sql
[0m18:40:33.863848 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_12hr.sql
[0m18:40:33.864473 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_24hr.sql
[0m18:40:33.867353 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_24hr.sql
[0m18:40:33.867941 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_short.sql
[0m18:40:33.869080 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_bounds_short.sql
[0m18:40:33.870272 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_quartiles_short.sql
[0m18:40:33.871341 [debug] [MainThread]: 1699: static parser successfully parsed example/all_agg_derived_cutoff_short.sql
[0m18:40:33.872449 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_nonrecent_events.sql
[0m18:40:33.873539 [debug] [MainThread]: 1699: static parser successfully parsed example/all_agg_derived_cutoff.sql
[0m18:40:33.874763 [debug] [MainThread]: 1699: static parser successfully parsed example/count_cutoff.sql
[0m18:40:33.875863 [debug] [MainThread]: 1699: static parser successfully parsed example/dev_ml_model_constraints.sql
[0m18:40:33.877106 [debug] [MainThread]: 1603: static parser failed on example/all_agg_derived.sql
[0m18:40:33.880474 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/all_agg_derived.sql
[0m18:40:33.881038 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_ref_including_LoBs.sql
[0m18:40:33.882128 [debug] [MainThread]: 1699: static parser successfully parsed example/reference_derived.sql
[0m18:40:33.896372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109940d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1097f14c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095aeee0>]}
[0m18:40:33.896529 [debug] [MainThread]: Flushing usage events
[0m18:40:34.346489 [error] [MainThread]: Encountered an error:
Compilation Error in model reference_derived (models/example/reference_derived.sql)
  Model 'model.anomaly_detection.reference_derived' (models/example/reference_derived.sql) depends on a source named 'derived_app_event_1.web_purchase_exported' which was not found
[0m18:40:34.347882 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 390, in load
    self.process_sources(self.root_project.project_name)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 921, in process_sources
    _process_sources_for_node(self.manifest, current_project, node)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1302, in _process_sources_for_node
    invalid_source_fail_unless_test(
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/parser/manifest.py", line 955, in invalid_source_fail_unless_test
    source_target_not_found(node, target_name, target_table_name, disabled=disabled)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/exceptions.py", line 656, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/opt/homebrew/Cellar/dbt-bigquery/1.2.0/libexec/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model reference_derived (models/example/reference_derived.sql)
  Model 'model.anomaly_detection.reference_derived' (models/example/reference_derived.sql) depends on a source named 'derived_app_event_1.web_purchase_exported' which was not found



============================== 2023-02-10 18:42:15.135674 | 5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd ==============================
[0m18:42:15.135716 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:42:15.136097 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m18:42:15.136211 [debug] [MainThread]: Tracking: tracking
[0m18:42:15.143751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096a3400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096a3370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096a33d0>]}
[0m18:42:15.186423 [debug] [MainThread]: Partial parsing enabled: 16 files deleted, 0 files added, 1 files changed.
[0m18:42:15.186736 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/trade_off_min_anomalies.sql
[0m18:42:15.186806 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/trade_off_max_RMSD.sql
[0m18:42:15.186862 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/trade_off_phase.sql
[0m18:42:15.186919 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/trade_off_min_anomalies_results.sql
[0m18:42:15.186970 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/trade_off_max_RMSD_results.sql
[0m18:42:15.187019 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/remaining_events_features.sql
[0m18:42:15.187067 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/all_models.sql
[0m18:42:15.187114 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/remaining_events.sql
[0m18:42:15.187266 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/remaining_events_min_anomalies.sql
[0m18:42:15.194527 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies.sql
[0m18:42:15.213059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f3efd0>]}
[0m18:42:15.248280 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095da730>]}
[0m18:42:15.248489 [info ] [MainThread]: Found 40 models, 0 tests, 0 snapshots, 0 analyses, 540 macros, 0 operations, 0 seed files, 2 sources, 0 exposures, 0 metrics
[0m18:42:15.248610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109a62ee0>]}
[0m18:42:15.249836 [info ] [MainThread]: 
[0m18:42:15.250069 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m18:42:15.251389 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow"
[0m18:42:15.251551 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:42:16.538607 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow_dbt_anomaly_detection"
[0m18:42:16.539288 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:42:16.958412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1083a6af0>]}
[0m18:42:16.959585 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:42:16.960203 [info ] [MainThread]: 
[0m18:42:16.963776 [debug] [Thread-1  ]: Began running node model.anomaly_detection.reference_derived
[0m18:42:16.964225 [info ] [Thread-1  ]: 1 of 39 START table model dbt_anomaly_detection.reference_derived .............. [RUN]
[0m18:42:16.964712 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.reference_derived"
[0m18:42:16.964866 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.reference_derived
[0m18:42:16.965128 [debug] [Thread-1  ]: Compiling model.anomaly_detection.reference_derived
[0m18:42:16.970106 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.reference_derived"
[0m18:42:16.970799 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:16.970938 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.reference_derived
[0m18:42:16.983679 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:42:17.401294 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.reference_derived"
[0m18:42:17.401710 [debug] [Thread-1  ]: On model.anomaly_detection.reference_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.reference_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived`
  
  
  OPTIONS()
  as (
    
SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`web_purchase_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY) AND DATE(collector_tstamp) < CURRENT_DATE()

UNION ALL

SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`pco_web_apply_filter_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY) AND DATE(collector_tstamp) < CURRENT_DATE()
  );
  
[0m18:42:21.848981 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:21.849424 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f93e20>]}
[0m18:42:21.849655 [info ] [Thread-1  ]: 1 of 39 OK created table model dbt_anomaly_detection.reference_derived ......... [[32mCREATE TABLE (1.1m rows, 34.8 MB processed)[0m in 4.88s]
[0m18:42:21.849926 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.reference_derived
[0m18:42:21.850296 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ref_including_LoBs
[0m18:42:21.850574 [info ] [Thread-1  ]: 2 of 39 START table model dbt_anomaly_detection.derived_ref_including_LoBs ..... [RUN]
[0m18:42:21.850839 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_ref_including_LoBs"
[0m18:42:21.850933 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_ref_including_LoBs
[0m18:42:21.851022 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_ref_including_LoBs
[0m18:42:21.853368 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_ref_including_LoBs"
[0m18:42:21.853820 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:21.853943 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_ref_including_LoBs
[0m18:42:21.855677 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:42:22.294912 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_ref_including_LoBs"
[0m18:42:22.295891 [debug] [Thread-1  ]: On model.anomaly_detection.derived_ref_including_LoBs: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_ref_including_LoBs"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs`
  
  
  OPTIONS()
  as (
    

SELECT collector_tstamp, event, user_event_name, app_id,
    CASE
    WHEN LOWER(app_id) LIKE '%pcexpress%' OR LOWER(app_id) LIKE '%pcx%' THEN "PCX"
    WHEN LOWER(app_id) LIKE '%sdm%' OR LOWER(app_id) LIKE '%beauty%' THEN "SDM Shop"
    WHEN LOWER(app_id) LIKE '%drx%' THEN "SDM Drx" 
    -- WHEN LOWER(page_urlhost) like '%pcoptimum.ca%' and (LOWER(app_id) like '%ios%' or LOWER(app_id) like '%android%') then 'PC Optimum' 
    -- not recommended to use page_urlhost for mobile apps
    WHEN LOWER(app_id) like '%pco%' THEN 'PC Optimum'
    -- WHEN LOWER(page_urlhost) like '%presidentschoice.ca%' THEN 'PC' -- placeholder for after the launch 
    -- WHEN LOWER(page_urlhost) like '%joefresh.com%' and LOWER(app_id) like '%ios%' then 'JF' -- placeholder for after the launch 
    WHEN LOWER(app_id) like '%jf%' THEN 'JF'
    ELSE app_id END AS LoB,
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived`
  );
  
[0m18:42:27.061920 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:27.062905 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f53f70>]}
[0m18:42:27.063446 [info ] [Thread-1  ]: 2 of 39 OK created table model dbt_anomaly_detection.derived_ref_including_LoBs  [[32mCREATE TABLE (1.1m rows, 34.4 MB processed)[0m in 5.21s]
[0m18:42:27.064010 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ref_including_LoBs
[0m18:42:27.064930 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived
[0m18:42:27.065463 [info ] [Thread-1  ]: 3 of 39 START table model dbt_anomaly_detection.all_agg_derived ................ [RUN]
[0m18:42:27.066072 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived"
[0m18:42:27.066251 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived
[0m18:42:27.066421 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived
[0m18:42:27.072798 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived"
[0m18:42:27.073482 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:27.073661 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived
[0m18:42:27.076497 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:42:27.479097 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived"
[0m18:42:27.480387 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
  
  
  OPTIONS()
  as (
    


SELECT "4hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_4hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/4)*4 AS _4hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _4hr_trunc,
    app_id,
    LoB,
    event_type)

UNION ALL


SELECT "8hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_8hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/8)*8 AS _8hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _8hr_trunc,
    app_id,
    LoB,
    event_type)

UNION ALL


SELECT "12hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_12hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/12)*12 AS _12hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _12hr_trunc,
    app_id,
    LoB,
    event_type)

UNION ALL


SELECT "24hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_24hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/24)*24 AS _24hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _24hr_trunc,
    app_id,
    LoB,
    event_type)

ORDER BY
  agg_tag,
  time_stamps,
  app_id,
  LoB,
  event_type


  );
  
[0m18:42:31.670097 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:31.671199 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b016f70>]}
[0m18:42:31.671746 [info ] [Thread-1  ]: 3 of 39 OK created table model dbt_anomaly_detection.all_agg_derived ........... [[32mCREATE TABLE (2.1k rows, 41.5 MB processed)[0m in 4.61s]
[0m18:42:31.672319 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived
[0m18:42:31.673322 [debug] [Thread-1  ]: Began running node model.anomaly_detection.count_cutoff
[0m18:42:31.673716 [info ] [Thread-1  ]: 4 of 39 START table model dbt_anomaly_detection.count_cutoff ................... [RUN]
[0m18:42:31.674315 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.count_cutoff"
[0m18:42:31.674526 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.count_cutoff
[0m18:42:31.674717 [debug] [Thread-1  ]: Compiling model.anomaly_detection.count_cutoff
[0m18:42:31.684368 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.count_cutoff"
[0m18:42:31.685190 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:31.685352 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.count_cutoff
[0m18:42:31.687833 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:42:32.167498 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.count_cutoff"
[0m18:42:32.169364 [debug] [Thread-1  ]: On model.anomaly_detection.count_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.count_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, LoB, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select agg_tag, app_event, LoB, min(time_stamps) as strt_time
from pairs
where event_count > 50
group by app_event, LoB, agg_tag 
order by app_event, LoB, agg_tag
  );
  
[0m18:42:34.396424 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:34.397492 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095a4040>]}
[0m18:42:34.398029 [info ] [Thread-1  ]: 4 of 39 OK created table model dbt_anomaly_detection.count_cutoff .............. [[32mCREATE TABLE (8.0 rows, 92.3 KB processed)[0m in 2.72s]
[0m18:42:34.398608 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.count_cutoff
[0m18:42:34.398870 [debug] [Thread-1  ]: Began running node model.anomaly_detection.dev_ml_model_constraints
[0m18:42:34.399499 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.dev_ml_model_constraints"
[0m18:42:34.399928 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.dev_ml_model_constraints
[0m18:42:34.400130 [debug] [Thread-1  ]: Compiling model.anomaly_detection.dev_ml_model_constraints
[0m18:42:34.405140 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.dev_ml_model_constraints"
[0m18:42:34.405911 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:34.406370 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.dev_ml_model_constraints
[0m18:42:34.406541 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff
[0m18:42:34.406925 [info ] [Thread-1  ]: 5 of 39 START table model dbt_anomaly_detection.all_agg_derived_cutoff ......... [RUN]
[0m18:42:34.407575 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff"
[0m18:42:34.407734 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff
[0m18:42:34.407868 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff
[0m18:42:34.411827 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m18:42:34.412442 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:34.412583 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff
[0m18:42:34.414941 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:42:34.842849 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m18:42:34.844155 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, LoB, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select time_stamps, app_event, LoB, agg_tag, event_count
from(
select time_stamps, strt_time, pairs.app_event, pairs.LoB, pairs.agg_tag, event_count
from pairs
inner join `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff` as cutoff
on pairs.agg_tag = cutoff.agg_tag
and pairs.app_event = cutoff.app_event
and pairs.LoB = cutoff.LoB
order by pairs.app_event, pairs.LoB, pairs.agg_tag, time_stamps)
where time_stamps >= strt_time
  );
  
[0m18:42:37.384468 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:37.386502 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b02cb80>]}
[0m18:42:37.387630 [info ] [Thread-1  ]: 5 of 39 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff .... [[32mCREATE TABLE (2.1k rows, 92.6 KB processed)[0m in 2.98s]
[0m18:42:37.388746 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff
[0m18:42:37.392308 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m18:42:37.393108 [info ] [Thread-1  ]: 6 of 39 START table model dbt_anomaly_detection.all_agg_derived_cutoff_long .... [RUN]
[0m18:42:37.394426 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m18:42:37.394917 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_long
[0m18:42:37.395154 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_long
[0m18:42:37.401374 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m18:42:37.402439 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:37.402709 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_long
[0m18:42:37.411643 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:42:37.850222 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m18:42:37.887524 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB(CURRENT_DATE(), INTERVAL 10 DAY)
  );
  
[0m18:42:40.614495 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:40.616000 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f67340>]}
[0m18:42:40.616733 [info ] [Thread-1  ]: 6 of 39 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_long  [[32mCREATE TABLE (1.9k rows, 90.2 KB processed)[0m in 3.22s]
[0m18:42:40.617501 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m18:42:40.618002 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m18:42:40.618307 [info ] [Thread-1  ]: 7 of 39 START table model dbt_anomaly_detection.all_agg_derived_cutoff_short ... [RUN]
[0m18:42:40.618966 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m18:42:40.619232 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_short
[0m18:42:40.619446 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_short
[0m18:42:40.629076 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m18:42:40.630905 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:40.631173 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_short
[0m18:42:40.635296 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:42:41.037993 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m18:42:41.040923 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB(CURRENT_DATE(), INTERVAL 15 DAY)
  );
  
[0m18:42:42.980158 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:42.982349 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f89f10>]}
[0m18:42:42.983348 [info ] [Thread-1  ]: 7 of 39 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_short  [[32mCREATE TABLE (1.8k rows, 90.2 KB processed)[0m in 2.36s]
[0m18:42:42.984348 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m18:42:42.984834 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_nonrecent_events
[0m18:42:42.985240 [info ] [Thread-1  ]: 8 of 39 START table model dbt_anomaly_detection.derived_nonrecent_events ....... [RUN]
[0m18:42:42.986279 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_nonrecent_events"
[0m18:42:42.986622 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_nonrecent_events
[0m18:42:42.986877 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_nonrecent_events
[0m18:42:42.995258 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m18:42:42.996505 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:42.996779 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_nonrecent_events
[0m18:42:43.001030 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:42:43.475015 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m18:42:43.477465 [debug] [Thread-1  ]: On model.anomaly_detection.derived_nonrecent_events: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_nonrecent_events"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events`
  
  
  OPTIONS()
  as (
    

SELECT MIN(time_stamps) AS strt_time, app_event, LoB 
FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
GROUP BY app_event, LoB
HAVING DATE(MIN(time_stamps)) < DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
ORDER BY strt_time
  );
  
[0m18:42:45.709578 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:45.711290 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b047130>]}
[0m18:42:45.712052 [info ] [Thread-1  ]: 8 of 39 OK created table model dbt_anomaly_detection.derived_nonrecent_events .. [[32mCREATE TABLE (2.0 rows, 62.6 KB processed)[0m in 2.73s]
[0m18:42:45.712948 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_nonrecent_events
[0m18:42:45.713567 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_long
[0m18:42:45.714005 [info ] [Thread-1  ]: 9 of 39 START table model dbt_anomaly_detection.aggregation_quartiles_long ..... [RUN]
[0m18:42:45.714988 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_long"
[0m18:42:45.715404 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_long
[0m18:42:45.715748 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_long
[0m18:42:45.729302 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m18:42:45.730915 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:45.731272 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_long
[0m18:42:45.739088 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:42:46.181337 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m18:42:46.183747 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m18:42:48.798712 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:48.799535 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f89970>]}
[0m18:42:48.800001 [info ] [Thread-1  ]: 9 of 39 OK created table model dbt_anomaly_detection.aggregation_quartiles_long  [[32mCREATE TABLE (8.0 rows, 53.0 KB processed)[0m in 3.08s]
[0m18:42:48.800473 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_long
[0m18:42:48.800699 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_short
[0m18:42:48.801394 [info ] [Thread-1  ]: 10 of 39 START table model dbt_anomaly_detection.aggregation_quartiles_short ... [RUN]
[0m18:42:48.801891 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_short"
[0m18:42:48.802069 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_short
[0m18:42:48.802290 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_short
[0m18:42:48.807090 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m18:42:48.808139 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:48.808334 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_short
[0m18:42:48.811352 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:42:49.227630 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m18:42:49.229953 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m18:42:51.366501 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:51.368556 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b001100>]}
[0m18:42:51.369984 [info ] [Thread-1  ]: 10 of 39 OK created table model dbt_anomaly_detection.aggregation_quartiles_short  [[32mCREATE TABLE (8.0 rows, 49.7 KB processed)[0m in 2.57s]
[0m18:42:51.370719 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_short
[0m18:42:51.370910 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_long
[0m18:42:51.371448 [info ] [Thread-1  ]: 11 of 39 START table model dbt_anomaly_detection.aggregation_bounds_long ....... [RUN]
[0m18:42:51.371939 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_long"
[0m18:42:51.372076 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_long
[0m18:42:51.372211 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_long
[0m18:42:51.376718 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m18:42:51.377626 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:51.377726 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_long
[0m18:42:51.379293 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:42:51.808458 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m18:42:51.810041 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m18:42:54.150559 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:54.152200 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b07b790>]}
[0m18:42:54.153043 [info ] [Thread-1  ]: 11 of 39 OK created table model dbt_anomaly_detection.aggregation_bounds_long .. [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.78s]
[0m18:42:54.153861 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_long
[0m18:42:54.154351 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_short
[0m18:42:54.155065 [info ] [Thread-1  ]: 12 of 39 START table model dbt_anomaly_detection.aggregation_bounds_short ...... [RUN]
[0m18:42:54.155779 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_short"
[0m18:42:54.155972 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_short
[0m18:42:54.156143 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_short
[0m18:42:54.163303 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m18:42:54.164181 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:54.164340 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_short
[0m18:42:54.167024 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:42:54.847129 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m18:42:54.849363 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m18:42:57.017449 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:57.017995 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0076a0>]}
[0m18:42:57.018311 [info ] [Thread-1  ]: 12 of 39 OK created table model dbt_anomaly_detection.aggregation_bounds_short . [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.86s]
[0m18:42:57.018651 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_short
[0m18:42:57.018828 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_long
[0m18:42:57.019278 [info ] [Thread-1  ]: 13 of 39 START table model dbt_anomaly_detection.aggregation_outliers_long ..... [RUN]
[0m18:42:57.020036 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_long"
[0m18:42:57.020196 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_long
[0m18:42:57.020326 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_long
[0m18:42:57.023558 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m18:42:57.024061 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:57.024189 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_long
[0m18:42:57.026178 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:42:57.414655 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m18:42:57.416054 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, LoB, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag, LoB,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m18:42:59.760139 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:59.761803 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b00ef70>]}
[0m18:42:59.762580 [info ] [Thread-1  ]: 13 of 39 OK created table model dbt_anomaly_detection.aggregation_outliers_long  [[32mCREATE TABLE (1.9k rows, 81.4 KB processed)[0m in 2.74s]
[0m18:42:59.763184 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_long
[0m18:42:59.763414 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_short
[0m18:42:59.764140 [info ] [Thread-1  ]: 14 of 39 START table model dbt_anomaly_detection.aggregation_outliers_short .... [RUN]
[0m18:42:59.764714 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_short"
[0m18:42:59.764899 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_short
[0m18:42:59.765070 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_short
[0m18:42:59.769976 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m18:42:59.770752 [debug] [Thread-1  ]: finished collecting timing info
[0m18:42:59.770905 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_short
[0m18:42:59.775949 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:43:00.123517 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m18:43:00.124635 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, LoB, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag, LoB,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m18:43:02.785165 [debug] [Thread-1  ]: finished collecting timing info
[0m18:43:02.786059 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0ac460>]}
[0m18:43:02.786465 [info ] [Thread-1  ]: 14 of 39 OK created table model dbt_anomaly_detection.aggregation_outliers_short  [[32mCREATE TABLE (1.8k rows, 76.3 KB processed)[0m in 3.02s]
[0m18:43:02.786859 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_short
[0m18:43:02.787038 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_4hr
[0m18:43:02.787377 [info ] [Thread-1  ]: 15 of 39 START model model dbt_anomaly_detection.derived_models_05mon_4hr ...... [RUN]
[0m18:43:02.788273 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_4hr"
[0m18:43:02.788450 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_4hr
[0m18:43:02.788586 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_4hr
[0m18:43:02.793235 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m18:43:02.793785 [debug] [Thread-1  ]: finished collecting timing info
[0m18:43:02.793907 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_4hr
[0m18:43:02.806580 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m18:43:02.806975 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:43:02.807109 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:43:14.328561 [debug] [Thread-1  ]: finished collecting timing info
[0m18:43:14.329714 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0032b0>]}
[0m18:43:14.330272 [info ] [Thread-1  ]: 15 of 39 OK created model model dbt_anomaly_detection.derived_models_05mon_4hr . [[32mOK[0m in 11.54s]
[0m18:43:14.330777 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_4hr
[0m18:43:14.331009 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_8hr
[0m18:43:14.331249 [info ] [Thread-1  ]: 16 of 39 START model model dbt_anomaly_detection.derived_models_05mon_8hr ...... [RUN]
[0m18:43:14.331927 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_8hr"
[0m18:43:14.332136 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_8hr
[0m18:43:14.332317 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_8hr
[0m18:43:14.339445 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m18:43:14.341261 [debug] [Thread-1  ]: finished collecting timing info
[0m18:43:14.341411 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_8hr
[0m18:43:14.344340 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m18:43:14.344736 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:43:14.344916 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:43:25.272079 [debug] [Thread-1  ]: finished collecting timing info
[0m18:43:25.273350 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b07b460>]}
[0m18:43:25.273876 [info ] [Thread-1  ]: 16 of 39 OK created model model dbt_anomaly_detection.derived_models_05mon_8hr . [[32mOK[0m in 10.94s]
[0m18:43:25.274332 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_8hr
[0m18:43:25.274528 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_4hr
[0m18:43:25.274855 [info ] [Thread-1  ]: 17 of 39 START model model dbt_anomaly_detection.derived_models_1mon_4hr ....... [RUN]
[0m18:43:25.275350 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_4hr"
[0m18:43:25.275508 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_4hr
[0m18:43:25.275664 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_4hr
[0m18:43:25.283565 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m18:43:25.284226 [debug] [Thread-1  ]: finished collecting timing info
[0m18:43:25.284369 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_4hr
[0m18:43:25.287534 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m18:43:25.287884 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:43:25.288035 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:43:36.489570 [debug] [Thread-1  ]: finished collecting timing info
[0m18:43:36.490861 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0ac1c0>]}
[0m18:43:36.491508 [info ] [Thread-1  ]: 17 of 39 OK created model model dbt_anomaly_detection.derived_models_1mon_4hr .. [[32mOK[0m in 11.22s]
[0m18:43:36.492125 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_4hr
[0m18:43:36.492357 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_8hr
[0m18:43:36.492810 [info ] [Thread-1  ]: 18 of 39 START model model dbt_anomaly_detection.derived_models_1mon_8hr ....... [RUN]
[0m18:43:36.493380 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_8hr"
[0m18:43:36.493568 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_8hr
[0m18:43:36.493735 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_8hr
[0m18:43:36.501830 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m18:43:36.503032 [debug] [Thread-1  ]: finished collecting timing info
[0m18:43:36.503321 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_8hr
[0m18:43:36.506741 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m18:43:36.507316 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:43:36.507550 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:43:47.393860 [debug] [Thread-1  ]: finished collecting timing info
[0m18:43:47.394894 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0a7d90>]}
[0m18:43:47.395441 [info ] [Thread-1  ]: 18 of 39 OK created model model dbt_anomaly_detection.derived_models_1mon_8hr .. [[32mOK[0m in 10.90s]
[0m18:43:47.396024 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_8hr
[0m18:43:47.396288 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_4hr
[0m18:43:47.396814 [info ] [Thread-1  ]: 19 of 39 START model model dbt_anomaly_detection.derived_models_2mon_4hr ....... [RUN]
[0m18:43:47.397521 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_4hr"
[0m18:43:47.397748 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_4hr
[0m18:43:47.397953 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_4hr
[0m18:43:47.405914 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m18:43:47.406824 [debug] [Thread-1  ]: finished collecting timing info
[0m18:43:47.406988 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_4hr
[0m18:43:47.410419 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m18:43:47.411000 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:43:47.411209 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:43:59.995933 [debug] [Thread-1  ]: finished collecting timing info
[0m18:43:59.997629 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0a75b0>]}
[0m18:43:59.998262 [info ] [Thread-1  ]: 19 of 39 OK created model model dbt_anomaly_detection.derived_models_2mon_4hr .. [[32mOK[0m in 12.60s]
[0m18:43:59.998917 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_4hr
[0m18:43:59.999137 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_8hr
[0m18:43:59.999522 [info ] [Thread-1  ]: 20 of 39 START model model dbt_anomaly_detection.derived_models_2mon_8hr ....... [RUN]
[0m18:44:00.000051 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_8hr"
[0m18:44:00.000205 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_8hr
[0m18:44:00.000340 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_8hr
[0m18:44:00.008652 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m18:44:00.009148 [debug] [Thread-1  ]: finished collecting timing info
[0m18:44:00.009266 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_8hr
[0m18:44:00.011661 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m18:44:00.011976 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:44:00.012119 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:44:11.150304 [debug] [Thread-1  ]: finished collecting timing info
[0m18:44:11.151354 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109aa8b50>]}
[0m18:44:11.151972 [info ] [Thread-1  ]: 20 of 39 OK created model model dbt_anomaly_detection.derived_models_2mon_8hr .. [[32mOK[0m in 11.15s]
[0m18:44:11.152791 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_8hr
[0m18:44:11.153163 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_12hr
[0m18:44:11.153831 [info ] [Thread-1  ]: 21 of 39 START model model dbt_anomaly_detection.derived_models_05mon_12hr ..... [RUN]
[0m18:44:11.155113 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_12hr"
[0m18:44:11.155481 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_12hr
[0m18:44:11.155755 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_12hr
[0m18:44:11.170280 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m18:44:11.170751 [debug] [Thread-1  ]: finished collecting timing info
[0m18:44:11.170850 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_12hr
[0m18:44:11.172958 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m18:44:11.173342 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:44:11.173464 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:44:22.160254 [debug] [Thread-1  ]: finished collecting timing info
[0m18:44:22.162594 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f53d00>]}
[0m18:44:22.163738 [info ] [Thread-1  ]: 21 of 39 OK created model model dbt_anomaly_detection.derived_models_05mon_12hr  [[32mOK[0m in 11.01s]
[0m18:44:22.164986 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_12hr
[0m18:44:22.165423 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_24hr
[0m18:44:22.165818 [info ] [Thread-1  ]: 22 of 39 START model model dbt_anomaly_detection.derived_models_05mon_24hr ..... [RUN]
[0m18:44:22.166899 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_24hr"
[0m18:44:22.167589 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_24hr
[0m18:44:22.167953 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_24hr
[0m18:44:22.181766 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m18:44:22.182761 [debug] [Thread-1  ]: finished collecting timing info
[0m18:44:22.182991 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_24hr
[0m18:44:22.187891 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m18:44:22.188611 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:44:22.188875 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:44:33.133729 [debug] [Thread-1  ]: finished collecting timing info
[0m18:44:33.136421 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b331f70>]}
[0m18:44:33.137695 [info ] [Thread-1  ]: 22 of 39 OK created model model dbt_anomaly_detection.derived_models_05mon_24hr  [[32mOK[0m in 10.97s]
[0m18:44:33.138834 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_24hr
[0m18:44:33.139299 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_12hr
[0m18:44:33.139693 [info ] [Thread-1  ]: 23 of 39 START model model dbt_anomaly_detection.derived_models_1mon_12hr ...... [RUN]
[0m18:44:33.140565 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_12hr"
[0m18:44:33.140908 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_12hr
[0m18:44:33.141211 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_12hr
[0m18:44:33.157804 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m18:44:33.159447 [debug] [Thread-1  ]: finished collecting timing info
[0m18:44:33.159689 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_12hr
[0m18:44:33.164158 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m18:44:33.164682 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:44:33.164914 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:44:44.109069 [debug] [Thread-1  ]: finished collecting timing info
[0m18:44:44.110963 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ade1f0>]}
[0m18:44:44.111888 [info ] [Thread-1  ]: 23 of 39 OK created model model dbt_anomaly_detection.derived_models_1mon_12hr . [[32mOK[0m in 10.97s]
[0m18:44:44.112867 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_12hr
[0m18:44:44.113560 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_24hr
[0m18:44:44.114045 [info ] [Thread-1  ]: 24 of 39 START model model dbt_anomaly_detection.derived_models_1mon_24hr ...... [RUN]
[0m18:44:44.115517 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_24hr"
[0m18:44:44.115811 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_24hr
[0m18:44:44.116059 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_24hr
[0m18:44:44.127443 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m18:44:44.128541 [debug] [Thread-1  ]: finished collecting timing info
[0m18:44:44.128811 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_24hr
[0m18:44:44.133628 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m18:44:44.135109 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:44:44.135533 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:44:54.680719 [debug] [Thread-1  ]: finished collecting timing info
[0m18:44:54.682970 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0fc640>]}
[0m18:44:54.683489 [info ] [Thread-1  ]: 24 of 39 OK created model model dbt_anomaly_detection.derived_models_1mon_24hr . [[32mOK[0m in 10.57s]
[0m18:44:54.683991 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_24hr
[0m18:44:54.684224 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_12hr
[0m18:44:54.684465 [info ] [Thread-1  ]: 25 of 39 START model model dbt_anomaly_detection.derived_models_2mon_12hr ...... [RUN]
[0m18:44:54.685087 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_12hr"
[0m18:44:54.685251 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_12hr
[0m18:44:54.685413 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_12hr
[0m18:44:54.693371 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m18:44:54.694363 [debug] [Thread-1  ]: finished collecting timing info
[0m18:44:54.694551 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_12hr
[0m18:44:54.697817 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m18:44:54.698285 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:44:54.698490 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:45:05.467867 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:05.468707 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b35b550>]}
[0m18:45:05.984732 [info ] [Thread-1  ]: 25 of 39 OK created model model dbt_anomaly_detection.derived_models_2mon_12hr . [[32mOK[0m in 10.78s]
[0m18:45:05.985517 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_12hr
[0m18:45:05.985698 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_24hr
[0m18:45:05.986002 [info ] [Thread-1  ]: 26 of 39 START model model dbt_anomaly_detection.derived_models_2mon_24hr ...... [RUN]
[0m18:45:05.986524 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_24hr"
[0m18:45:05.986660 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_24hr
[0m18:45:05.986779 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_24hr
[0m18:45:05.991497 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m18:45:05.993135 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:05.993291 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_24hr
[0m18:45:05.996088 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m18:45:05.996486 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:45:05.996641 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m18:45:16.468718 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:16.469670 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f8c8b0>]}
[0m18:45:16.470208 [info ] [Thread-1  ]: 26 of 39 OK created model model dbt_anomaly_detection.derived_models_2mon_24hr . [[32mOK[0m in 10.48s]
[0m18:45:16.470772 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_24hr
[0m18:45:16.471754 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ml_detect
[0m18:45:16.472406 [info ] [Thread-1  ]: 27 of 39 START table model dbt_anomaly_detection.derived_ml_detect ............. [RUN]
[0m18:45:16.473092 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_ml_detect"
[0m18:45:16.473312 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_ml_detect
[0m18:45:16.473520 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_ml_detect
[0m18:45:16.491126 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_ml_detect"
[0m18:45:16.491723 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:16.491870 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_ml_detect
[0m18:45:16.494036 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:45:16.922081 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_ml_detect"
[0m18:45:16.924453 [debug] [Thread-1  ]: On model.anomaly_detection.derived_ml_detect: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_ml_detect"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_ml_detect`
  
  
  OPTIONS()
  as (
    -- 2 periods of training * 2 probabality threshold * 4 aggregation levels 


  WITH test_set AS (
  SELECT
  time_stamps,
        event_count,
        app_event,
        LoB,
        agg_tag
      FROM
        `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`  
      WHERE (agg_tag = "4hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 10 DAY)) 
      OR (agg_tag = "8hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 10 DAY))
      OR (agg_tag = "12hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 15 DAY))
      OR (agg_tag = "24hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 15 DAY))
  )

  
  

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
       

  
  UNION ALL
  



    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
       

  
  ORDER BY
  agg_tag,
  time_stamps,
  app_event,
  LoB,
  prob_threshold,
  training_period
  


  );
  
[0m18:45:20.522221 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:20.524159 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0a7dc0>]}
[0m18:45:20.524710 [info ] [Thread-1  ]: 27 of 39 OK created table model dbt_anomaly_detection.derived_ml_detect ........ [[32mCREATE TABLE (1.5k rows, 487.7 KB processed)[0m in 4.05s]
[0m18:45:20.525135 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ml_detect
[0m18:45:20.525792 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ml_detect_tweaked
[0m18:45:20.526170 [info ] [Thread-1  ]: 28 of 39 START table model dbt_anomaly_detection.ml_detect_tweaked ............. [RUN]
[0m18:45:20.526548 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ml_detect_tweaked"
[0m18:45:20.526675 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ml_detect_tweaked
[0m18:45:20.526799 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ml_detect_tweaked
[0m18:45:20.532418 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ml_detect_tweaked"
[0m18:45:20.532927 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:20.533048 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ml_detect_tweaked
[0m18:45:20.535461 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:45:20.927870 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.ml_detect_tweaked"
[0m18:45:20.929144 [debug] [Thread-1  ]: On model.anomaly_detection.ml_detect_tweaked: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.ml_detect_tweaked"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked`
  
  
  OPTIONS()
  as (
     
  
  with neg_bound_reset as (
SELECT app_event, LoB, agg_tag, time_stamps, prob_threshold, training_period, event_count, 
  IF(lower_bound<0, 2, 0.77*lower_bound) AS lower_bound, 1.3*upper_bound AS upper_bound, anomaly_probability, is_anomaly
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ml_detect` )

SELECT app_event, LoB, agg_tag, time_stamps, prob_threshold, training_period, event_count, 
  lower_bound, upper_bound, anomaly_probability,
  (upper_bound < event_count OR event_count < lower_bound) AS is_anomaly
FROM neg_bound_reset
  );
  
[0m18:45:23.052081 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:23.053703 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b3752e0>]}
[0m18:45:23.054408 [info ] [Thread-1  ]: 28 of 39 OK created table model dbt_anomaly_detection.ml_detect_tweaked ........ [[32mCREATE TABLE (1.5k rows, 147.0 KB processed)[0m in 2.53s]
[0m18:45:23.055019 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ml_detect_tweaked
[0m18:45:23.055974 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_model_features_dbt
[0m18:45:23.056548 [info ] [Thread-1  ]: 29 of 39 START table model dbt_anomaly_detection.derived_model_features_dbt .... [RUN]
[0m18:45:23.057171 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_model_features_dbt"
[0m18:45:23.057349 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_model_features_dbt
[0m18:45:23.057525 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_model_features_dbt
[0m18:45:23.062316 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_model_features_dbt"
[0m18:45:23.062977 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:23.063130 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_model_features_dbt
[0m18:45:23.065718 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:45:23.439879 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_model_features_dbt"
[0m18:45:23.441210 [debug] [Thread-1  ]: On model.anomaly_detection.derived_model_features_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_model_features_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_model_features_dbt`
  
  
  OPTIONS()
  as (
    
  
  SELECT
    app_event, LoB, 
    CONCAT(agg_tag, '_', prob_threshold, "threshold", '_', RTRIM(LTRIM(training_period, "derived_models_"), CONCAT('_', agg_tag))) AS control_config,
    SUM(CASE WHEN is_anomaly = TRUE THEN 1 ELSE 0 END) AS anomalies,
    SQRT(AVG( POWER(upper_bound - lower_bound, 2) ) ) / AVG(lower_bound) AS RMSD_prcnt,
    SUM(CASE WHEN lower_bound < 0 THEN 1 ELSE 0 END) AS neg_lower
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked` AS all_configs
  GROUP BY
    app_event, 
    LoB,
    control_config
  ORDER BY
    control_config,
    app_event,
    LoB
  );
  
[0m18:45:25.756416 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:25.757422 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0b8100>]}
[0m18:45:25.757980 [info ] [Thread-1  ]: 29 of 39 OK created table model dbt_anomaly_detection.derived_model_features_dbt  [[32mCREATE TABLE (48.0 rows, 113.9 KB processed)[0m in 2.70s]
[0m18:45:25.758533 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_model_features_dbt
[0m18:45:25.759393 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_model_features_dbt
[0m18:45:25.759864 [info ] [Thread-1  ]: 30 of 39 START table model dbt_anomaly_detection.filtered_model_features_dbt ... [RUN]
[0m18:45:25.760361 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_model_features_dbt"
[0m18:45:25.760527 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_model_features_dbt
[0m18:45:25.760696 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_model_features_dbt
[0m18:45:25.767367 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_model_features_dbt"
[0m18:45:25.768262 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:25.768432 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_model_features_dbt
[0m18:45:25.771393 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:45:26.174311 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.filtered_model_features_dbt"
[0m18:45:26.175423 [debug] [Thread-1  ]: On model.anomaly_detection.filtered_model_features_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.filtered_model_features_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
  
  
  OPTIONS()
  as (
    
SELECT features.app_event, features.LoB, control_config, anomalies, RMSD_prcnt, neg_lower
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events` AS non_recent
INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`derived_model_features_dbt` AS features
  ON non_recent.app_event = features.app_event
  AND non_recent.LoB = features.LoB
  );
  
[0m18:45:28.296409 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:28.297283 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b31b0d0>]}
[0m18:45:28.297736 [info ] [Thread-1  ]: 30 of 39 OK created table model dbt_anomaly_detection.filtered_model_features_dbt  [[32mCREATE TABLE (48.0 rows, 3.5 KB processed)[0m in 2.54s]
[0m18:45:28.298199 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_model_features_dbt
[0m18:45:28.298961 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ref_distinct_tuples
[0m18:45:28.299344 [info ] [Thread-1  ]: 31 of 39 START table model dbt_anomaly_detection.ref_distinct_tuples ........... [RUN]
[0m18:45:28.299879 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ref_distinct_tuples"
[0m18:45:28.300055 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ref_distinct_tuples
[0m18:45:28.300217 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ref_distinct_tuples
[0m18:45:28.309240 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ref_distinct_tuples"
[0m18:45:28.310004 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:28.310143 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ref_distinct_tuples
[0m18:45:28.312552 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:45:28.724239 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.ref_distinct_tuples"
[0m18:45:28.725267 [debug] [Thread-1  ]: On model.anomaly_detection.ref_distinct_tuples: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.ref_distinct_tuples"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`ref_distinct_tuples`
  
  
  OPTIONS()
  as (
    
SELECT DISTINCT app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
WHERE app_event LIKE '%add%'
UNION ALL
SELECT DISTINCT app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
WHERE app_event NOT LIKE '%ad%'
ORDER BY app_event
  );
  
[0m18:45:30.850831 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:30.852829 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0532b0>]}
[0m18:45:30.853988 [info ] [Thread-1  ]: 31 of 39 OK created table model dbt_anomaly_detection.ref_distinct_tuples ...... [[32mCREATE TABLE (2.0 rows, 720.0 Bytes processed)[0m in 2.55s]
[0m18:45:30.855039 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ref_distinct_tuples
[0m18:45:30.856324 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_features_null_filtered
[0m18:45:30.856665 [info ] [Thread-1  ]: 32 of 39 START table model dbt_anomaly_detection.remaining_events_features_null_filtered  [RUN]
[0m18:45:30.857432 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_features_null_filtered"
[0m18:45:30.857705 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_features_null_filtered
[0m18:45:30.857922 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_features_null_filtered
[0m18:45:30.867321 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_features_null_filtered"
[0m18:45:30.869251 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:30.869497 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_features_null_filtered
[0m18:45:30.875087 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:45:31.311534 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_features_null_filtered"
[0m18:45:31.317029 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_features_null_filtered: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_features_null_filtered"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
where RMSD_prcnt is not null
  );
  
[0m18:45:33.975872 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:33.976569 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0ba640>]}
[0m18:45:33.976994 [info ] [Thread-1  ]: 32 of 39 OK created table model dbt_anomaly_detection.remaining_events_features_null_filtered  [[32mCREATE TABLE (48.0 rows, 3.5 KB processed)[0m in 3.12s]
[0m18:45:33.977474 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_features_null_filtered
[0m18:45:33.978080 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies
[0m18:45:33.978396 [info ] [Thread-1  ]: 33 of 39 START table model dbt_anomaly_detection.remaining_events_min_anomalies  [RUN]
[0m18:45:33.979068 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies"
[0m18:45:33.979309 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies
[0m18:45:33.979495 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies
[0m18:45:33.985121 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies"
[0m18:45:33.985891 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:33.986083 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies
[0m18:45:33.990715 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:45:34.399076 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_anomalies"
[0m18:45:34.402970 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies`
  
  
  OPTIONS()
  as (
    

SELECT app_event, LoB, MIN(anomalies) AS anomalies 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered`
  GROUP BY app_event, LoB
  ORDER BY app_event, LoB
  );
  
[0m18:45:36.503975 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:36.504765 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b3671f0>]}
[0m18:45:36.505211 [info ] [Thread-1  ]: 33 of 39 OK created table model dbt_anomaly_detection.remaining_events_min_anomalies  [[32mCREATE TABLE (2.0 rows, 1.4 KB processed)[0m in 2.53s]
[0m18:45:36.505702 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies
[0m18:45:36.506577 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m18:45:36.506967 [info ] [Thread-1  ]: 34 of 39 START table model dbt_anomaly_detection.remaining_events_min_anomalies_results  [RUN]
[0m18:45:36.507765 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m18:45:36.508111 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies_results
[0m18:45:36.508499 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies_results
[0m18:45:36.519435 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m18:45:36.521450 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:36.521622 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies_results
[0m18:45:36.524219 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:45:36.931279 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m18:45:36.934194 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_anomalies_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_anomalies_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results`
  
  
  OPTIONS()
  as (
    

SELECT neg_lower_criteria_res.app_event, neg_lower_criteria_res.LoB, neg_lower_criteria_res.control_config, 
    neg_lower_criteria_res.anomalies, neg_lower_criteria_res.RMSD_prcnt, neg_lower_criteria_res.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered` AS neg_lower_criteria_res
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies` AS min_neg_lower_min_anomalies
    ON neg_lower_criteria_res.anomalies = min_neg_lower_min_anomalies.anomalies
      AND neg_lower_criteria_res.app_event = min_neg_lower_min_anomalies.app_event
      AND neg_lower_criteria_res.LoB = min_neg_lower_min_anomalies.LoB
  ORDER BY neg_lower_criteria_res.app_event, neg_lower_criteria_res.LoB, RMSD_prcnt DESC
  );
  
[0m18:45:39.272829 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:39.273563 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f3e550>]}
[0m18:45:39.273993 [info ] [Thread-1  ]: 34 of 39 OK created table model dbt_anomaly_detection.remaining_events_min_anomalies_results  [[32mCREATE TABLE (30.0 rows, 3.5 KB processed)[0m in 2.77s]
[0m18:45:39.274617 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m18:45:39.275650 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD
[0m18:45:39.276375 [info ] [Thread-1  ]: 35 of 39 START table model dbt_anomaly_detection.remaining_events_min_RMSD ..... [RUN]
[0m18:45:39.277175 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD"
[0m18:45:39.277418 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD
[0m18:45:39.277609 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD
[0m18:45:39.285649 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD"
[0m18:45:39.298470 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:39.298845 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD
[0m18:45:39.303208 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:45:39.736351 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_RMSD"
[0m18:45:39.737336 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_RMSD: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_RMSD"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD`
  
  
  OPTIONS()
  as (
    

SELECT app_event, LoB, MIN(RMSD_prcnt) AS RMSD_prcnt 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results`
  GROUP BY app_event, LoB
  ORDER BY app_event, LoB
  );
  
[0m18:45:41.981347 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:41.989575 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b00fd00>]}
[0m18:45:41.990432 [info ] [Thread-1  ]: 35 of 39 OK created table model dbt_anomaly_detection.remaining_events_min_RMSD  [[32mCREATE TABLE (2.0 rows, 900.0 Bytes processed)[0m in 2.71s]
[0m18:45:41.991844 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD
[0m18:45:41.993357 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m18:45:41.993946 [info ] [Thread-1  ]: 36 of 39 START table model dbt_anomaly_detection.remaining_events_min_RMSD_results  [RUN]
[0m18:45:41.994706 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m18:45:41.994990 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD_results
[0m18:45:41.995215 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD_results
[0m18:45:42.001943 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m18:45:42.002901 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:42.003137 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD_results
[0m18:45:42.010347 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:45:42.400161 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m18:45:42.403141 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_RMSD_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_RMSD_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
  
  
  OPTIONS()
  as (
    

SELECT anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.LoB, anomalies_criteria_res_dbt.control_config, 
    anomalies_criteria_res_dbt.anomalies, anomalies_criteria_res_dbt.RMSD_prcnt, anomalies_criteria_res_dbt.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results` AS anomalies_criteria_res_dbt
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD` AS min_anomalies_max_RMSD
    ON anomalies_criteria_res_dbt.RMSD_prcnt = min_anomalies_max_RMSD.RMSD_prcnt
      AND anomalies_criteria_res_dbt.app_event = min_anomalies_max_RMSD.app_event
      AND anomalies_criteria_res_dbt.LoB = min_anomalies_max_RMSD.LoB
  ORDER BY anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.LoB, control_config
  );
  
[0m18:45:44.985343 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:44.986574 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b305850>]}
[0m18:45:44.987388 [info ] [Thread-1  ]: 36 of 39 OK created table model dbt_anomaly_detection.remaining_events_min_RMSD_results  [[32mCREATE TABLE (2.0 rows, 2.2 KB processed)[0m in 2.99s]
[0m18:45:44.988612 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m18:45:44.990114 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_all_models
[0m18:45:44.990740 [info ] [Thread-1  ]: 37 of 39 START table model dbt_anomaly_detection.filtered_all_models ........... [RUN]
[0m18:45:44.991532 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_all_models"
[0m18:45:44.991825 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_all_models
[0m18:45:44.992094 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_all_models
[0m18:45:44.999958 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_all_models"
[0m18:45:45.000822 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:45.001029 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_all_models
[0m18:45:45.004699 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:45:45.387064 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.filtered_all_models"
[0m18:45:45.388375 [debug] [Thread-1  ]: On model.anomaly_detection.filtered_all_models: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.filtered_all_models"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`filtered_all_models`
  
  
  OPTIONS()
  as (
    
SELECT app_event, LoB, control_config, anomalies, RMSD_prcnt, neg_lower, 
  CASE WHEN (neg_lower = 0 AND anomalies < 5 AND RMSD_prcnt > 0.40 AND RMSD_prcnt < 5.00 ) THEN "ideal_model" ELSE "non_ideal_model" END AS model_quality_tag,
FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
WHERE app_event LIKE '%add%'
UNION ALL 
SELECT app_event, LoB, control_config, anomalies, RMSD_prcnt, neg_lower, 
  CASE WHEN (neg_lower = 0 AND anomalies < 5 AND RMSD_prcnt > 0.40 AND RMSD_prcnt < 5.00 ) THEN "ideal_model" ELSE "non_ideal_model" END AS model_quality_tag,
FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
WHERE app_event NOT LIKE '%ad%'
ORDER BY app_event, LoB
  );
  
[0m18:45:47.729082 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:47.731363 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f53730>]}
[0m18:45:47.731806 [info ] [Thread-1  ]: 37 of 39 OK created table model dbt_anomaly_detection.filtered_all_models ...... [[32mCREATE TABLE (2.0 rows, 146.0 Bytes processed)[0m in 2.74s]
[0m18:45:47.732239 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_all_models
[0m18:45:47.732795 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m18:45:47.733568 [info ] [Thread-1  ]: 38 of 39 START table model dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [RUN]
[0m18:45:47.734534 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m18:45:47.734696 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m18:45:47.734836 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m18:45:47.738390 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m18:45:47.738912 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:47.739056 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m18:45:47.741457 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:45:48.114468 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m18:45:48.115914 [debug] [Thread-1  ]: On model.anomaly_detection.derived_alerts_fired_automation_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_alerts_fired_automation_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_alerts_fired_automation_dbt`
  
  
  OPTIONS()
  as (
    
  
  WITH ml_detect_updated AS (
    SELECT app_event, LoB, 
      CONCAT(agg_tag, '_', prob_threshold, "threshold", '_', RTRIM(LTRIM(training_period, "derived_models_"), CONCAT('_', agg_tag))) AS control_config,
      time_stamps, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked`
  )
  SELECT time_stamps, all_configs.app_event AS app_event, all_configs.LoB AS LoB, control_table.control_config AS control_config, 
    anomalies, RMSD_prcnt, neg_lower, model_quality_tag,
    event_count, lower_bound, upper_bound, anomaly_probability, is_anomaly
  FROM ml_detect_updated AS all_configs 
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`filtered_all_models` AS control_table 
    ON all_configs.app_event = control_table.app_event
      AND all_configs.LoB = control_table.LoB
      AND all_configs.control_config = control_table.control_config
  ORDER BY all_configs.app_event, all_configs.LoB, time_stamps
  );
  
[0m18:45:50.363404 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:50.364163 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0ac160>]}
[0m18:45:50.364449 [info ] [Thread-1  ]: 38 of 39 OK created table model dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [[32mCREATE TABLE (55.0 rows, 148.7 KB processed)[0m in 2.63s]
[0m18:45:50.364687 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m18:45:50.364925 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_events_with_anomalies
[0m18:45:50.365269 [info ] [Thread-1  ]: 39 of 39 START table model dbt_anomaly_detection.derived_events_with_anomalies . [RUN]
[0m18:45:50.365605 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_events_with_anomalies"
[0m18:45:50.365714 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_events_with_anomalies
[0m18:45:50.365806 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_events_with_anomalies
[0m18:45:50.371403 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_events_with_anomalies"
[0m18:45:50.372049 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:50.372148 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_events_with_anomalies
[0m18:45:50.374847 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:45:50.633027 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_events_with_anomalies"
[0m18:45:50.634691 [debug] [Thread-1  ]: On model.anomaly_detection.derived_events_with_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_events_with_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_events_with_anomalies`
  
  
  OPTIONS()
  as (
    
SELECT time_stamps, app_event, LoB, event_count AS event_count_anomalous_yesterday
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_alerts_fired_automation_dbt`
WHERE DATE(time_stamps) = CURRENT_DATE() - 1 
  AND is_anomaly
  );
  
[0m18:45:52.886362 [debug] [Thread-1  ]: finished collecting timing info
[0m18:45:52.886903 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5897b1ee-e0da-4b0e-8ecc-13ac421fb8fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b420550>]}
[0m18:45:52.887207 [info ] [Thread-1  ]: 39 of 39 OK created table model dbt_anomaly_detection.derived_events_with_anomalies  [[32mCREATE TABLE (0.0 rows, 2.1 KB processed)[0m in 2.52s]
[0m18:45:52.887530 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_events_with_anomalies
[0m18:45:52.888484 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m18:45:52.888892 [info ] [MainThread]: 
[0m18:45:52.889087 [info ] [MainThread]: Finished running 27 table models, 12 model models in 0 hours 3 minutes and 37.64 seconds (217.64s).
[0m18:45:52.889257 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:45:52.889343 [debug] [MainThread]: Connection 'model.anomaly_detection.derived_events_with_anomalies' was properly closed.
[0m18:45:52.901906 [info ] [MainThread]: 
[0m18:45:52.902154 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:45:52.902397 [info ] [MainThread]: 
[0m18:45:52.902564 [info ] [MainThread]: Done. PASS=39 WARN=0 ERROR=0 SKIP=0 TOTAL=39
[0m18:45:52.902808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095a41f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109598cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095cab20>]}
[0m18:45:52.902998 [debug] [MainThread]: Flushing usage events


============================== 2023-02-10 19:48:56.281215 | 379a7d3b-d58f-46bb-bed5-f538af97ad82 ==============================
[0m19:48:56.281240 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:48:56.281849 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:48:56.281949 [debug] [MainThread]: Tracking: tracking
[0m19:48:56.290956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068a3370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068a3b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068a3820>]}
[0m19:48:56.348502 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 0 files added, 0 files changed.
[0m19:48:56.348724 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/dev_ml_model_constraints.sql
[0m19:48:56.357325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106cf60d0>]}
[0m19:48:56.364461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106c58940>]}
[0m19:48:56.364615 [info ] [MainThread]: Found 39 models, 0 tests, 0 snapshots, 0 analyses, 540 macros, 0 operations, 0 seed files, 2 sources, 0 exposures, 0 metrics
[0m19:48:56.364751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106c4d7f0>]}
[0m19:48:56.366087 [info ] [MainThread]: 
[0m19:48:56.366340 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m19:48:56.367727 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow"
[0m19:48:56.367946 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:48:57.590684 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow_dbt_anomaly_detection"
[0m19:48:57.591239 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:48:57.815932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106900a00>]}
[0m19:48:57.817325 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:48:57.817762 [info ] [MainThread]: 
[0m19:48:57.825359 [debug] [Thread-1  ]: Began running node model.anomaly_detection.reference_derived
[0m19:48:57.825936 [info ] [Thread-1  ]: 1 of 39 START table model dbt_anomaly_detection.reference_derived .............. [RUN]
[0m19:48:57.826809 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.reference_derived"
[0m19:48:57.827007 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.reference_derived
[0m19:48:57.827300 [debug] [Thread-1  ]: Compiling model.anomaly_detection.reference_derived
[0m19:48:57.832619 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.reference_derived"
[0m19:48:57.833238 [debug] [Thread-1  ]: finished collecting timing info
[0m19:48:57.833339 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.reference_derived
[0m19:48:57.846088 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:48:58.196192 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.reference_derived"
[0m19:48:58.197344 [debug] [Thread-1  ]: On model.anomaly_detection.reference_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.reference_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived`
  
  
  OPTIONS()
  as (
    
SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`web_purchase_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY) AND DATE(collector_tstamp) < CURRENT_DATE()

UNION ALL

SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`pco_web_apply_filter_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY) AND DATE(collector_tstamp) < CURRENT_DATE()
  );
  
[0m19:49:02.515579 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:02.516008 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d79100>]}
[0m19:49:02.516285 [info ] [Thread-1  ]: 1 of 39 OK created table model dbt_anomaly_detection.reference_derived ......... [[32mCREATE TABLE (1.1m rows, 34.8 MB processed)[0m in 4.69s]
[0m19:49:02.516595 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.reference_derived
[0m19:49:02.516980 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ref_including_LoBs
[0m19:49:02.517265 [info ] [Thread-1  ]: 2 of 39 START table model dbt_anomaly_detection.derived_ref_including_LoBs ..... [RUN]
[0m19:49:02.517688 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_ref_including_LoBs"
[0m19:49:02.517790 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_ref_including_LoBs
[0m19:49:02.517887 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_ref_including_LoBs
[0m19:49:02.556141 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_ref_including_LoBs"
[0m19:49:02.556571 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:02.556667 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_ref_including_LoBs
[0m19:49:02.558017 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:49:02.839760 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_ref_including_LoBs"
[0m19:49:02.841021 [debug] [Thread-1  ]: On model.anomaly_detection.derived_ref_including_LoBs: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_ref_including_LoBs"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs`
  
  
  OPTIONS()
  as (
    

SELECT collector_tstamp, event, user_event_name, app_id,
    CASE
    WHEN LOWER(app_id) LIKE '%pcexpress%' OR LOWER(app_id) LIKE '%pcx%' THEN "PCX"
    WHEN LOWER(app_id) LIKE '%sdm%' OR LOWER(app_id) LIKE '%beauty%' THEN "SDM Shop"
    WHEN LOWER(app_id) LIKE '%drx%' THEN "SDM Drx" 
    -- WHEN LOWER(page_urlhost) like '%pcoptimum.ca%' and (LOWER(app_id) like '%ios%' or LOWER(app_id) like '%android%') then 'PC Optimum' 
    -- not recommended to use page_urlhost for mobile apps
    WHEN LOWER(app_id) like '%pco%' THEN 'PC Optimum'
    -- WHEN LOWER(page_urlhost) like '%presidentschoice.ca%' THEN 'PC' -- placeholder for after the launch 
    -- WHEN LOWER(page_urlhost) like '%joefresh.com%' and LOWER(app_id) like '%ios%' then 'JF' -- placeholder for after the launch 
    WHEN LOWER(app_id) like '%jf%' THEN 'JF'
    ELSE app_id END AS LoB,
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived`
  );
  
[0m19:49:06.738503 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:06.739340 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d89d00>]}
[0m19:49:06.739803 [info ] [Thread-1  ]: 2 of 39 OK created table model dbt_anomaly_detection.derived_ref_including_LoBs  [[32mCREATE TABLE (1.1m rows, 34.4 MB processed)[0m in 4.22s]
[0m19:49:06.740272 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ref_including_LoBs
[0m19:49:06.740988 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived
[0m19:49:06.741369 [info ] [Thread-1  ]: 3 of 39 START table model dbt_anomaly_detection.all_agg_derived ................ [RUN]
[0m19:49:06.741831 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived"
[0m19:49:06.742000 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived
[0m19:49:06.742172 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived
[0m19:49:06.748028 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived"
[0m19:49:06.748737 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:06.748915 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived
[0m19:49:06.751598 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:49:07.019436 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived"
[0m19:49:07.020611 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
  
  
  OPTIONS()
  as (
    


SELECT "4hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_4hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/4)*4 AS _4hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _4hr_trunc,
    app_id,
    LoB,
    event_type)

UNION ALL


SELECT "8hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_8hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/8)*8 AS _8hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _8hr_trunc,
    app_id,
    LoB,
    event_type)

UNION ALL


SELECT "12hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_12hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/12)*12 AS _12hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _12hr_trunc,
    app_id,
    LoB,
    event_type)

UNION ALL


SELECT "24hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_24hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/24)*24 AS _24hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _24hr_trunc,
    app_id,
    LoB,
    event_type)

ORDER BY
  agg_tag,
  time_stamps,
  app_id,
  LoB,
  event_type


  );
  
[0m19:49:11.503550 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:11.504361 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106849a60>]}
[0m19:49:11.504813 [info ] [Thread-1  ]: 3 of 39 OK created table model dbt_anomaly_detection.all_agg_derived ........... [[32mCREATE TABLE (2.1k rows, 41.5 MB processed)[0m in 4.76s]
[0m19:49:11.505301 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived
[0m19:49:11.506083 [debug] [Thread-1  ]: Began running node model.anomaly_detection.count_cutoff
[0m19:49:11.506650 [info ] [Thread-1  ]: 4 of 39 START table model dbt_anomaly_detection.count_cutoff ................... [RUN]
[0m19:49:11.507241 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.count_cutoff"
[0m19:49:11.507427 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.count_cutoff
[0m19:49:11.507595 [debug] [Thread-1  ]: Compiling model.anomaly_detection.count_cutoff
[0m19:49:11.511978 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.count_cutoff"
[0m19:49:11.512813 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:11.512977 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.count_cutoff
[0m19:49:11.515750 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:49:11.823635 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.count_cutoff"
[0m19:49:11.824941 [debug] [Thread-1  ]: On model.anomaly_detection.count_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.count_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, LoB, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select agg_tag, app_event, LoB, min(time_stamps) as strt_time
from pairs
where event_count > 50
group by app_event, LoB, agg_tag 
order by app_event, LoB, agg_tag
  );
  
[0m19:49:13.870747 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:13.872697 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068502b0>]}
[0m19:49:13.873557 [info ] [Thread-1  ]: 4 of 39 OK created table model dbt_anomaly_detection.count_cutoff .............. [[32mCREATE TABLE (8.0 rows, 92.3 KB processed)[0m in 2.37s]
[0m19:49:13.874154 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.count_cutoff
[0m19:49:13.874924 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff
[0m19:49:13.875175 [info ] [Thread-1  ]: 5 of 39 START table model dbt_anomaly_detection.all_agg_derived_cutoff ......... [RUN]
[0m19:49:13.875654 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff"
[0m19:49:13.875802 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff
[0m19:49:13.875949 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff
[0m19:49:13.882162 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m19:49:13.882756 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:13.882903 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff
[0m19:49:13.885177 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:49:14.371608 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m19:49:14.372681 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, LoB, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select time_stamps, app_event, LoB, agg_tag, event_count
from(
select time_stamps, strt_time, pairs.app_event, pairs.LoB, pairs.agg_tag, event_count
from pairs
inner join `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff` as cutoff
on pairs.agg_tag = cutoff.agg_tag
and pairs.app_event = cutoff.app_event
and pairs.LoB = cutoff.LoB
order by pairs.app_event, pairs.LoB, pairs.agg_tag, time_stamps)
where time_stamps >= strt_time
  );
  
[0m19:49:17.044989 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:17.046074 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106dbe100>]}
[0m19:49:17.046610 [info ] [Thread-1  ]: 5 of 39 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff .... [[32mCREATE TABLE (2.1k rows, 92.6 KB processed)[0m in 3.17s]
[0m19:49:17.047145 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff
[0m19:49:17.048004 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m19:49:17.048473 [info ] [Thread-1  ]: 6 of 39 START table model dbt_anomaly_detection.all_agg_derived_cutoff_long .... [RUN]
[0m19:49:17.048981 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m19:49:17.049147 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_long
[0m19:49:17.049307 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_long
[0m19:49:17.053862 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m19:49:17.054648 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:17.054809 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_long
[0m19:49:17.057550 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:49:17.307227 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m19:49:17.309397 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB(CURRENT_DATE(), INTERVAL 10 DAY)
  );
  
[0m19:49:19.320525 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:19.321932 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106dbfc70>]}
[0m19:49:19.322517 [info ] [Thread-1  ]: 6 of 39 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_long  [[32mCREATE TABLE (1.9k rows, 90.2 KB processed)[0m in 2.27s]
[0m19:49:19.323030 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m19:49:19.323241 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m19:49:19.323600 [info ] [Thread-1  ]: 7 of 39 START table model dbt_anomaly_detection.all_agg_derived_cutoff_short ... [RUN]
[0m19:49:19.324158 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m19:49:19.324318 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_short
[0m19:49:19.324457 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_short
[0m19:49:19.328602 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m19:49:19.329379 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:19.329526 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_short
[0m19:49:19.335930 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:49:19.603240 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m19:49:19.605203 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB(CURRENT_DATE(), INTERVAL 15 DAY)
  );
  
[0m19:49:21.939331 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:21.940400 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106dbf160>]}
[0m19:49:21.940967 [info ] [Thread-1  ]: 7 of 39 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_short  [[32mCREATE TABLE (1.8k rows, 90.2 KB processed)[0m in 2.62s]
[0m19:49:21.941524 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m19:49:21.941790 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_nonrecent_events
[0m19:49:21.942072 [info ] [Thread-1  ]: 8 of 39 START table model dbt_anomaly_detection.derived_nonrecent_events ....... [RUN]
[0m19:49:21.943014 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_nonrecent_events"
[0m19:49:21.943522 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_nonrecent_events
[0m19:49:21.943776 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_nonrecent_events
[0m19:49:21.950081 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m19:49:21.950776 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:21.950932 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_nonrecent_events
[0m19:49:21.953927 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:49:22.255252 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m19:49:22.255763 [debug] [Thread-1  ]: On model.anomaly_detection.derived_nonrecent_events: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_nonrecent_events"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events`
  
  
  OPTIONS()
  as (
    

SELECT MIN(time_stamps) AS strt_time, app_event, LoB 
FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
GROUP BY app_event, LoB
HAVING DATE(MIN(time_stamps)) < DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
ORDER BY strt_time
  );
  
[0m19:49:24.545336 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:24.546346 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106dba190>]}
[0m19:49:24.546849 [info ] [Thread-1  ]: 8 of 39 OK created table model dbt_anomaly_detection.derived_nonrecent_events .. [[32mCREATE TABLE (2.0 rows, 62.6 KB processed)[0m in 2.60s]
[0m19:49:24.547370 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_nonrecent_events
[0m19:49:24.547632 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_long
[0m19:49:24.547905 [info ] [Thread-1  ]: 9 of 39 START table model dbt_anomaly_detection.aggregation_quartiles_long ..... [RUN]
[0m19:49:24.548768 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_long"
[0m19:49:24.549035 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_long
[0m19:49:24.549773 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_long
[0m19:49:24.555421 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m19:49:24.556433 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:24.556688 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_long
[0m19:49:24.564374 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:49:24.994596 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m19:49:24.999175 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m19:49:27.323232 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:27.324159 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106de9310>]}
[0m19:49:27.324792 [info ] [Thread-1  ]: 9 of 39 OK created table model dbt_anomaly_detection.aggregation_quartiles_long  [[32mCREATE TABLE (8.0 rows, 53.0 KB processed)[0m in 2.78s]
[0m19:49:27.325360 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_long
[0m19:49:27.325643 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_short
[0m19:49:27.326411 [info ] [Thread-1  ]: 10 of 39 START table model dbt_anomaly_detection.aggregation_quartiles_short ... [RUN]
[0m19:49:27.327594 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_short"
[0m19:49:27.328507 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_short
[0m19:49:27.328724 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_short
[0m19:49:27.356329 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m19:49:27.386791 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:27.420655 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_short
[0m19:49:27.430088 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:49:27.656847 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m19:49:27.657656 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m19:49:29.747119 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:29.748467 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e2c130>]}
[0m19:49:29.749381 [info ] [Thread-1  ]: 10 of 39 OK created table model dbt_anomaly_detection.aggregation_quartiles_short  [[32mCREATE TABLE (8.0 rows, 49.7 KB processed)[0m in 2.42s]
[0m19:49:29.750707 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_short
[0m19:49:29.750982 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_long
[0m19:49:29.751264 [info ] [Thread-1  ]: 11 of 39 START table model dbt_anomaly_detection.aggregation_bounds_long ....... [RUN]
[0m19:49:29.752039 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_long"
[0m19:49:29.752361 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_long
[0m19:49:29.752567 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_long
[0m19:49:29.762419 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m19:49:29.763391 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:29.763628 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_long
[0m19:49:29.770006 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:49:30.093156 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m19:49:30.095451 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m19:49:32.365062 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:32.367198 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106da17c0>]}
[0m19:49:32.368271 [info ] [Thread-1  ]: 11 of 39 OK created table model dbt_anomaly_detection.aggregation_bounds_long .. [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.62s]
[0m19:49:32.369367 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_long
[0m19:49:32.369785 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_short
[0m19:49:32.370933 [info ] [Thread-1  ]: 12 of 39 START table model dbt_anomaly_detection.aggregation_bounds_short ...... [RUN]
[0m19:49:32.371890 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_short"
[0m19:49:32.372265 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_short
[0m19:49:32.372598 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_short
[0m19:49:32.382501 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m19:49:32.384141 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:32.384460 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_short
[0m19:49:32.388795 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:49:32.616531 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m19:49:32.618062 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m19:49:34.816622 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:34.817484 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106dd9d90>]}
[0m19:49:34.818080 [info ] [Thread-1  ]: 12 of 39 OK created table model dbt_anomaly_detection.aggregation_bounds_short . [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.45s]
[0m19:49:34.818745 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_short
[0m19:49:34.818979 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_long
[0m19:49:34.819287 [info ] [Thread-1  ]: 13 of 39 START table model dbt_anomaly_detection.aggregation_outliers_long ..... [RUN]
[0m19:49:34.820099 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_long"
[0m19:49:34.820310 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_long
[0m19:49:34.820871 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_long
[0m19:49:34.835651 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m19:49:34.837707 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:34.838194 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_long
[0m19:49:34.842265 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:49:35.134832 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m19:49:35.137323 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, LoB, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag, LoB,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m19:49:37.574053 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:37.576065 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e7ae50>]}
[0m19:49:37.577010 [info ] [Thread-1  ]: 13 of 39 OK created table model dbt_anomaly_detection.aggregation_outliers_long  [[32mCREATE TABLE (1.9k rows, 81.4 KB processed)[0m in 2.76s]
[0m19:49:37.577954 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_long
[0m19:49:37.578509 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_short
[0m19:49:37.578904 [info ] [Thread-1  ]: 14 of 39 START table model dbt_anomaly_detection.aggregation_outliers_short .... [RUN]
[0m19:49:37.579631 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_short"
[0m19:49:37.579903 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_short
[0m19:49:37.580158 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_short
[0m19:49:37.588846 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m19:49:37.590064 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:37.590331 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_short
[0m19:49:37.594073 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:49:37.866796 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m19:49:37.868992 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, LoB, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag, LoB,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m19:49:39.915522 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:39.917543 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e321c0>]}
[0m19:49:39.918863 [info ] [Thread-1  ]: 14 of 39 OK created table model dbt_anomaly_detection.aggregation_outliers_short  [[32mCREATE TABLE (1.8k rows, 76.3 KB processed)[0m in 2.34s]
[0m19:49:39.920007 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_short
[0m19:49:39.920519 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_4hr
[0m19:49:39.920923 [info ] [Thread-1  ]: 15 of 39 START model model dbt_anomaly_detection.derived_models_05mon_4hr ...... [RUN]
[0m19:49:39.923400 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_4hr"
[0m19:49:39.923843 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_4hr
[0m19:49:39.924111 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_4hr
[0m19:49:39.935410 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m19:49:39.936677 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:39.936899 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_4hr
[0m19:49:39.961866 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m19:49:39.962854 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:49:39.963065 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m19:49:52.109871 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:52.112086 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106849bb0>]}
[0m19:49:52.113127 [info ] [Thread-1  ]: 15 of 39 OK created model model dbt_anomaly_detection.derived_models_05mon_4hr . [[32mOK[0m in 12.19s]
[0m19:49:52.113875 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_4hr
[0m19:49:52.114190 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_8hr
[0m19:49:52.114477 [info ] [Thread-1  ]: 16 of 39 START model model dbt_anomaly_detection.derived_models_05mon_8hr ...... [RUN]
[0m19:49:52.115079 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_8hr"
[0m19:49:52.115299 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_8hr
[0m19:49:52.116085 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_8hr
[0m19:49:52.132125 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m19:49:52.134845 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:52.135129 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_8hr
[0m19:49:52.139280 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m19:49:52.140114 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:49:52.140374 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m19:50:04.823681 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:04.825873 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068491c0>]}
[0m19:50:04.826807 [info ] [Thread-1  ]: 16 of 39 OK created model model dbt_anomaly_detection.derived_models_05mon_8hr . [[32mOK[0m in 12.71s]
[0m19:50:04.827749 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_8hr
[0m19:50:04.828301 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_4hr
[0m19:50:04.828700 [info ] [Thread-1  ]: 17 of 39 START model model dbt_anomaly_detection.derived_models_1mon_4hr ....... [RUN]
[0m19:50:04.829550 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_4hr"
[0m19:50:04.829895 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_4hr
[0m19:50:04.830208 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_4hr
[0m19:50:04.847442 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m19:50:04.848589 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:04.848821 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_4hr
[0m19:50:04.852917 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m19:50:04.853539 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:50:04.853796 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m19:50:18.132946 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:18.135153 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106838280>]}
[0m19:50:18.136424 [info ] [Thread-1  ]: 17 of 39 OK created model model dbt_anomaly_detection.derived_models_1mon_4hr .. [[32mOK[0m in 13.31s]
[0m19:50:18.137676 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_4hr
[0m19:50:18.138220 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_8hr
[0m19:50:18.138765 [info ] [Thread-1  ]: 18 of 39 START model model dbt_anomaly_detection.derived_models_1mon_8hr ....... [RUN]
[0m19:50:18.140262 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_8hr"
[0m19:50:18.140648 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_8hr
[0m19:50:18.140971 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_8hr
[0m19:50:18.156487 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m19:50:18.157770 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:18.158013 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_8hr
[0m19:50:18.163636 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m19:50:18.164187 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:50:18.164448 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m19:50:29.557451 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:29.559896 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d53a30>]}
[0m19:50:29.560969 [info ] [Thread-1  ]: 18 of 39 OK created model model dbt_anomaly_detection.derived_models_1mon_8hr .. [[32mOK[0m in 11.42s]
[0m19:50:29.562127 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_8hr
[0m19:50:29.562570 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_4hr
[0m19:50:29.562981 [info ] [Thread-1  ]: 19 of 39 START model model dbt_anomaly_detection.derived_models_2mon_4hr ....... [RUN]
[0m19:50:29.563894 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_4hr"
[0m19:50:29.564465 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_4hr
[0m19:50:29.565883 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_4hr
[0m19:50:29.579380 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m19:50:29.580579 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:29.580807 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_4hr
[0m19:50:29.585320 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m19:50:29.585831 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:50:29.586097 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m19:50:41.776133 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:41.777828 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d53910>]}
[0m19:50:41.778525 [info ] [Thread-1  ]: 19 of 39 OK created model model dbt_anomaly_detection.derived_models_2mon_4hr .. [[32mOK[0m in 12.21s]
[0m19:50:41.779236 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_4hr
[0m19:50:41.779652 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_8hr
[0m19:50:41.779931 [info ] [Thread-1  ]: 20 of 39 START model model dbt_anomaly_detection.derived_models_2mon_8hr ....... [RUN]
[0m19:50:41.780502 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_8hr"
[0m19:50:41.780709 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_8hr
[0m19:50:41.780933 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_8hr
[0m19:50:41.796115 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m19:50:41.797295 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:41.797493 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_8hr
[0m19:50:41.801555 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m19:50:41.802046 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:50:41.802307 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m19:50:52.190780 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:52.192649 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ee2d60>]}
[0m19:50:52.193491 [info ] [Thread-1  ]: 20 of 39 OK created model model dbt_anomaly_detection.derived_models_2mon_8hr .. [[32mOK[0m in 10.41s]
[0m19:50:52.194341 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_8hr
[0m19:50:52.194992 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_12hr
[0m19:50:52.195636 [info ] [Thread-1  ]: 21 of 39 START model model dbt_anomaly_detection.derived_models_05mon_12hr ..... [RUN]
[0m19:50:52.196934 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_12hr"
[0m19:50:52.197413 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_12hr
[0m19:50:52.198668 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_12hr
[0m19:50:52.233927 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m19:50:52.235638 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:52.236111 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_12hr
[0m19:50:52.240352 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m19:50:52.241055 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:50:52.241271 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m19:51:06.486798 [debug] [Thread-1  ]: finished collecting timing info
[0m19:51:06.487595 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106848550>]}
[0m19:51:06.488036 [info ] [Thread-1  ]: 21 of 39 OK created model model dbt_anomaly_detection.derived_models_05mon_12hr  [[32mOK[0m in 14.29s]
[0m19:51:06.489411 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_12hr
[0m19:51:06.489679 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_24hr
[0m19:51:06.489987 [info ] [Thread-1  ]: 22 of 39 START model model dbt_anomaly_detection.derived_models_05mon_24hr ..... [RUN]
[0m19:51:06.490862 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_24hr"
[0m19:51:06.491990 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_24hr
[0m19:51:06.492257 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_24hr
[0m19:51:06.504266 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m19:51:06.506598 [debug] [Thread-1  ]: finished collecting timing info
[0m19:51:06.506883 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_24hr
[0m19:51:06.509712 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m19:51:06.510627 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:51:06.510860 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m19:51:16.304271 [debug] [Thread-1  ]: finished collecting timing info
[0m19:51:16.304915 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e582e0>]}
[0m19:51:16.305173 [info ] [Thread-1  ]: 22 of 39 OK created model model dbt_anomaly_detection.derived_models_05mon_24hr  [[32mOK[0m in 9.81s]
[0m19:51:16.305394 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_24hr
[0m19:51:16.305494 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_12hr
[0m19:51:16.305591 [info ] [Thread-1  ]: 23 of 39 START model model dbt_anomaly_detection.derived_models_1mon_12hr ...... [RUN]
[0m19:51:16.305782 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_12hr"
[0m19:51:16.306051 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_12hr
[0m19:51:16.306406 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_12hr
[0m19:51:16.313753 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m19:51:16.315335 [debug] [Thread-1  ]: finished collecting timing info
[0m19:51:16.315501 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_12hr
[0m19:51:16.317340 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m19:51:16.317574 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:51:16.317679 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m19:51:29.960830 [debug] [Thread-1  ]: finished collecting timing info
[0m19:51:29.961909 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e5f280>]}
[0m19:51:29.962480 [info ] [Thread-1  ]: 23 of 39 OK created model model dbt_anomaly_detection.derived_models_1mon_12hr . [[32mOK[0m in 13.66s]
[0m19:51:29.962974 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_12hr
[0m19:51:29.963203 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_24hr
[0m19:51:29.963564 [info ] [Thread-1  ]: 24 of 39 START model model dbt_anomaly_detection.derived_models_1mon_24hr ...... [RUN]
[0m19:51:29.964144 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_24hr"
[0m19:51:29.964330 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_24hr
[0m19:51:29.964512 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_24hr
[0m19:51:29.974258 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m19:51:29.975161 [debug] [Thread-1  ]: finished collecting timing info
[0m19:51:29.975449 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_24hr
[0m19:51:29.978770 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m19:51:29.979261 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:51:29.979468 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m19:51:40.229928 [debug] [Thread-1  ]: finished collecting timing info
[0m19:51:40.230854 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106db4eb0>]}
[0m19:51:40.231405 [info ] [Thread-1  ]: 24 of 39 OK created model model dbt_anomaly_detection.derived_models_1mon_24hr . [[32mOK[0m in 10.27s]
[0m19:51:40.231961 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_24hr
[0m19:51:40.232218 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_12hr
[0m19:51:40.232522 [info ] [Thread-1  ]: 25 of 39 START model model dbt_anomaly_detection.derived_models_2mon_12hr ...... [RUN]
[0m19:51:40.233672 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_12hr"
[0m19:51:40.233930 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_12hr
[0m19:51:40.234107 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_12hr
[0m19:51:40.241631 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m19:51:40.243445 [debug] [Thread-1  ]: finished collecting timing info
[0m19:51:40.243730 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_12hr
[0m19:51:40.247493 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m19:51:40.248081 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:51:40.248262 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m19:51:55.258563 [debug] [Thread-1  ]: finished collecting timing info
[0m19:51:55.260352 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e82c40>]}
[0m19:51:55.522394 [info ] [Thread-1  ]: 25 of 39 OK created model model dbt_anomaly_detection.derived_models_2mon_12hr . [[32mOK[0m in 15.03s]
[0m19:51:55.523192 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_12hr
[0m19:51:55.523471 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_24hr
[0m19:51:55.523950 [info ] [Thread-1  ]: 26 of 39 START model model dbt_anomaly_detection.derived_models_2mon_24hr ...... [RUN]
[0m19:51:55.524671 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_24hr"
[0m19:51:55.524893 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_24hr
[0m19:51:55.525088 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_24hr
[0m19:51:55.531450 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m19:51:55.532601 [debug] [Thread-1  ]: finished collecting timing info
[0m19:51:55.532796 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_24hr
[0m19:51:55.538021 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m19:51:55.538577 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:51:55.538786 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m19:52:06.457882 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:06.458910 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e6a730>]}
[0m19:52:06.459495 [info ] [Thread-1  ]: 26 of 39 OK created model model dbt_anomaly_detection.derived_models_2mon_24hr . [[32mOK[0m in 10.93s]
[0m19:52:06.460103 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_24hr
[0m19:52:06.460895 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ml_detect
[0m19:52:06.461517 [info ] [Thread-1  ]: 27 of 39 START table model dbt_anomaly_detection.derived_ml_detect ............. [RUN]
[0m19:52:06.462082 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_ml_detect"
[0m19:52:06.462314 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_ml_detect
[0m19:52:06.462505 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_ml_detect
[0m19:52:06.482729 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_ml_detect"
[0m19:52:06.483359 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:06.483492 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_ml_detect
[0m19:52:06.485796 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:52:06.736515 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_ml_detect"
[0m19:52:06.739445 [debug] [Thread-1  ]: On model.anomaly_detection.derived_ml_detect: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_ml_detect"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_ml_detect`
  
  
  OPTIONS()
  as (
    -- 2 periods of training * 2 probabality threshold * 4 aggregation levels 


  WITH test_set AS (
  SELECT
  time_stamps,
        event_count,
        app_event,
        LoB,
        agg_tag
      FROM
        `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`  
      WHERE (agg_tag = "4hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 10 DAY)) 
      OR (agg_tag = "8hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 10 DAY))
      OR (agg_tag = "12hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 15 DAY))
      OR (agg_tag = "24hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 15 DAY))
  )

  
  

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
       

  
  UNION ALL
  



    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
       

  
  ORDER BY
  agg_tag,
  time_stamps,
  app_event,
  LoB,
  prob_threshold,
  training_period
  


  );
  
[0m19:52:10.178869 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:10.179870 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e65910>]}
[0m19:52:10.180446 [info ] [Thread-1  ]: 27 of 39 OK created table model dbt_anomaly_detection.derived_ml_detect ........ [[32mCREATE TABLE (1.5k rows, 487.7 KB processed)[0m in 3.72s]
[0m19:52:10.181026 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ml_detect
[0m19:52:10.181965 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ml_detect_tweaked
[0m19:52:10.182555 [info ] [Thread-1  ]: 28 of 39 START table model dbt_anomaly_detection.ml_detect_tweaked ............. [RUN]
[0m19:52:10.183159 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ml_detect_tweaked"
[0m19:52:10.183362 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ml_detect_tweaked
[0m19:52:10.183554 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ml_detect_tweaked
[0m19:52:10.188665 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ml_detect_tweaked"
[0m19:52:10.189513 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:10.189714 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ml_detect_tweaked
[0m19:52:10.192661 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:52:10.633405 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.ml_detect_tweaked"
[0m19:52:10.634691 [debug] [Thread-1  ]: On model.anomaly_detection.ml_detect_tweaked: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.ml_detect_tweaked"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked`
  
  
  OPTIONS()
  as (
     
  
  with neg_bound_reset as (
SELECT app_event, LoB, agg_tag, time_stamps, prob_threshold, training_period, event_count, 
  IF(lower_bound<0, 2, 0.77*lower_bound) AS lower_bound, 1.3*upper_bound AS upper_bound, anomaly_probability, is_anomaly
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ml_detect` )

SELECT app_event, LoB, agg_tag, time_stamps, prob_threshold, training_period, event_count, 
  lower_bound, upper_bound, anomaly_probability,
  (upper_bound < event_count OR event_count < lower_bound) AS is_anomaly
FROM neg_bound_reset
  );
  
[0m19:52:12.820511 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:12.821736 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106848370>]}
[0m19:52:12.822229 [info ] [Thread-1  ]: 28 of 39 OK created table model dbt_anomaly_detection.ml_detect_tweaked ........ [[32mCREATE TABLE (1.5k rows, 147.0 KB processed)[0m in 2.64s]
[0m19:52:12.822639 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ml_detect_tweaked
[0m19:52:12.823154 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_model_features_dbt
[0m19:52:12.823399 [info ] [Thread-1  ]: 29 of 39 START table model dbt_anomaly_detection.derived_model_features_dbt .... [RUN]
[0m19:52:12.823973 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_model_features_dbt"
[0m19:52:12.824122 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_model_features_dbt
[0m19:52:12.824262 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_model_features_dbt
[0m19:52:12.827932 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_model_features_dbt"
[0m19:52:12.829416 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:12.829564 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_model_features_dbt
[0m19:52:12.834625 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:52:13.175157 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_model_features_dbt"
[0m19:52:13.176378 [debug] [Thread-1  ]: On model.anomaly_detection.derived_model_features_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_model_features_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_model_features_dbt`
  
  
  OPTIONS()
  as (
    
  
  SELECT
    app_event, LoB, 
    CONCAT(agg_tag, '_', prob_threshold, "threshold", '_', RTRIM(LTRIM(training_period, "derived_models_"), CONCAT('_', agg_tag))) AS control_config,
    SUM(CASE WHEN is_anomaly = TRUE THEN 1 ELSE 0 END) AS anomalies,
    SQRT(AVG( POWER(upper_bound - lower_bound, 2) ) ) / AVG(lower_bound) AS RMSD_prcnt,
    SUM(CASE WHEN lower_bound < 0 THEN 1 ELSE 0 END) AS neg_lower
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked` AS all_configs
  GROUP BY
    app_event, 
    LoB,
    control_config
  ORDER BY
    control_config,
    app_event,
    LoB
  );
  
[0m19:52:15.180297 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:15.181160 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107463df0>]}
[0m19:52:15.181681 [info ] [Thread-1  ]: 29 of 39 OK created table model dbt_anomaly_detection.derived_model_features_dbt  [[32mCREATE TABLE (48.0 rows, 113.9 KB processed)[0m in 2.36s]
[0m19:52:15.182218 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_model_features_dbt
[0m19:52:15.183071 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_model_features_dbt
[0m19:52:15.183599 [info ] [Thread-1  ]: 30 of 39 START table model dbt_anomaly_detection.filtered_model_features_dbt ... [RUN]
[0m19:52:15.184207 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_model_features_dbt"
[0m19:52:15.184405 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_model_features_dbt
[0m19:52:15.184599 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_model_features_dbt
[0m19:52:15.189587 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_model_features_dbt"
[0m19:52:15.190371 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:15.190550 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_model_features_dbt
[0m19:52:15.193353 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:52:15.457489 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.filtered_model_features_dbt"
[0m19:52:15.458646 [debug] [Thread-1  ]: On model.anomaly_detection.filtered_model_features_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.filtered_model_features_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
  
  
  OPTIONS()
  as (
    
SELECT features.app_event, features.LoB, control_config, anomalies, RMSD_prcnt, neg_lower
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events` AS non_recent
INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`derived_model_features_dbt` AS features
  ON non_recent.app_event = features.app_event
  AND non_recent.LoB = features.LoB
  );
  
[0m19:52:17.940810 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:17.941151 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d7e4c0>]}
[0m19:52:17.941383 [info ] [Thread-1  ]: 30 of 39 OK created table model dbt_anomaly_detection.filtered_model_features_dbt  [[32mCREATE TABLE (48.0 rows, 3.5 KB processed)[0m in 2.76s]
[0m19:52:17.941628 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_model_features_dbt
[0m19:52:17.942085 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ref_distinct_tuples
[0m19:52:17.942292 [info ] [Thread-1  ]: 31 of 39 START table model dbt_anomaly_detection.ref_distinct_tuples ........... [RUN]
[0m19:52:17.942604 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ref_distinct_tuples"
[0m19:52:17.942701 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ref_distinct_tuples
[0m19:52:17.942799 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ref_distinct_tuples
[0m19:52:17.945510 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ref_distinct_tuples"
[0m19:52:17.946009 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:17.946121 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ref_distinct_tuples
[0m19:52:17.947976 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:52:18.170728 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.ref_distinct_tuples"
[0m19:52:18.172213 [debug] [Thread-1  ]: On model.anomaly_detection.ref_distinct_tuples: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.ref_distinct_tuples"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`ref_distinct_tuples`
  
  
  OPTIONS()
  as (
    
SELECT DISTINCT app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
WHERE app_event LIKE '%add%'
UNION ALL
SELECT DISTINCT app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
WHERE app_event NOT LIKE '%ad%'
ORDER BY app_event
  );
  
[0m19:52:20.644678 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:20.645667 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107479040>]}
[0m19:52:20.646218 [info ] [Thread-1  ]: 31 of 39 OK created table model dbt_anomaly_detection.ref_distinct_tuples ...... [[32mCREATE TABLE (2.0 rows, 720.0 Bytes processed)[0m in 2.70s]
[0m19:52:20.646781 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ref_distinct_tuples
[0m19:52:20.647038 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_features_null_filtered
[0m19:52:20.647308 [info ] [Thread-1  ]: 32 of 39 START table model dbt_anomaly_detection.remaining_events_features_null_filtered  [RUN]
[0m19:52:20.647885 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_features_null_filtered"
[0m19:52:20.648055 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_features_null_filtered
[0m19:52:20.648230 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_features_null_filtered
[0m19:52:20.658917 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_features_null_filtered"
[0m19:52:20.661062 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:20.661225 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_features_null_filtered
[0m19:52:20.663647 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:52:20.935965 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_features_null_filtered"
[0m19:52:20.937798 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_features_null_filtered: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_features_null_filtered"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
where RMSD_prcnt is not null
  );
  
[0m19:52:23.414439 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:23.415507 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106848e80>]}
[0m19:52:23.416059 [info ] [Thread-1  ]: 32 of 39 OK created table model dbt_anomaly_detection.remaining_events_features_null_filtered  [[32mCREATE TABLE (48.0 rows, 3.5 KB processed)[0m in 2.77s]
[0m19:52:23.416615 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_features_null_filtered
[0m19:52:23.417403 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies
[0m19:52:23.417716 [info ] [Thread-1  ]: 33 of 39 START table model dbt_anomaly_detection.remaining_events_min_anomalies  [RUN]
[0m19:52:23.418214 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies"
[0m19:52:23.418387 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies
[0m19:52:23.418557 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies
[0m19:52:23.423236 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies"
[0m19:52:23.424032 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:23.424198 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies
[0m19:52:23.426959 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:52:23.683144 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_anomalies"
[0m19:52:23.684553 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies`
  
  
  OPTIONS()
  as (
    

SELECT app_event, LoB, MIN(anomalies) AS anomalies 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered`
  GROUP BY app_event, LoB
  ORDER BY app_event, LoB
  );
  
[0m19:52:25.847808 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:25.848554 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ea7940>]}
[0m19:52:25.848998 [info ] [Thread-1  ]: 33 of 39 OK created table model dbt_anomaly_detection.remaining_events_min_anomalies  [[32mCREATE TABLE (2.0 rows, 1.4 KB processed)[0m in 2.43s]
[0m19:52:25.849421 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies
[0m19:52:25.850100 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m19:52:25.850374 [info ] [Thread-1  ]: 34 of 39 START table model dbt_anomaly_detection.remaining_events_min_anomalies_results  [RUN]
[0m19:52:25.850813 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m19:52:25.850967 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies_results
[0m19:52:25.851112 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies_results
[0m19:52:25.856682 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m19:52:25.858377 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:25.858541 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies_results
[0m19:52:25.860681 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:52:26.182358 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m19:52:26.183325 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_anomalies_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_anomalies_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results`
  
  
  OPTIONS()
  as (
    

SELECT neg_lower_criteria_res.app_event, neg_lower_criteria_res.LoB, neg_lower_criteria_res.control_config, 
    neg_lower_criteria_res.anomalies, neg_lower_criteria_res.RMSD_prcnt, neg_lower_criteria_res.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered` AS neg_lower_criteria_res
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies` AS min_neg_lower_min_anomalies
    ON neg_lower_criteria_res.anomalies = min_neg_lower_min_anomalies.anomalies
      AND neg_lower_criteria_res.app_event = min_neg_lower_min_anomalies.app_event
      AND neg_lower_criteria_res.LoB = min_neg_lower_min_anomalies.LoB
  ORDER BY neg_lower_criteria_res.app_event, neg_lower_criteria_res.LoB, RMSD_prcnt DESC
  );
  
[0m19:52:28.656546 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:28.658272 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107439190>]}
[0m19:52:28.659073 [info ] [Thread-1  ]: 34 of 39 OK created table model dbt_anomaly_detection.remaining_events_min_anomalies_results  [[32mCREATE TABLE (28.0 rows, 3.5 KB processed)[0m in 2.81s]
[0m19:52:28.659701 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m19:52:28.660563 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD
[0m19:52:28.660997 [info ] [Thread-1  ]: 35 of 39 START table model dbt_anomaly_detection.remaining_events_min_RMSD ..... [RUN]
[0m19:52:28.661535 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD"
[0m19:52:28.661699 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD
[0m19:52:28.661862 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD
[0m19:52:28.668164 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD"
[0m19:52:28.669573 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:28.669763 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD
[0m19:52:28.672943 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:52:28.889595 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_RMSD"
[0m19:52:28.892570 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_RMSD: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_RMSD"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD`
  
  
  OPTIONS()
  as (
    

SELECT app_event, LoB, MIN(RMSD_prcnt) AS RMSD_prcnt 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results`
  GROUP BY app_event, LoB
  ORDER BY app_event, LoB
  );
  
[0m19:52:31.097715 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:31.098542 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106867d60>]}
[0m19:52:31.098965 [info ] [Thread-1  ]: 35 of 39 OK created table model dbt_anomaly_detection.remaining_events_min_RMSD  [[32mCREATE TABLE (2.0 rows, 840.0 Bytes processed)[0m in 2.44s]
[0m19:52:31.099375 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD
[0m19:52:31.100101 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m19:52:31.100518 [info ] [Thread-1  ]: 36 of 39 START table model dbt_anomaly_detection.remaining_events_min_RMSD_results  [RUN]
[0m19:52:31.100973 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m19:52:31.101127 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD_results
[0m19:52:31.101270 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD_results
[0m19:52:31.105534 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m19:52:31.106102 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:31.106237 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD_results
[0m19:52:31.108603 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:52:31.419806 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m19:52:31.423324 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_RMSD_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_RMSD_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
  
  
  OPTIONS()
  as (
    

SELECT anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.LoB, anomalies_criteria_res_dbt.control_config, 
    anomalies_criteria_res_dbt.anomalies, anomalies_criteria_res_dbt.RMSD_prcnt, anomalies_criteria_res_dbt.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results` AS anomalies_criteria_res_dbt
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD` AS min_anomalies_max_RMSD
    ON anomalies_criteria_res_dbt.RMSD_prcnt = min_anomalies_max_RMSD.RMSD_prcnt
      AND anomalies_criteria_res_dbt.app_event = min_anomalies_max_RMSD.app_event
      AND anomalies_criteria_res_dbt.LoB = min_anomalies_max_RMSD.LoB
  ORDER BY anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.LoB, control_config
  );
  
[0m19:52:33.517805 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:33.519054 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e21ac0>]}
[0m19:52:33.519591 [info ] [Thread-1  ]: 36 of 39 OK created table model dbt_anomaly_detection.remaining_events_min_RMSD_results  [[32mCREATE TABLE (2.0 rows, 2.1 KB processed)[0m in 2.42s]
[0m19:52:33.520058 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m19:52:33.520743 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_all_models
[0m19:52:33.521234 [info ] [Thread-1  ]: 37 of 39 START table model dbt_anomaly_detection.filtered_all_models ........... [RUN]
[0m19:52:33.521697 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_all_models"
[0m19:52:33.521838 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_all_models
[0m19:52:33.521978 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_all_models
[0m19:52:33.526213 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_all_models"
[0m19:52:33.526841 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:33.526978 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_all_models
[0m19:52:33.531074 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:52:33.731056 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.filtered_all_models"
[0m19:52:33.733361 [debug] [Thread-1  ]: On model.anomaly_detection.filtered_all_models: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.filtered_all_models"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`filtered_all_models`
  
  
  OPTIONS()
  as (
    
SELECT app_event, LoB, control_config, anomalies, RMSD_prcnt, neg_lower, 
  CASE WHEN (neg_lower = 0 AND anomalies < 5 AND RMSD_prcnt > 0.40 AND RMSD_prcnt < 5.00 ) THEN "ideal_model" ELSE "non_ideal_model" END AS model_quality_tag,
FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
WHERE app_event LIKE '%add%'
UNION ALL 
SELECT app_event, LoB, control_config, anomalies, RMSD_prcnt, neg_lower, 
  CASE WHEN (neg_lower = 0 AND anomalies < 5 AND RMSD_prcnt > 0.40 AND RMSD_prcnt < 5.00 ) THEN "ideal_model" ELSE "non_ideal_model" END AS model_quality_tag,
FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
WHERE app_event NOT LIKE '%ad%'
ORDER BY app_event, LoB
  );
  
[0m19:52:35.905449 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:35.907564 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074af8b0>]}
[0m19:52:35.908128 [info ] [Thread-1  ]: 37 of 39 OK created table model dbt_anomaly_detection.filtered_all_models ...... [[32mCREATE TABLE (2.0 rows, 146.0 Bytes processed)[0m in 2.39s]
[0m19:52:35.908598 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_all_models
[0m19:52:35.909293 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m19:52:35.909634 [info ] [Thread-1  ]: 38 of 39 START table model dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [RUN]
[0m19:52:35.910109 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m19:52:35.910269 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m19:52:35.910412 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m19:52:35.914635 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m19:52:35.915371 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:35.915526 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m19:52:35.917945 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:52:36.217749 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m19:52:36.219511 [debug] [Thread-1  ]: On model.anomaly_detection.derived_alerts_fired_automation_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_alerts_fired_automation_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_alerts_fired_automation_dbt`
  
  
  OPTIONS()
  as (
    
  
  WITH ml_detect_updated AS (
    SELECT app_event, LoB, 
      CONCAT(agg_tag, '_', prob_threshold, "threshold", '_', RTRIM(LTRIM(training_period, "derived_models_"), CONCAT('_', agg_tag))) AS control_config,
      time_stamps, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked`
  )
  SELECT time_stamps, all_configs.app_event AS app_event, all_configs.LoB AS LoB, control_table.control_config AS control_config, 
    anomalies, RMSD_prcnt, neg_lower, model_quality_tag,
    event_count, lower_bound, upper_bound, anomaly_probability, is_anomaly
  FROM ml_detect_updated AS all_configs 
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`filtered_all_models` AS control_table 
    ON all_configs.app_event = control_table.app_event
      AND all_configs.LoB = control_table.LoB
      AND all_configs.control_config = control_table.control_config
  ORDER BY all_configs.app_event, all_configs.LoB, time_stamps
  );
  
[0m19:52:39.154989 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:39.156019 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e650d0>]}
[0m19:52:39.156580 [info ] [Thread-1  ]: 38 of 39 OK created table model dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [[32mCREATE TABLE (55.0 rows, 148.7 KB processed)[0m in 3.25s]
[0m19:52:39.157151 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m19:52:39.158069 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_events_with_anomalies
[0m19:52:39.158629 [info ] [Thread-1  ]: 39 of 39 START table model dbt_anomaly_detection.derived_events_with_anomalies . [RUN]
[0m19:52:39.159234 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_events_with_anomalies"
[0m19:52:39.159438 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_events_with_anomalies
[0m19:52:39.159640 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_events_with_anomalies
[0m19:52:39.166374 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_events_with_anomalies"
[0m19:52:39.167235 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:39.167398 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_events_with_anomalies
[0m19:52:39.170556 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:52:39.373995 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_events_with_anomalies"
[0m19:52:39.375077 [debug] [Thread-1  ]: On model.anomaly_detection.derived_events_with_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_events_with_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_events_with_anomalies`
  
  
  OPTIONS()
  as (
    
SELECT time_stamps, app_event, LoB, event_count AS event_count_anomalous_yesterday
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_alerts_fired_automation_dbt`
WHERE DATE(time_stamps) = CURRENT_DATE() - 1 
  AND is_anomaly
  );
  
[0m19:52:41.620835 [debug] [Thread-1  ]: finished collecting timing info
[0m19:52:41.622494 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '379a7d3b-d58f-46bb-bed5-f538af97ad82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107445fa0>]}
[0m19:52:41.623218 [info ] [Thread-1  ]: 39 of 39 OK created table model dbt_anomaly_detection.derived_events_with_anomalies  [[32mCREATE TABLE (0.0 rows, 2.1 KB processed)[0m in 2.46s]
[0m19:52:41.623932 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_events_with_anomalies
[0m19:52:41.626030 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m19:52:41.626727 [info ] [MainThread]: 
[0m19:52:41.627039 [info ] [MainThread]: Finished running 27 table models, 12 model models in 0 hours 3 minutes and 45.26 seconds (225.26s).
[0m19:52:41.627284 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:52:41.627415 [debug] [MainThread]: Connection 'model.anomaly_detection.derived_events_with_anomalies' was properly closed.
[0m19:52:41.645617 [info ] [MainThread]: 
[0m19:52:41.645928 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:52:41.646298 [info ] [MainThread]: 
[0m19:52:41.646460 [info ] [MainThread]: Done. PASS=39 WARN=0 ERROR=0 SKIP=0 TOTAL=39
[0m19:52:41.646686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074af8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ea1880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074e4820>]}
[0m19:52:41.646862 [debug] [MainThread]: Flushing usage events


============================== 2023-02-10 20:00:51.771553 | 23634629-1ec3-41bb-be7e-f171ea5ee9e0 ==============================
[0m20:00:51.771584 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:00:51.771963 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:00:51.772048 [debug] [MainThread]: Tracking: tracking
[0m20:00:51.779405 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f61d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f75970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f75a60>]}
[0m20:00:51.833547 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m20:00:51.833850 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/filtered_all_models.sql
[0m20:00:51.833974 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_alerts_fired_automation_dbt.sql
[0m20:00:51.842843 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_all_models.sql
[0m20:00:51.848643 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_alerts_fired_automation_dbt.sql
[0m20:00:51.862663 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092c80d0>]}
[0m20:00:51.896841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109207700>]}
[0m20:00:51.897040 [info ] [MainThread]: Found 39 models, 0 tests, 0 snapshots, 0 analyses, 540 macros, 0 operations, 0 seed files, 2 sources, 0 exposures, 0 metrics
[0m20:00:51.897163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109207760>]}
[0m20:00:51.898402 [info ] [MainThread]: 
[0m20:00:51.898657 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m20:00:51.899964 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow"
[0m20:00:51.900143 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:00:52.875625 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow_dbt_anomaly_detection"
[0m20:00:52.876226 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:00:53.210111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ea0190>]}
[0m20:00:53.211400 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:00:53.211949 [info ] [MainThread]: 
[0m20:00:53.216995 [debug] [Thread-1  ]: Began running node model.anomaly_detection.reference_derived
[0m20:00:53.217389 [info ] [Thread-1  ]: 1 of 39 START table model dbt_anomaly_detection.reference_derived .............. [RUN]
[0m20:00:53.217912 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.reference_derived"
[0m20:00:53.218084 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.reference_derived
[0m20:00:53.218308 [debug] [Thread-1  ]: Compiling model.anomaly_detection.reference_derived
[0m20:00:53.223280 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.reference_derived"
[0m20:00:53.223753 [debug] [Thread-1  ]: finished collecting timing info
[0m20:00:53.223850 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.reference_derived
[0m20:00:53.236903 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:00:53.690314 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.reference_derived"
[0m20:00:53.691123 [debug] [Thread-1  ]: On model.anomaly_detection.reference_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.reference_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived`
  
  
  OPTIONS()
  as (
    
SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`web_purchase_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY) AND DATE(collector_tstamp) < CURRENT_DATE()

UNION ALL

SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`pco_web_apply_filter_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY) AND DATE(collector_tstamp) < CURRENT_DATE()
  );
  
[0m20:00:57.702164 [debug] [Thread-1  ]: finished collecting timing info
[0m20:00:57.702820 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109364e20>]}
[0m20:00:57.703200 [info ] [Thread-1  ]: 1 of 39 OK created table model dbt_anomaly_detection.reference_derived ......... [[32mCREATE TABLE (1.1m rows, 34.8 MB processed)[0m in 4.49s]
[0m20:00:57.703588 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.reference_derived
[0m20:00:57.704312 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ref_including_LoBs
[0m20:00:57.704630 [info ] [Thread-1  ]: 2 of 39 START table model dbt_anomaly_detection.derived_ref_including_LoBs ..... [RUN]
[0m20:00:57.704983 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_ref_including_LoBs"
[0m20:00:57.705103 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_ref_including_LoBs
[0m20:00:57.705221 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_ref_including_LoBs
[0m20:00:57.708394 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_ref_including_LoBs"
[0m20:00:57.708981 [debug] [Thread-1  ]: finished collecting timing info
[0m20:00:57.709101 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_ref_including_LoBs
[0m20:00:57.711066 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:00:58.069326 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_ref_including_LoBs"
[0m20:00:58.070799 [debug] [Thread-1  ]: On model.anomaly_detection.derived_ref_including_LoBs: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_ref_including_LoBs"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs`
  
  
  OPTIONS()
  as (
    

SELECT collector_tstamp, event, user_event_name, app_id,
    CASE
    WHEN LOWER(app_id) LIKE '%pcexpress%' OR LOWER(app_id) LIKE '%pcx%' THEN "PCX"
    WHEN LOWER(app_id) LIKE '%sdm%' OR LOWER(app_id) LIKE '%beauty%' THEN "SDM Shop"
    WHEN LOWER(app_id) LIKE '%drx%' THEN "SDM Drx" 
    -- WHEN LOWER(page_urlhost) like '%pcoptimum.ca%' and (LOWER(app_id) like '%ios%' or LOWER(app_id) like '%android%') then 'PC Optimum' 
    -- not recommended to use page_urlhost for mobile apps
    WHEN LOWER(app_id) like '%pco%' THEN 'PC Optimum'
    -- WHEN LOWER(page_urlhost) like '%presidentschoice.ca%' THEN 'PC' -- placeholder for after the launch 
    -- WHEN LOWER(page_urlhost) like '%joefresh.com%' and LOWER(app_id) like '%ios%' then 'JF' -- placeholder for after the launch 
    WHEN LOWER(app_id) like '%jf%' THEN 'JF'
    ELSE app_id END AS LoB,
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived`
  );
  
[0m20:01:03.489817 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:03.491445 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109366100>]}
[0m20:01:03.492172 [info ] [Thread-1  ]: 2 of 39 OK created table model dbt_anomaly_detection.derived_ref_including_LoBs  [[32mCREATE TABLE (1.1m rows, 34.4 MB processed)[0m in 5.79s]
[0m20:01:03.492715 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ref_including_LoBs
[0m20:01:03.493472 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived
[0m20:01:03.493913 [info ] [Thread-1  ]: 3 of 39 START table model dbt_anomaly_detection.all_agg_derived ................ [RUN]
[0m20:01:03.494521 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived"
[0m20:01:03.494711 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived
[0m20:01:03.494877 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived
[0m20:01:03.501714 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived"
[0m20:01:03.502593 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:03.502779 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived
[0m20:01:03.505659 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:01:03.849208 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived"
[0m20:01:03.850316 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
  
  
  OPTIONS()
  as (
    


SELECT "4hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_4hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/4)*4 AS _4hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _4hr_trunc,
    app_id,
    LoB,
    event_type)

UNION ALL


SELECT "8hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_8hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/8)*8 AS _8hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _8hr_trunc,
    app_id,
    LoB,
    event_type)

UNION ALL


SELECT "12hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_12hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/12)*12 AS _12hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _12hr_trunc,
    app_id,
    LoB,
    event_type)

UNION ALL


SELECT "24hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_24hr_trunc AS STRING))) AS time_stamps,
  app_id,
  LoB,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/24)*24 AS _24hr_trunc,
    app_id, 
    LoB,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id, LoB,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ref_including_LoBs` )
  GROUP BY
    date_trunc,
    _24hr_trunc,
    app_id,
    LoB,
    event_type)

ORDER BY
  agg_tag,
  time_stamps,
  app_id,
  LoB,
  event_type


  );
  
[0m20:01:08.518227 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:08.519780 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109313670>]}
[0m20:01:08.520396 [info ] [Thread-1  ]: 3 of 39 OK created table model dbt_anomaly_detection.all_agg_derived ........... [[32mCREATE TABLE (2.1k rows, 41.5 MB processed)[0m in 5.03s]
[0m20:01:08.520903 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived
[0m20:01:08.521747 [debug] [Thread-1  ]: Began running node model.anomaly_detection.count_cutoff
[0m20:01:08.522262 [info ] [Thread-1  ]: 4 of 39 START table model dbt_anomaly_detection.count_cutoff ................... [RUN]
[0m20:01:08.522746 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.count_cutoff"
[0m20:01:08.522892 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.count_cutoff
[0m20:01:08.523034 [debug] [Thread-1  ]: Compiling model.anomaly_detection.count_cutoff
[0m20:01:08.529826 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.count_cutoff"
[0m20:01:08.530611 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:08.530775 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.count_cutoff
[0m20:01:08.534841 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:01:08.742117 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.count_cutoff"
[0m20:01:08.743244 [debug] [Thread-1  ]: On model.anomaly_detection.count_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.count_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, LoB, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select agg_tag, app_event, LoB, min(time_stamps) as strt_time
from pairs
where event_count > 50
group by app_event, LoB, agg_tag 
order by app_event, LoB, agg_tag
  );
  
[0m20:01:10.822516 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:10.822949 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1093c5250>]}
[0m20:01:10.823201 [info ] [Thread-1  ]: 4 of 39 OK created table model dbt_anomaly_detection.count_cutoff .............. [[32mCREATE TABLE (8.0 rows, 92.3 KB processed)[0m in 2.30s]
[0m20:01:10.823461 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.count_cutoff
[0m20:01:10.823791 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff
[0m20:01:10.824053 [info ] [Thread-1  ]: 5 of 39 START table model dbt_anomaly_detection.all_agg_derived_cutoff ......... [RUN]
[0m20:01:10.824363 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff"
[0m20:01:10.824453 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff
[0m20:01:10.824539 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff
[0m20:01:10.827079 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m20:01:10.827464 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:10.827554 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff
[0m20:01:10.829099 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:01:11.096353 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m20:01:11.097634 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, LoB, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select time_stamps, app_event, LoB, agg_tag, event_count
from(
select time_stamps, strt_time, pairs.app_event, pairs.LoB, pairs.agg_tag, event_count
from pairs
inner join `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff` as cutoff
on pairs.agg_tag = cutoff.agg_tag
and pairs.app_event = cutoff.app_event
and pairs.LoB = cutoff.LoB
order by pairs.app_event, pairs.LoB, pairs.agg_tag, time_stamps)
where time_stamps >= strt_time
  );
  
[0m20:01:13.773464 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:13.774532 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109366070>]}
[0m20:01:13.775089 [info ] [Thread-1  ]: 5 of 39 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff .... [[32mCREATE TABLE (2.1k rows, 92.6 KB processed)[0m in 2.95s]
[0m20:01:13.775676 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff
[0m20:01:13.776638 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m20:01:13.777226 [info ] [Thread-1  ]: 6 of 39 START table model dbt_anomaly_detection.all_agg_derived_cutoff_long .... [RUN]
[0m20:01:13.778275 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m20:01:13.778468 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_long
[0m20:01:13.778633 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_long
[0m20:01:13.783119 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m20:01:13.784005 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:13.784214 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_long
[0m20:01:13.787333 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:01:14.010702 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m20:01:14.012036 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB(CURRENT_DATE(), INTERVAL 10 DAY)
  );
  
[0m20:01:16.133986 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:16.135033 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109407eb0>]}
[0m20:01:16.135661 [info ] [Thread-1  ]: 6 of 39 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_long  [[32mCREATE TABLE (1.9k rows, 90.2 KB processed)[0m in 2.36s]
[0m20:01:16.137909 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m20:01:16.138311 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m20:01:16.138990 [info ] [Thread-1  ]: 7 of 39 START table model dbt_anomaly_detection.all_agg_derived_cutoff_short ... [RUN]
[0m20:01:16.141045 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m20:01:16.141743 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_short
[0m20:01:16.142061 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_short
[0m20:01:16.160220 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m20:01:16.161008 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:16.161258 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_short
[0m20:01:16.164492 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:01:16.392832 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m20:01:16.393824 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB(CURRENT_DATE(), INTERVAL 15 DAY)
  );
  
[0m20:01:18.531707 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:18.533661 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f75d30>]}
[0m20:01:18.534387 [info ] [Thread-1  ]: 7 of 39 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_short  [[32mCREATE TABLE (1.8k rows, 90.2 KB processed)[0m in 2.39s]
[0m20:01:18.535103 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m20:01:18.535509 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_nonrecent_events
[0m20:01:18.541611 [info ] [Thread-1  ]: 8 of 39 START table model dbt_anomaly_detection.derived_nonrecent_events ....... [RUN]
[0m20:01:18.542751 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_nonrecent_events"
[0m20:01:18.543110 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_nonrecent_events
[0m20:01:18.543387 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_nonrecent_events
[0m20:01:18.554944 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m20:01:18.558390 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:18.558839 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_nonrecent_events
[0m20:01:18.562037 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:01:18.825233 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m20:01:18.826602 [debug] [Thread-1  ]: On model.anomaly_detection.derived_nonrecent_events: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_nonrecent_events"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events`
  
  
  OPTIONS()
  as (
    

SELECT MIN(time_stamps) AS strt_time, app_event, LoB 
FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
GROUP BY app_event, LoB
HAVING DATE(MIN(time_stamps)) < DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
ORDER BY strt_time
  );
  
[0m20:01:21.368777 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:21.371366 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1093c1d00>]}
[0m20:01:21.371936 [info ] [Thread-1  ]: 8 of 39 OK created table model dbt_anomaly_detection.derived_nonrecent_events .. [[32mCREATE TABLE (2.0 rows, 62.6 KB processed)[0m in 2.83s]
[0m20:01:21.372496 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_nonrecent_events
[0m20:01:21.372740 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_long
[0m20:01:21.373002 [info ] [Thread-1  ]: 9 of 39 START table model dbt_anomaly_detection.aggregation_quartiles_long ..... [RUN]
[0m20:01:21.394675 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_long"
[0m20:01:21.395274 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_long
[0m20:01:21.395843 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_long
[0m20:01:21.401528 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m20:01:21.402977 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:21.403931 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_long
[0m20:01:21.408762 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:01:21.624900 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m20:01:21.626683 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m20:01:23.738197 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:23.738807 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109304eb0>]}
[0m20:01:23.739178 [info ] [Thread-1  ]: 9 of 39 OK created table model dbt_anomaly_detection.aggregation_quartiles_long  [[32mCREATE TABLE (8.0 rows, 53.0 KB processed)[0m in 2.34s]
[0m20:01:23.739584 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_long
[0m20:01:23.739776 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_short
[0m20:01:23.740149 [info ] [Thread-1  ]: 10 of 39 START table model dbt_anomaly_detection.aggregation_quartiles_short ... [RUN]
[0m20:01:23.740524 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_short"
[0m20:01:23.740605 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_short
[0m20:01:23.740675 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_short
[0m20:01:23.745301 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m20:01:23.745825 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:23.745981 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_short
[0m20:01:23.748420 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:01:24.076340 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m20:01:24.077631 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m20:01:41.311709 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:41.313813 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10936cd00>]}
[0m20:01:41.314936 [info ] [Thread-1  ]: 10 of 39 OK created table model dbt_anomaly_detection.aggregation_quartiles_short  [[32mCREATE TABLE (8.0 rows, 49.7 KB processed)[0m in 17.57s]
[0m20:01:41.316604 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_short
[0m20:01:41.317283 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_long
[0m20:01:41.317883 [info ] [Thread-1  ]: 11 of 39 START table model dbt_anomaly_detection.aggregation_bounds_long ....... [RUN]
[0m20:01:41.319110 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_long"
[0m20:01:41.319553 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_long
[0m20:01:41.320056 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_long
[0m20:01:41.329158 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m20:01:41.330844 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:41.331100 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_long
[0m20:01:41.335648 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:01:41.616855 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m20:01:41.619399 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m20:01:43.748796 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:43.751037 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1093404f0>]}
[0m20:01:43.752279 [info ] [Thread-1  ]: 11 of 39 OK created table model dbt_anomaly_detection.aggregation_bounds_long .. [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.43s]
[0m20:01:43.753520 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_long
[0m20:01:43.753957 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_short
[0m20:01:43.754396 [info ] [Thread-1  ]: 12 of 39 START table model dbt_anomaly_detection.aggregation_bounds_short ...... [RUN]
[0m20:01:43.755289 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_short"
[0m20:01:43.755657 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_short
[0m20:01:43.755973 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_short
[0m20:01:43.764764 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m20:01:43.766359 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:43.766703 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_short
[0m20:01:43.776887 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:01:44.216618 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m20:01:44.219086 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m20:01:46.480024 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:46.482166 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10936ba90>]}
[0m20:01:46.483275 [info ] [Thread-1  ]: 12 of 39 OK created table model dbt_anomaly_detection.aggregation_bounds_short . [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.73s]
[0m20:01:46.484223 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_short
[0m20:01:46.484713 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_long
[0m20:01:46.485133 [info ] [Thread-1  ]: 13 of 39 START table model dbt_anomaly_detection.aggregation_outliers_long ..... [RUN]
[0m20:01:46.486008 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_long"
[0m20:01:46.486344 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_long
[0m20:01:46.486649 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_long
[0m20:01:46.499898 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m20:01:46.500820 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:46.501046 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_long
[0m20:01:46.504573 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:01:46.694326 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m20:01:46.695845 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, LoB, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag, LoB,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m20:01:49.244690 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:49.246874 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10931f520>]}
[0m20:01:49.247907 [info ] [Thread-1  ]: 13 of 39 OK created table model dbt_anomaly_detection.aggregation_outliers_long  [[32mCREATE TABLE (1.9k rows, 81.4 KB processed)[0m in 2.76s]
[0m20:01:49.248862 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_long
[0m20:01:49.249412 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_short
[0m20:01:49.249813 [info ] [Thread-1  ]: 14 of 39 START table model dbt_anomaly_detection.aggregation_outliers_short .... [RUN]
[0m20:01:49.250688 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_short"
[0m20:01:49.251007 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_short
[0m20:01:49.251315 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_short
[0m20:01:49.261415 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m20:01:49.263118 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:49.263463 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_short
[0m20:01:49.267513 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:01:49.447274 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m20:01:49.449730 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, LoB, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag, LoB,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m20:01:51.792410 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:51.794652 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1093c1880>]}
[0m20:01:51.795812 [info ] [Thread-1  ]: 14 of 39 OK created table model dbt_anomaly_detection.aggregation_outliers_short  [[32mCREATE TABLE (1.8k rows, 76.3 KB processed)[0m in 2.54s]
[0m20:01:51.797057 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_short
[0m20:01:51.797621 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_4hr
[0m20:01:51.798187 [info ] [Thread-1  ]: 15 of 39 START model model dbt_anomaly_detection.derived_models_05mon_4hr ...... [RUN]
[0m20:01:51.799342 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_4hr"
[0m20:01:51.799779 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_4hr
[0m20:01:51.800200 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_4hr
[0m20:01:51.816749 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m20:01:51.817915 [debug] [Thread-1  ]: finished collecting timing info
[0m20:01:51.818155 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_4hr
[0m20:01:51.840933 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m20:01:51.841484 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:01:51.841686 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:02:03.724872 [debug] [Thread-1  ]: finished collecting timing info
[0m20:02:03.726134 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1094747c0>]}
[0m20:02:03.727057 [info ] [Thread-1  ]: 15 of 39 OK created model model dbt_anomaly_detection.derived_models_05mon_4hr . [[32mOK[0m in 11.93s]
[0m20:02:03.727998 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_4hr
[0m20:02:03.728467 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_8hr
[0m20:02:03.729000 [info ] [Thread-1  ]: 16 of 39 START model model dbt_anomaly_detection.derived_models_05mon_8hr ...... [RUN]
[0m20:02:03.730329 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_8hr"
[0m20:02:03.730984 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_8hr
[0m20:02:03.731383 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_8hr
[0m20:02:03.743698 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m20:02:03.745696 [debug] [Thread-1  ]: finished collecting timing info
[0m20:02:03.745937 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_8hr
[0m20:02:03.750226 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m20:02:03.750906 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:02:03.751167 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:02:14.121199 [debug] [Thread-1  ]: finished collecting timing info
[0m20:02:14.123543 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10930c400>]}
[0m20:02:14.124655 [info ] [Thread-1  ]: 16 of 39 OK created model model dbt_anomaly_detection.derived_models_05mon_8hr . [[32mOK[0m in 10.39s]
[0m20:02:14.125824 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_8hr
[0m20:02:14.126249 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_4hr
[0m20:02:14.126647 [info ] [Thread-1  ]: 17 of 39 START model model dbt_anomaly_detection.derived_models_1mon_4hr ....... [RUN]
[0m20:02:14.128160 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_4hr"
[0m20:02:14.128648 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_4hr
[0m20:02:14.129186 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_4hr
[0m20:02:14.142887 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m20:02:14.143913 [debug] [Thread-1  ]: finished collecting timing info
[0m20:02:14.144134 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_4hr
[0m20:02:14.148876 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m20:02:14.149438 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:02:14.149697 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:02:25.731967 [debug] [Thread-1  ]: finished collecting timing info
[0m20:02:25.734465 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10930fa30>]}
[0m20:02:25.735711 [info ] [Thread-1  ]: 17 of 39 OK created model model dbt_anomaly_detection.derived_models_1mon_4hr .. [[32mOK[0m in 11.61s]
[0m20:02:25.736893 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_4hr
[0m20:02:25.737433 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_8hr
[0m20:02:25.738006 [info ] [Thread-1  ]: 18 of 39 START model model dbt_anomaly_detection.derived_models_1mon_8hr ....... [RUN]
[0m20:02:25.739177 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_8hr"
[0m20:02:25.739635 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_8hr
[0m20:02:25.740072 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_8hr
[0m20:02:25.758504 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m20:02:25.759662 [debug] [Thread-1  ]: finished collecting timing info
[0m20:02:25.759894 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_8hr
[0m20:02:25.764492 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m20:02:25.765022 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:02:25.765283 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:02:36.424675 [debug] [Thread-1  ]: finished collecting timing info
[0m20:02:36.426473 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109316eb0>]}
[0m20:02:36.427360 [info ] [Thread-1  ]: 18 of 39 OK created model model dbt_anomaly_detection.derived_models_1mon_8hr .. [[32mOK[0m in 10.69s]
[0m20:02:36.428789 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_8hr
[0m20:02:36.429264 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_4hr
[0m20:02:36.429681 [info ] [Thread-1  ]: 19 of 39 START model model dbt_anomaly_detection.derived_models_2mon_4hr ....... [RUN]
[0m20:02:36.430606 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_4hr"
[0m20:02:36.430950 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_4hr
[0m20:02:36.431262 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_4hr
[0m20:02:36.442615 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m20:02:36.443554 [debug] [Thread-1  ]: finished collecting timing info
[0m20:02:36.443799 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_4hr
[0m20:02:36.448841 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m20:02:36.449490 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:02:36.449807 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:02:49.264006 [debug] [Thread-1  ]: finished collecting timing info
[0m20:02:49.266412 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109305730>]}
[0m20:02:49.267679 [info ] [Thread-1  ]: 19 of 39 OK created model model dbt_anomaly_detection.derived_models_2mon_4hr .. [[32mOK[0m in 12.84s]
[0m20:02:49.269311 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_4hr
[0m20:02:49.269965 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_8hr
[0m20:02:49.270977 [info ] [Thread-1  ]: 20 of 39 START model model dbt_anomaly_detection.derived_models_2mon_8hr ....... [RUN]
[0m20:02:49.272371 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_8hr"
[0m20:02:49.272860 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_8hr
[0m20:02:49.273414 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_8hr
[0m20:02:49.289892 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m20:02:49.291053 [debug] [Thread-1  ]: finished collecting timing info
[0m20:02:49.291296 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_8hr
[0m20:02:49.298314 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m20:02:49.298884 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:02:49.299150 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:02:59.992431 [debug] [Thread-1  ]: finished collecting timing info
[0m20:02:59.994827 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1094ccfa0>]}
[0m20:02:59.996001 [info ] [Thread-1  ]: 20 of 39 OK created model model dbt_anomaly_detection.derived_models_2mon_8hr .. [[32mOK[0m in 10.72s]
[0m20:02:59.997290 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_8hr
[0m20:02:59.997842 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_12hr
[0m20:02:59.998368 [info ] [Thread-1  ]: 21 of 39 START model model dbt_anomaly_detection.derived_models_05mon_12hr ..... [RUN]
[0m20:02:59.999746 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_12hr"
[0m20:03:00.000086 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_12hr
[0m20:03:00.000395 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_12hr
[0m20:03:00.018186 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m20:03:00.019237 [debug] [Thread-1  ]: finished collecting timing info
[0m20:03:00.019436 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_12hr
[0m20:03:00.023365 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m20:03:00.023917 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:03:00.024164 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:03:11.757365 [debug] [Thread-1  ]: finished collecting timing info
[0m20:03:11.759626 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10944e2b0>]}
[0m20:03:11.760856 [info ] [Thread-1  ]: 21 of 39 OK created model model dbt_anomaly_detection.derived_models_05mon_12hr  [[32mOK[0m in 11.76s]
[0m20:03:11.762083 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_12hr
[0m20:03:11.762786 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_24hr
[0m20:03:11.763369 [info ] [Thread-1  ]: 22 of 39 START model model dbt_anomaly_detection.derived_models_05mon_24hr ..... [RUN]
[0m20:03:11.764674 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_24hr"
[0m20:03:11.765084 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_24hr
[0m20:03:11.765402 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_24hr
[0m20:03:11.781363 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m20:03:11.782598 [debug] [Thread-1  ]: finished collecting timing info
[0m20:03:11.782832 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_24hr
[0m20:03:11.787553 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m20:03:11.788094 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:03:11.788347 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:03:23.207546 [debug] [Thread-1  ]: finished collecting timing info
[0m20:03:23.208586 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092b7670>]}
[0m20:03:23.209141 [info ] [Thread-1  ]: 22 of 39 OK created model model dbt_anomaly_detection.derived_models_05mon_24hr  [[32mOK[0m in 11.44s]
[0m20:03:23.209701 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_24hr
[0m20:03:23.209962 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_12hr
[0m20:03:23.210246 [info ] [Thread-1  ]: 23 of 39 START model model dbt_anomaly_detection.derived_models_1mon_12hr ...... [RUN]
[0m20:03:23.211344 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_12hr"
[0m20:03:23.211740 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_12hr
[0m20:03:23.211958 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_12hr
[0m20:03:23.218064 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m20:03:23.218869 [debug] [Thread-1  ]: finished collecting timing info
[0m20:03:23.219031 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_12hr
[0m20:03:23.225347 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m20:03:23.226289 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:03:23.226516 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:03:49.970429 [debug] [Thread-1  ]: finished collecting timing info
[0m20:03:49.971988 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10925ee50>]}
[0m20:03:49.972650 [info ] [Thread-1  ]: 23 of 39 OK created model model dbt_anomaly_detection.derived_models_1mon_12hr . [[32mOK[0m in 26.76s]
[0m20:03:49.973212 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_12hr
[0m20:03:49.973467 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_24hr
[0m20:03:49.973749 [info ] [Thread-1  ]: 24 of 39 START model model dbt_anomaly_detection.derived_models_1mon_24hr ...... [RUN]
[0m20:03:49.974453 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_24hr"
[0m20:03:49.974653 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_24hr
[0m20:03:49.974831 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_24hr
[0m20:03:49.981118 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m20:03:49.981891 [debug] [Thread-1  ]: finished collecting timing info
[0m20:03:49.982056 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_24hr
[0m20:03:49.985380 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m20:03:49.985950 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:03:49.986176 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:04:03.550631 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:03.551666 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109437910>]}
[0m20:04:03.552261 [info ] [Thread-1  ]: 24 of 39 OK created model model dbt_anomaly_detection.derived_models_1mon_24hr . [[32mOK[0m in 13.58s]
[0m20:04:03.552849 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_24hr
[0m20:04:03.553113 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_12hr
[0m20:04:03.553560 [info ] [Thread-1  ]: 25 of 39 START model model dbt_anomaly_detection.derived_models_2mon_12hr ...... [RUN]
[0m20:04:03.554138 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_12hr"
[0m20:04:03.554340 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_12hr
[0m20:04:03.554531 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_12hr
[0m20:04:03.562657 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m20:04:03.563461 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:03.563623 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_12hr
[0m20:04:03.567264 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m20:04:03.567964 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:04:03.568190 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:04:15.994373 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:15.995522 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10947faf0>]}
[0m20:04:16.206773 [info ] [Thread-1  ]: 25 of 39 OK created model model dbt_anomaly_detection.derived_models_2mon_12hr . [[32mOK[0m in 12.44s]
[0m20:04:16.207520 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_12hr
[0m20:04:16.207763 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_24hr
[0m20:04:16.208396 [info ] [Thread-1  ]: 26 of 39 START model model dbt_anomaly_detection.derived_models_2mon_24hr ...... [RUN]
[0m20:04:16.209157 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_24hr"
[0m20:04:16.209346 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_24hr
[0m20:04:16.209518 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_24hr
[0m20:04:16.217564 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m20:04:16.218483 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:16.218661 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_24hr
[0m20:04:16.224865 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m20:04:16.225345 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:04:16.225526 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'LoB', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  LoB,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:04:31.903967 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:31.904777 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1093244c0>]}
[0m20:04:31.905231 [info ] [Thread-1  ]: 26 of 39 OK created model model dbt_anomaly_detection.derived_models_2mon_24hr . [[32mOK[0m in 15.70s]
[0m20:04:31.905674 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_24hr
[0m20:04:31.906335 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ml_detect
[0m20:04:31.906657 [info ] [Thread-1  ]: 27 of 39 START table model dbt_anomaly_detection.derived_ml_detect ............. [RUN]
[0m20:04:31.907083 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_ml_detect"
[0m20:04:31.907226 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_ml_detect
[0m20:04:31.907366 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_ml_detect
[0m20:04:31.925343 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_ml_detect"
[0m20:04:31.926020 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:31.926178 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_ml_detect
[0m20:04:31.928672 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:04:32.119441 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_ml_detect"
[0m20:04:32.120699 [debug] [Thread-1  ]: On model.anomaly_detection.derived_ml_detect: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_ml_detect"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_ml_detect`
  
  
  OPTIONS()
  as (
    -- 2 periods of training * 2 probabality threshold * 4 aggregation levels 


  WITH test_set AS (
  SELECT
  time_stamps,
        event_count,
        app_event,
        LoB,
        agg_tag
      FROM
        `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`  
      WHERE (agg_tag = "4hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 10 DAY)) 
      OR (agg_tag = "8hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 10 DAY))
      OR (agg_tag = "12hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 15 DAY))
      OR (agg_tag = "24hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 15 DAY))
  )

  
  

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
       

  
  UNION ALL
  



    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, LoB, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
       

  
  ORDER BY
  agg_tag,
  time_stamps,
  app_event,
  LoB,
  prob_threshold,
  training_period
  


  );
  
[0m20:04:35.608864 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:35.609695 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109439a90>]}
[0m20:04:35.610144 [info ] [Thread-1  ]: 27 of 39 OK created table model dbt_anomaly_detection.derived_ml_detect ........ [[32mCREATE TABLE (1.5k rows, 487.7 KB processed)[0m in 3.70s]
[0m20:04:35.610609 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ml_detect
[0m20:04:35.611329 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ml_detect_tweaked
[0m20:04:35.611582 [info ] [Thread-1  ]: 28 of 39 START table model dbt_anomaly_detection.ml_detect_tweaked ............. [RUN]
[0m20:04:35.612178 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ml_detect_tweaked"
[0m20:04:35.612331 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ml_detect_tweaked
[0m20:04:35.612475 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ml_detect_tweaked
[0m20:04:35.616579 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ml_detect_tweaked"
[0m20:04:35.617348 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:35.617482 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ml_detect_tweaked
[0m20:04:35.619882 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:04:35.916959 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.ml_detect_tweaked"
[0m20:04:35.918034 [debug] [Thread-1  ]: On model.anomaly_detection.ml_detect_tweaked: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.ml_detect_tweaked"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked`
  
  
  OPTIONS()
  as (
     
  
  with neg_bound_reset as (
SELECT app_event, LoB, agg_tag, time_stamps, prob_threshold, training_period, event_count, 
  IF(lower_bound<0, 2, 0.77*lower_bound) AS lower_bound, 1.3*upper_bound AS upper_bound, anomaly_probability, is_anomaly
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ml_detect` )

SELECT app_event, LoB, agg_tag, time_stamps, prob_threshold, training_period, event_count, 
  lower_bound, upper_bound, anomaly_probability,
  (upper_bound < event_count OR event_count < lower_bound) AS is_anomaly
FROM neg_bound_reset
  );
  
[0m20:04:38.009686 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:38.010200 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1094874c0>]}
[0m20:04:38.010512 [info ] [Thread-1  ]: 28 of 39 OK created table model dbt_anomaly_detection.ml_detect_tweaked ........ [[32mCREATE TABLE (1.5k rows, 147.0 KB processed)[0m in 2.40s]
[0m20:04:38.010819 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ml_detect_tweaked
[0m20:04:38.011395 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_model_features_dbt
[0m20:04:38.011643 [info ] [Thread-1  ]: 29 of 39 START table model dbt_anomaly_detection.derived_model_features_dbt .... [RUN]
[0m20:04:38.011950 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_model_features_dbt"
[0m20:04:38.012059 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_model_features_dbt
[0m20:04:38.012163 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_model_features_dbt
[0m20:04:38.018764 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_model_features_dbt"
[0m20:04:38.019261 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:38.019372 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_model_features_dbt
[0m20:04:38.021187 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:04:38.241611 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_model_features_dbt"
[0m20:04:38.242486 [debug] [Thread-1  ]: On model.anomaly_detection.derived_model_features_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_model_features_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_model_features_dbt`
  
  
  OPTIONS()
  as (
    
  
  SELECT
    app_event, LoB, 
    CONCAT(agg_tag, '_', prob_threshold, "threshold", '_', RTRIM(LTRIM(training_period, "derived_models_"), CONCAT('_', agg_tag))) AS control_config,
    SUM(CASE WHEN is_anomaly = TRUE THEN 1 ELSE 0 END) AS anomalies,
    SQRT(AVG( POWER(upper_bound - lower_bound, 2) ) ) / AVG(lower_bound) AS RMSD_prcnt,
    SUM(CASE WHEN lower_bound < 0 THEN 1 ELSE 0 END) AS neg_lower
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked` AS all_configs
  GROUP BY
    app_event, 
    LoB,
    control_config
  ORDER BY
    control_config,
    app_event,
    LoB
  );
  
[0m20:04:40.495132 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:40.496100 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10931aca0>]}
[0m20:04:40.496616 [info ] [Thread-1  ]: 29 of 39 OK created table model dbt_anomaly_detection.derived_model_features_dbt  [[32mCREATE TABLE (48.0 rows, 113.9 KB processed)[0m in 2.48s]
[0m20:04:40.497386 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_model_features_dbt
[0m20:04:40.498413 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_model_features_dbt
[0m20:04:40.498820 [info ] [Thread-1  ]: 30 of 39 START table model dbt_anomaly_detection.filtered_model_features_dbt ... [RUN]
[0m20:04:40.499379 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_model_features_dbt"
[0m20:04:40.499558 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_model_features_dbt
[0m20:04:40.499742 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_model_features_dbt
[0m20:04:40.504584 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_model_features_dbt"
[0m20:04:40.505422 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:40.505608 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_model_features_dbt
[0m20:04:40.508606 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:04:40.705961 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.filtered_model_features_dbt"
[0m20:04:40.707817 [debug] [Thread-1  ]: On model.anomaly_detection.filtered_model_features_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.filtered_model_features_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
  
  
  OPTIONS()
  as (
    
SELECT features.app_event, features.LoB, control_config, anomalies, RMSD_prcnt, neg_lower
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events` AS non_recent
INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`derived_model_features_dbt` AS features
  ON non_recent.app_event = features.app_event
  AND non_recent.LoB = features.LoB
  );
  
[0m20:04:43.662909 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:43.663777 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109538dc0>]}
[0m20:04:43.664216 [info ] [Thread-1  ]: 30 of 39 OK created table model dbt_anomaly_detection.filtered_model_features_dbt  [[32mCREATE TABLE (48.0 rows, 3.5 KB processed)[0m in 3.16s]
[0m20:04:43.664651 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_model_features_dbt
[0m20:04:43.665588 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ref_distinct_tuples
[0m20:04:43.665998 [info ] [Thread-1  ]: 31 of 39 START table model dbt_anomaly_detection.ref_distinct_tuples ........... [RUN]
[0m20:04:43.666474 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ref_distinct_tuples"
[0m20:04:43.666608 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ref_distinct_tuples
[0m20:04:43.666735 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ref_distinct_tuples
[0m20:04:43.670559 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ref_distinct_tuples"
[0m20:04:43.671256 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:43.671386 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ref_distinct_tuples
[0m20:04:43.675684 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:04:43.908947 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.ref_distinct_tuples"
[0m20:04:43.909712 [debug] [Thread-1  ]: On model.anomaly_detection.ref_distinct_tuples: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.ref_distinct_tuples"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`ref_distinct_tuples`
  
  
  OPTIONS()
  as (
    
SELECT DISTINCT app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
WHERE app_event LIKE '%add%'
UNION ALL
SELECT DISTINCT app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
WHERE app_event NOT LIKE '%ad%'
ORDER BY app_event
  );
  
[0m20:04:46.073010 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:46.073830 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109523220>]}
[0m20:04:46.074307 [info ] [Thread-1  ]: 31 of 39 OK created table model dbt_anomaly_detection.ref_distinct_tuples ...... [[32mCREATE TABLE (2.0 rows, 720.0 Bytes processed)[0m in 2.41s]
[0m20:04:46.074796 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ref_distinct_tuples
[0m20:04:46.075006 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_features_null_filtered
[0m20:04:46.075401 [info ] [Thread-1  ]: 32 of 39 START table model dbt_anomaly_detection.remaining_events_features_null_filtered  [RUN]
[0m20:04:46.075945 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_features_null_filtered"
[0m20:04:46.076130 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_features_null_filtered
[0m20:04:46.076303 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_features_null_filtered
[0m20:04:46.084324 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_features_null_filtered"
[0m20:04:46.084985 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:46.085124 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_features_null_filtered
[0m20:04:46.087791 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:04:46.399665 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_features_null_filtered"
[0m20:04:46.401753 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_features_null_filtered: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_features_null_filtered"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
where RMSD_prcnt is not null
  );
  
[0m20:04:48.666497 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:48.667243 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109503df0>]}
[0m20:04:48.667735 [info ] [Thread-1  ]: 32 of 39 OK created table model dbt_anomaly_detection.remaining_events_features_null_filtered  [[32mCREATE TABLE (48.0 rows, 3.5 KB processed)[0m in 2.59s]
[0m20:04:48.668233 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_features_null_filtered
[0m20:04:48.668921 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies
[0m20:04:48.669378 [info ] [Thread-1  ]: 33 of 39 START table model dbt_anomaly_detection.remaining_events_min_anomalies  [RUN]
[0m20:04:48.669932 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies"
[0m20:04:48.670101 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies
[0m20:04:48.670261 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies
[0m20:04:48.674327 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies"
[0m20:04:48.674980 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:48.675140 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies
[0m20:04:48.677566 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:04:48.912336 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_anomalies"
[0m20:04:48.914427 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies`
  
  
  OPTIONS()
  as (
    

SELECT app_event, LoB, MIN(anomalies) AS anomalies 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered`
  GROUP BY app_event, LoB
  ORDER BY app_event, LoB
  );
  
[0m20:04:50.862541 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:50.863067 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109517c40>]}
[0m20:04:50.863332 [info ] [Thread-1  ]: 33 of 39 OK created table model dbt_anomaly_detection.remaining_events_min_anomalies  [[32mCREATE TABLE (2.0 rows, 1.4 KB processed)[0m in 2.19s]
[0m20:04:50.863601 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies
[0m20:04:50.863963 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m20:04:50.864140 [info ] [Thread-1  ]: 34 of 39 START table model dbt_anomaly_detection.remaining_events_min_anomalies_results  [RUN]
[0m20:04:50.864426 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m20:04:50.864521 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies_results
[0m20:04:50.864606 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies_results
[0m20:04:50.867455 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m20:04:50.867891 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:50.867993 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies_results
[0m20:04:50.872851 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:04:51.208998 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m20:04:51.210859 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_anomalies_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_anomalies_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results`
  
  
  OPTIONS()
  as (
    

SELECT neg_lower_criteria_res.app_event, neg_lower_criteria_res.LoB, neg_lower_criteria_res.control_config, 
    neg_lower_criteria_res.anomalies, neg_lower_criteria_res.RMSD_prcnt, neg_lower_criteria_res.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered` AS neg_lower_criteria_res
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies` AS min_neg_lower_min_anomalies
    ON neg_lower_criteria_res.anomalies = min_neg_lower_min_anomalies.anomalies
      AND neg_lower_criteria_res.app_event = min_neg_lower_min_anomalies.app_event
      AND neg_lower_criteria_res.LoB = min_neg_lower_min_anomalies.LoB
  ORDER BY neg_lower_criteria_res.app_event, neg_lower_criteria_res.LoB, RMSD_prcnt DESC
  );
  
[0m20:04:53.358428 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:53.360538 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109598c10>]}
[0m20:04:53.361332 [info ] [Thread-1  ]: 34 of 39 OK created table model dbt_anomaly_detection.remaining_events_min_anomalies_results  [[32mCREATE TABLE (30.0 rows, 3.5 KB processed)[0m in 2.50s]
[0m20:04:53.362064 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m20:04:53.363148 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD
[0m20:04:53.363813 [info ] [Thread-1  ]: 35 of 39 START table model dbt_anomaly_detection.remaining_events_min_RMSD ..... [RUN]
[0m20:04:53.364584 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD"
[0m20:04:53.364807 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD
[0m20:04:53.365014 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD
[0m20:04:53.371794 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD"
[0m20:04:53.372722 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:53.372933 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD
[0m20:04:53.376432 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:04:53.568304 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_RMSD"
[0m20:04:53.569546 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_RMSD: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_RMSD"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD`
  
  
  OPTIONS()
  as (
    

SELECT app_event, LoB, MIN(RMSD_prcnt) AS RMSD_prcnt 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results`
  GROUP BY app_event, LoB
  ORDER BY app_event, LoB
  );
  
[0m20:04:55.988436 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:55.989277 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092bd4c0>]}
[0m20:04:55.989817 [info ] [Thread-1  ]: 35 of 39 OK created table model dbt_anomaly_detection.remaining_events_min_RMSD  [[32mCREATE TABLE (2.0 rows, 900.0 Bytes processed)[0m in 2.62s]
[0m20:04:55.990452 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD
[0m20:04:55.991314 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m20:04:55.991780 [info ] [Thread-1  ]: 36 of 39 START table model dbt_anomaly_detection.remaining_events_min_RMSD_results  [RUN]
[0m20:04:55.992388 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m20:04:55.992547 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD_results
[0m20:04:55.992685 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD_results
[0m20:04:55.996960 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m20:04:55.997673 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:55.997861 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD_results
[0m20:04:56.000686 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:04:56.221014 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m20:04:56.222508 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_RMSD_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_RMSD_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
  
  
  OPTIONS()
  as (
    

SELECT anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.LoB, anomalies_criteria_res_dbt.control_config, 
    anomalies_criteria_res_dbt.anomalies, anomalies_criteria_res_dbt.RMSD_prcnt, anomalies_criteria_res_dbt.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results` AS anomalies_criteria_res_dbt
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD` AS min_anomalies_max_RMSD
    ON anomalies_criteria_res_dbt.RMSD_prcnt = min_anomalies_max_RMSD.RMSD_prcnt
      AND anomalies_criteria_res_dbt.app_event = min_anomalies_max_RMSD.app_event
      AND anomalies_criteria_res_dbt.LoB = min_anomalies_max_RMSD.LoB
  ORDER BY anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.LoB, control_config
  );
  
[0m20:04:58.317396 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:58.319069 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109547f40>]}
[0m20:04:58.319816 [info ] [Thread-1  ]: 36 of 39 OK created table model dbt_anomaly_detection.remaining_events_min_RMSD_results  [[32mCREATE TABLE (2.0 rows, 2.2 KB processed)[0m in 2.33s]
[0m20:04:58.320537 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m20:04:58.321731 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_all_models
[0m20:04:58.322084 [info ] [Thread-1  ]: 37 of 39 START table model dbt_anomaly_detection.filtered_all_models ........... [RUN]
[0m20:04:58.322697 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_all_models"
[0m20:04:58.322878 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_all_models
[0m20:04:58.323063 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_all_models
[0m20:04:58.330823 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_all_models"
[0m20:04:58.331521 [debug] [Thread-1  ]: finished collecting timing info
[0m20:04:58.331675 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_all_models
[0m20:04:58.336674 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:04:58.518036 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.filtered_all_models"
[0m20:04:58.519626 [debug] [Thread-1  ]: On model.anomaly_detection.filtered_all_models: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.filtered_all_models"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`filtered_all_models`
  
  
  OPTIONS()
  as (
    
SELECT app_event, LoB, control_config, anomalies, RMSD_prcnt, neg_lower
FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
WHERE app_event LIKE '%add%'
UNION ALL 
SELECT app_event, LoB, control_config, anomalies, RMSD_prcnt, neg_lower
FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
WHERE app_event NOT LIKE '%ad%'
ORDER BY app_event, LoB
  );
  
[0m20:05:01.255922 [debug] [Thread-1  ]: finished collecting timing info
[0m20:05:01.256798 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1094b4610>]}
[0m20:05:01.257252 [info ] [Thread-1  ]: 37 of 39 OK created table model dbt_anomaly_detection.filtered_all_models ...... [[32mCREATE TABLE (2.0 rows, 146.0 Bytes processed)[0m in 2.93s]
[0m20:05:01.257719 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_all_models
[0m20:05:01.258560 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m20:05:01.259175 [info ] [Thread-1  ]: 38 of 39 START table model dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [RUN]
[0m20:05:01.259761 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m20:05:01.259940 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m20:05:01.260107 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m20:05:01.266918 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m20:05:01.267660 [debug] [Thread-1  ]: finished collecting timing info
[0m20:05:01.267827 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m20:05:01.270458 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:05:01.499247 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m20:05:01.501169 [debug] [Thread-1  ]: On model.anomaly_detection.derived_alerts_fired_automation_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_alerts_fired_automation_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_alerts_fired_automation_dbt`
  
  
  OPTIONS()
  as (
    
  
  WITH ml_detect_updated AS (
    SELECT app_event, LoB, 
      CONCAT(agg_tag, '_', prob_threshold, "threshold", '_', RTRIM(LTRIM(training_period, "derived_models_"), CONCAT('_', agg_tag))) AS control_config,
      time_stamps, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked`
  )
  SELECT time_stamps, all_configs.app_event AS app_event, all_configs.LoB AS LoB, control_table.control_config AS control_config, 
    anomalies, RMSD_prcnt, neg_lower, event_count, lower_bound, upper_bound, anomaly_probability, is_anomaly
  FROM ml_detect_updated AS all_configs 
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`filtered_all_models` AS control_table 
    ON all_configs.app_event = control_table.app_event
      AND all_configs.LoB = control_table.LoB
      AND all_configs.control_config = control_table.control_config
  ORDER BY all_configs.app_event, all_configs.LoB, time_stamps
  );
  
[0m20:05:03.854324 [debug] [Thread-1  ]: finished collecting timing info
[0m20:05:03.855401 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109324220>]}
[0m20:05:03.855946 [info ] [Thread-1  ]: 38 of 39 OK created table model dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [[32mCREATE TABLE (55.0 rows, 148.6 KB processed)[0m in 2.60s]
[0m20:05:03.856522 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m20:05:03.857135 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_events_with_anomalies
[0m20:05:03.857817 [info ] [Thread-1  ]: 39 of 39 START table model dbt_anomaly_detection.derived_events_with_anomalies . [RUN]
[0m20:05:03.858392 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_events_with_anomalies"
[0m20:05:03.858583 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_events_with_anomalies
[0m20:05:03.858756 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_events_with_anomalies
[0m20:05:03.863326 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_events_with_anomalies"
[0m20:05:03.865080 [debug] [Thread-1  ]: finished collecting timing info
[0m20:05:03.865257 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_events_with_anomalies
[0m20:05:03.867961 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:05:04.222717 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_events_with_anomalies"
[0m20:05:04.224764 [debug] [Thread-1  ]: On model.anomaly_detection.derived_events_with_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_events_with_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_events_with_anomalies`
  
  
  OPTIONS()
  as (
    
SELECT time_stamps, app_event, LoB, event_count AS event_count_anomalous_yesterday
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_alerts_fired_automation_dbt`
WHERE DATE(time_stamps) = CURRENT_DATE() - 1 
  AND is_anomaly
  );
  
[0m20:05:06.322852 [debug] [Thread-1  ]: finished collecting timing info
[0m20:05:06.323875 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23634629-1ec3-41bb-be7e-f171ea5ee9e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1094e2b20>]}
[0m20:05:06.324432 [info ] [Thread-1  ]: 39 of 39 OK created table model dbt_anomaly_detection.derived_events_with_anomalies  [[32mCREATE TABLE (0.0 rows, 2.1 KB processed)[0m in 2.47s]
[0m20:05:06.324986 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_events_with_anomalies
[0m20:05:06.326752 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m20:05:06.327446 [info ] [MainThread]: 
[0m20:05:06.327780 [info ] [MainThread]: Finished running 27 table models, 12 model models in 0 hours 4 minutes and 14.43 seconds (254.43s).
[0m20:05:06.328032 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:05:06.328160 [debug] [MainThread]: Connection 'model.anomaly_detection.derived_events_with_anomalies' was properly closed.
[0m20:05:06.347227 [info ] [MainThread]: 
[0m20:05:06.347504 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:05:06.347721 [info ] [MainThread]: 
[0m20:05:06.347875 [info ] [MainThread]: Done. PASS=39 WARN=0 ERROR=0 SKIP=0 TOTAL=39
[0m20:05:06.348132 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ea0190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ea0130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f57f10>]}
[0m20:05:06.348343 [debug] [MainThread]: Flushing usage events


============================== 2023-02-10 20:19:08.435840 | 74f3e5cd-dede-47ee-85d0-fb1e54b9ace0 ==============================
[0m20:19:08.435896 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:19:08.436306 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:19:08.436408 [debug] [MainThread]: Tracking: tracking
[0m20:19:08.445463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107190b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107190a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071909a0>]}
[0m20:19:08.498347 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 0 files added, 29 files changed.
[0m20:19:08.499737 [debug] [MainThread]: Partial parsing: deleted file: anomaly_detection://models/example/derived_ref_including_LoBs.sql
[0m20:19:08.499844 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_2mon_8hr.sql
[0m20:19:08.499929 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/aggregation_outliers_long.sql
[0m20:19:08.500002 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/all_agg_derived_cutoff.sql
[0m20:19:08.500076 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_05mon_4hr.sql
[0m20:19:08.500149 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_05mon_24hr.sql
[0m20:19:08.500219 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/filtered_all_models.sql
[0m20:19:08.500292 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/remaining_events_min_anomalies_results.sql
[0m20:19:08.500361 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/count_cutoff.sql
[0m20:19:08.500430 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_2mon_24hr.sql
[0m20:19:08.500497 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_events_with_anomalies.sql
[0m20:19:08.500566 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_2mon_4hr.sql
[0m20:19:08.500635 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/filtered_model_features_dbt.sql
[0m20:19:08.500701 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_1mon_4hr.sql
[0m20:19:08.500768 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_alerts_fired_automation_dbt.sql
[0m20:19:08.500834 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/ml_detect_tweaked.sql
[0m20:19:08.500901 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_1mon_8hr.sql
[0m20:19:08.500969 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/remaining_events_min_RMSD_results.sql
[0m20:19:08.501035 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/remaining_events_min_RMSD.sql
[0m20:19:08.501102 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/remaining_events_min_anomalies.sql
[0m20:19:08.501175 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/aggregation_outliers_short.sql
[0m20:19:08.501242 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_nonrecent_events.sql
[0m20:19:08.501309 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_1mon_12hr.sql
[0m20:19:08.501377 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_05mon_8hr.sql
[0m20:19:08.501444 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_1mon_24hr.sql
[0m20:19:08.501510 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/all_agg_derived.sql
[0m20:19:08.501577 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_model_features_dbt.sql
[0m20:19:08.501643 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_05mon_12hr.sql
[0m20:19:08.501710 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_ml_detect.sql
[0m20:19:08.501775 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_2mon_12hr.sql
[0m20:19:08.509206 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_events_with_anomalies.sql
[0m20:19:08.514913 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_alerts_fired_automation_dbt.sql
[0m20:19:08.516248 [debug] [MainThread]: 1699: static parser successfully parsed example/ref_distinct_tuples.sql
[0m20:19:08.517451 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_all_models.sql
[0m20:19:08.518674 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_RMSD_results.sql
[0m20:19:08.519800 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_RMSD.sql
[0m20:19:08.521290 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies_results.sql
[0m20:19:08.522504 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies.sql
[0m20:19:08.523966 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_features_null_filtered.sql
[0m20:19:08.525078 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_model_features_dbt.sql
[0m20:19:08.526336 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_model_features_dbt.sql
[0m20:19:08.527459 [debug] [MainThread]: 1699: static parser successfully parsed example/ml_detect_tweaked.sql
[0m20:19:08.529049 [debug] [MainThread]: 1603: static parser failed on example/derived_ml_detect.sql
[0m20:19:08.536848 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_ml_detect.sql
[0m20:19:08.537808 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_4hr.sql
[0m20:19:08.540764 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_4hr.sql
[0m20:19:08.541347 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_8hr.sql
[0m20:19:08.543923 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_8hr.sql
[0m20:19:08.544500 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_4hr.sql
[0m20:19:08.547478 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_4hr.sql
[0m20:19:08.548066 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_8hr.sql
[0m20:19:08.550568 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_8hr.sql
[0m20:19:08.551100 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_4hr.sql
[0m20:19:08.554104 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_4hr.sql
[0m20:19:08.554706 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_8hr.sql
[0m20:19:08.557511 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_8hr.sql
[0m20:19:08.558108 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_long.sql
[0m20:19:08.559381 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_bounds_long.sql
[0m20:19:08.560530 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_quartiles_long.sql
[0m20:19:08.561738 [debug] [MainThread]: 1699: static parser successfully parsed example/all_agg_derived_cutoff_long.sql
[0m20:19:08.562848 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_12hr.sql
[0m20:19:08.565709 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_12hr.sql
[0m20:19:08.566269 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_24hr.sql
[0m20:19:08.569717 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_24hr.sql
[0m20:19:08.570283 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_12hr.sql
[0m20:19:08.601540 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_12hr.sql
[0m20:19:08.602235 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_24hr.sql
[0m20:19:08.605200 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_24hr.sql
[0m20:19:08.605809 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_12hr.sql
[0m20:19:08.608633 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_12hr.sql
[0m20:19:08.609219 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_24hr.sql
[0m20:19:08.612068 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_24hr.sql
[0m20:19:08.612636 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_short.sql
[0m20:19:08.613932 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_bounds_short.sql
[0m20:19:08.615164 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_quartiles_short.sql
[0m20:19:08.616280 [debug] [MainThread]: 1699: static parser successfully parsed example/all_agg_derived_cutoff_short.sql
[0m20:19:08.617392 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_nonrecent_events.sql
[0m20:19:08.618487 [debug] [MainThread]: 1699: static parser successfully parsed example/all_agg_derived_cutoff.sql
[0m20:19:08.619715 [debug] [MainThread]: 1699: static parser successfully parsed example/count_cutoff.sql
[0m20:19:08.620973 [debug] [MainThread]: 1603: static parser failed on example/all_agg_derived.sql
[0m20:19:08.623818 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/all_agg_derived.sql
[0m20:19:08.647373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075e60d0>]}
[0m20:19:08.655135 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075178b0>]}
[0m20:19:08.655316 [info ] [MainThread]: Found 38 models, 0 tests, 0 snapshots, 0 analyses, 540 macros, 0 operations, 0 seed files, 2 sources, 0 exposures, 0 metrics
[0m20:19:08.655444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10750e700>]}
[0m20:19:08.656662 [info ] [MainThread]: 
[0m20:19:08.656893 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m20:19:08.658171 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow"
[0m20:19:08.658353 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:19:10.057845 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow_dbt_anomaly_detection"
[0m20:19:10.058427 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:19:10.383088 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107548730>]}
[0m20:19:10.384199 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:19:10.384586 [info ] [MainThread]: 
[0m20:19:10.391341 [debug] [Thread-1  ]: Began running node model.anomaly_detection.reference_derived
[0m20:19:10.391870 [info ] [Thread-1  ]: 1 of 38 START table model dbt_anomaly_detection.reference_derived .............. [RUN]
[0m20:19:10.392379 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.reference_derived"
[0m20:19:10.392539 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.reference_derived
[0m20:19:10.392764 [debug] [Thread-1  ]: Compiling model.anomaly_detection.reference_derived
[0m20:19:10.397479 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.reference_derived"
[0m20:19:10.398189 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:10.398329 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.reference_derived
[0m20:19:10.414084 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:19:10.665057 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.reference_derived"
[0m20:19:10.666117 [debug] [Thread-1  ]: On model.anomaly_detection.reference_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.reference_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived`
  
  
  OPTIONS()
  as (
    
SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`web_purchase_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY) AND DATE(collector_tstamp) < CURRENT_DATE()

UNION ALL

SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`pco_web_apply_filter_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY) AND DATE(collector_tstamp) < CURRENT_DATE()
  );
  
[0m20:19:14.103480 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:14.104174 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075e79d0>]}
[0m20:19:14.104572 [info ] [Thread-1  ]: 1 of 38 OK created table model dbt_anomaly_detection.reference_derived ......... [[32mCREATE TABLE (1.1m rows, 34.8 MB processed)[0m in 3.71s]
[0m20:19:14.104956 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.reference_derived
[0m20:19:14.105682 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived
[0m20:19:14.105934 [info ] [Thread-1  ]: 2 of 38 START table model dbt_anomaly_detection.all_agg_derived ................ [RUN]
[0m20:19:14.106322 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived"
[0m20:19:14.106458 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived
[0m20:19:14.106584 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived
[0m20:19:14.111267 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived"
[0m20:19:14.111981 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:14.112093 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived
[0m20:19:14.114030 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:19:14.381624 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived"
[0m20:19:14.383003 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
  
  
  OPTIONS()
  as (
    


SELECT "4hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_4hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/4)*4 AS _4hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _4hr_trunc,
    app_id,
    event_type)

UNION ALL


SELECT "8hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_8hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/8)*8 AS _8hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _8hr_trunc,
    app_id,
    event_type)

UNION ALL


SELECT "12hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_12hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/12)*12 AS _12hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _12hr_trunc,
    app_id,
    event_type)

UNION ALL


SELECT "24hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_24hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/24)*24 AS _24hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _24hr_trunc,
    app_id,
    event_type)

ORDER BY
  agg_tag,
  time_stamps,
  app_id,
  event_type


  );
  
[0m20:19:18.771307 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:18.772137 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107741d00>]}
[0m20:19:18.772629 [info ] [Thread-1  ]: 2 of 38 OK created table model dbt_anomaly_detection.all_agg_derived ........... [[32mCREATE TABLE (2.1k rows, 34.4 MB processed)[0m in 4.67s]
[0m20:19:18.773103 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived
[0m20:19:18.773873 [debug] [Thread-1  ]: Began running node model.anomaly_detection.count_cutoff
[0m20:19:18.774370 [info ] [Thread-1  ]: 3 of 38 START table model dbt_anomaly_detection.count_cutoff ................... [RUN]
[0m20:19:18.774814 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.count_cutoff"
[0m20:19:18.774962 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.count_cutoff
[0m20:19:18.775099 [debug] [Thread-1  ]: Compiling model.anomaly_detection.count_cutoff
[0m20:19:18.779245 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.count_cutoff"
[0m20:19:18.779843 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:18.779998 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.count_cutoff
[0m20:19:18.782247 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:19:19.160028 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.count_cutoff"
[0m20:19:19.161839 [debug] [Thread-1  ]: On model.anomaly_detection.count_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.count_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select agg_tag, app_event, min(time_stamps) as strt_time
from pairs
where event_count > 50
group by app_event, agg_tag 
order by app_event, agg_tag
  );
  
[0m20:19:22.469366 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:22.470298 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10887e520>]}
[0m20:19:22.470882 [info ] [Thread-1  ]: 3 of 38 OK created table model dbt_anomaly_detection.count_cutoff .............. [[32mCREATE TABLE (8.0 rows, 77.7 KB processed)[0m in 3.70s]
[0m20:19:22.471431 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.count_cutoff
[0m20:19:22.472297 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff
[0m20:19:22.472800 [info ] [Thread-1  ]: 4 of 38 START table model dbt_anomaly_detection.all_agg_derived_cutoff ......... [RUN]
[0m20:19:22.473298 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff"
[0m20:19:22.473459 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff
[0m20:19:22.473620 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff
[0m20:19:22.480699 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m20:19:22.481619 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:22.481790 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff
[0m20:19:22.484423 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:19:22.767331 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m20:19:22.768538 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select time_stamps, app_event, agg_tag, event_count
from(
select time_stamps, strt_time, pairs.app_event, pairs.agg_tag, event_count
from pairs
inner join `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff` as cutoff
on pairs.agg_tag = cutoff.agg_tag
and pairs.app_event = cutoff.app_event
order by pairs.app_event, pairs.agg_tag, time_stamps)
where time_stamps >= strt_time
  );
  
[0m20:19:25.620341 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:25.621220 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108858070>]}
[0m20:19:25.621653 [info ] [Thread-1  ]: 4 of 38 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff .... [[32mCREATE TABLE (2.1k rows, 77.9 KB processed)[0m in 3.15s]
[0m20:19:25.622070 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff
[0m20:19:25.622870 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m20:19:25.623290 [info ] [Thread-1  ]: 5 of 38 START table model dbt_anomaly_detection.all_agg_derived_cutoff_long .... [RUN]
[0m20:19:25.623696 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m20:19:25.623821 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_long
[0m20:19:25.623953 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_long
[0m20:19:25.627764 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m20:19:25.628431 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:25.628564 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_long
[0m20:19:25.630644 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:19:25.914111 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m20:19:25.915376 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB(CURRENT_DATE(), INTERVAL 10 DAY)
  );
  
[0m20:19:28.410366 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:28.412106 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108823f70>]}
[0m20:19:28.412855 [info ] [Thread-1  ]: 5 of 38 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_long  [[32mCREATE TABLE (1.9k rows, 75.6 KB processed)[0m in 2.79s]
[0m20:19:28.413610 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m20:19:28.413904 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m20:19:28.414188 [info ] [Thread-1  ]: 6 of 38 START table model dbt_anomaly_detection.all_agg_derived_cutoff_short ... [RUN]
[0m20:19:28.415224 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m20:19:28.415460 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_short
[0m20:19:28.415644 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_short
[0m20:19:28.420445 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m20:19:28.421303 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:28.421481 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_short
[0m20:19:28.426883 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:19:28.686988 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m20:19:28.688305 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB(CURRENT_DATE(), INTERVAL 15 DAY)
  );
  
[0m20:19:31.122469 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:31.124026 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108823220>]}
[0m20:19:31.124607 [info ] [Thread-1  ]: 6 of 38 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_short  [[32mCREATE TABLE (1.8k rows, 75.6 KB processed)[0m in 2.71s]
[0m20:19:31.125078 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m20:19:31.125286 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_nonrecent_events
[0m20:19:31.125871 [info ] [Thread-1  ]: 7 of 38 START table model dbt_anomaly_detection.derived_nonrecent_events ....... [RUN]
[0m20:19:31.126375 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_nonrecent_events"
[0m20:19:31.126529 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_nonrecent_events
[0m20:19:31.126686 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_nonrecent_events
[0m20:19:31.130599 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m20:19:31.131428 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:31.131575 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_nonrecent_events
[0m20:19:31.133947 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:19:31.558401 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m20:19:31.559256 [debug] [Thread-1  ]: On model.anomaly_detection.derived_nonrecent_events: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_nonrecent_events"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events`
  
  
  OPTIONS()
  as (
    

SELECT MIN(time_stamps) AS strt_time, app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
GROUP BY app_event
HAVING DATE(MIN(time_stamps)) < DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
ORDER BY strt_time
  );
  
[0m20:19:33.837746 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:33.838674 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107741f40>]}
[0m20:19:33.839133 [info ] [Thread-1  ]: 7 of 38 OK created table model dbt_anomaly_detection.derived_nonrecent_events .. [[32mCREATE TABLE (2.0 rows, 48.0 KB processed)[0m in 2.71s]
[0m20:19:33.839614 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_nonrecent_events
[0m20:19:33.839841 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_long
[0m20:19:33.840253 [info ] [Thread-1  ]: 8 of 38 START table model dbt_anomaly_detection.aggregation_quartiles_long ..... [RUN]
[0m20:19:33.840785 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_long"
[0m20:19:33.840961 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_long
[0m20:19:33.841134 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_long
[0m20:19:33.847337 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m20:19:33.848103 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:33.848267 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_long
[0m20:19:33.850988 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:19:34.102417 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m20:19:34.105033 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m20:19:36.597627 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:36.599150 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108864310>]}
[0m20:19:36.599930 [info ] [Thread-1  ]: 8 of 38 OK created table model dbt_anomaly_detection.aggregation_quartiles_long  [[32mCREATE TABLE (8.0 rows, 53.0 KB processed)[0m in 2.76s]
[0m20:19:36.600524 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_long
[0m20:19:36.600747 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_short
[0m20:19:36.600990 [info ] [Thread-1  ]: 9 of 38 START table model dbt_anomaly_detection.aggregation_quartiles_short .... [RUN]
[0m20:19:36.601691 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_short"
[0m20:19:36.602068 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_short
[0m20:19:36.602290 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_short
[0m20:19:36.609378 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m20:19:36.610051 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:36.610212 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_short
[0m20:19:36.614568 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:19:36.871823 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m20:19:36.873332 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m20:19:39.080811 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:39.081658 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10881b820>]}
[0m20:19:39.082142 [info ] [Thread-1  ]: 9 of 38 OK created table model dbt_anomaly_detection.aggregation_quartiles_short  [[32mCREATE TABLE (8.0 rows, 49.7 KB processed)[0m in 2.48s]
[0m20:19:39.082579 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_short
[0m20:19:39.082772 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_long
[0m20:19:39.083277 [info ] [Thread-1  ]: 10 of 38 START table model dbt_anomaly_detection.aggregation_bounds_long ....... [RUN]
[0m20:19:39.084076 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_long"
[0m20:19:39.084258 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_long
[0m20:19:39.084422 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_long
[0m20:19:39.088492 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m20:19:39.089165 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:39.089309 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_long
[0m20:19:39.091626 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:19:39.347826 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m20:19:39.349128 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m20:19:41.546761 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:41.547363 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107753fd0>]}
[0m20:19:41.547713 [info ] [Thread-1  ]: 10 of 38 OK created table model dbt_anomaly_detection.aggregation_bounds_long .. [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.46s]
[0m20:19:41.548085 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_long
[0m20:19:41.548266 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_short
[0m20:19:41.548585 [info ] [Thread-1  ]: 11 of 38 START table model dbt_anomaly_detection.aggregation_bounds_short ...... [RUN]
[0m20:19:41.549171 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_short"
[0m20:19:41.549350 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_short
[0m20:19:41.549544 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_short
[0m20:19:41.554216 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m20:19:41.554770 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:41.554908 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_short
[0m20:19:41.557143 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:19:41.838275 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m20:19:41.839403 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m20:19:45.392064 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:45.392779 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10881b850>]}
[0m20:19:45.393113 [info ] [Thread-1  ]: 11 of 38 OK created table model dbt_anomaly_detection.aggregation_bounds_short . [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 3.84s]
[0m20:19:45.393421 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_short
[0m20:19:45.393553 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_long
[0m20:19:45.393804 [info ] [Thread-1  ]: 12 of 38 START table model dbt_anomaly_detection.aggregation_outliers_long ..... [RUN]
[0m20:19:45.394666 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_long"
[0m20:19:45.394870 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_long
[0m20:19:45.395043 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_long
[0m20:19:45.403437 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m20:19:45.404163 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:45.404459 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_long
[0m20:19:45.406439 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:19:45.687093 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m20:19:45.689357 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m20:19:47.879730 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:47.881302 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10779bbe0>]}
[0m20:19:47.882057 [info ] [Thread-1  ]: 12 of 38 OK created table model dbt_anomaly_detection.aggregation_outliers_long  [[32mCREATE TABLE (1.9k rows, 68.3 KB processed)[0m in 2.49s]
[0m20:19:47.882672 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_long
[0m20:19:47.882955 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_short
[0m20:19:47.883927 [info ] [Thread-1  ]: 13 of 38 START table model dbt_anomaly_detection.aggregation_outliers_short .... [RUN]
[0m20:19:47.884764 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_short"
[0m20:19:47.884956 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_short
[0m20:19:47.885129 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_short
[0m20:19:47.889973 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m20:19:47.891787 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:47.892081 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_short
[0m20:19:47.895073 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:19:48.275918 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m20:19:48.277065 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m20:19:50.752024 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:50.752739 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077b33a0>]}
[0m20:19:50.753077 [info ] [Thread-1  ]: 13 of 38 OK created table model dbt_anomaly_detection.aggregation_outliers_short  [[32mCREATE TABLE (1.8k rows, 64.0 KB processed)[0m in 2.87s]
[0m20:19:50.753390 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_short
[0m20:19:50.753526 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_4hr
[0m20:19:50.753651 [info ] [Thread-1  ]: 14 of 38 START model model dbt_anomaly_detection.derived_models_05mon_4hr ...... [RUN]
[0m20:19:50.753897 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_4hr"
[0m20:19:50.754293 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_4hr
[0m20:19:50.754896 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_4hr
[0m20:19:50.759246 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m20:19:50.759690 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:50.759793 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_4hr
[0m20:19:50.776462 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m20:19:50.777011 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:19:50.777180 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:20:01.518058 [debug] [Thread-1  ]: finished collecting timing info
[0m20:20:01.519064 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10885e820>]}
[0m20:20:01.519646 [info ] [Thread-1  ]: 14 of 38 OK created model model dbt_anomaly_detection.derived_models_05mon_4hr . [[32mOK[0m in 10.77s]
[0m20:20:01.520222 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_4hr
[0m20:20:01.520494 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_8hr
[0m20:20:01.520922 [info ] [Thread-1  ]: 15 of 38 START model model dbt_anomaly_detection.derived_models_05mon_8hr ...... [RUN]
[0m20:20:01.521519 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_8hr"
[0m20:20:01.521709 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_8hr
[0m20:20:01.521880 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_8hr
[0m20:20:01.529370 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m20:20:01.530290 [debug] [Thread-1  ]: finished collecting timing info
[0m20:20:01.530461 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_8hr
[0m20:20:01.533786 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m20:20:01.534598 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:20:01.534816 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:20:12.249364 [debug] [Thread-1  ]: finished collecting timing info
[0m20:20:12.251162 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1088b6610>]}
[0m20:20:12.251810 [info ] [Thread-1  ]: 15 of 38 OK created model model dbt_anomaly_detection.derived_models_05mon_8hr . [[32mOK[0m in 10.73s]
[0m20:20:12.252348 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_8hr
[0m20:20:12.252584 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_4hr
[0m20:20:12.252832 [info ] [Thread-1  ]: 16 of 38 START model model dbt_anomaly_detection.derived_models_1mon_4hr ....... [RUN]
[0m20:20:12.253576 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_4hr"
[0m20:20:12.253802 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_4hr
[0m20:20:12.253983 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_4hr
[0m20:20:12.261361 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m20:20:12.263138 [debug] [Thread-1  ]: finished collecting timing info
[0m20:20:12.263352 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_4hr
[0m20:20:12.266601 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m20:20:12.267102 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:20:12.267281 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:20:22.935695 [debug] [Thread-1  ]: finished collecting timing info
[0m20:20:22.936636 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108852d90>]}
[0m20:20:22.937203 [info ] [Thread-1  ]: 16 of 38 OK created model model dbt_anomaly_detection.derived_models_1mon_4hr .. [[32mOK[0m in 10.68s]
[0m20:20:22.937777 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_4hr
[0m20:20:22.938039 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_8hr
[0m20:20:22.938337 [info ] [Thread-1  ]: 17 of 38 START model model dbt_anomaly_detection.derived_models_1mon_8hr ....... [RUN]
[0m20:20:22.939404 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_8hr"
[0m20:20:22.939717 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_8hr
[0m20:20:22.939895 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_8hr
[0m20:20:22.947666 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m20:20:22.949460 [debug] [Thread-1  ]: finished collecting timing info
[0m20:20:22.949753 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_8hr
[0m20:20:22.955305 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m20:20:22.955803 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:20:22.956013 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:20:33.491462 [debug] [Thread-1  ]: finished collecting timing info
[0m20:20:33.492788 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1088ad6a0>]}
[0m20:20:33.493411 [info ] [Thread-1  ]: 17 of 38 OK created model model dbt_anomaly_detection.derived_models_1mon_8hr .. [[32mOK[0m in 10.55s]
[0m20:20:33.494009 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_8hr
[0m20:20:33.494285 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_4hr
[0m20:20:33.494930 [info ] [Thread-1  ]: 18 of 38 START model model dbt_anomaly_detection.derived_models_2mon_4hr ....... [RUN]
[0m20:20:33.495818 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_4hr"
[0m20:20:33.496007 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_4hr
[0m20:20:33.496182 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_4hr
[0m20:20:33.503622 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m20:20:33.504333 [debug] [Thread-1  ]: finished collecting timing info
[0m20:20:33.504483 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_4hr
[0m20:20:33.508242 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m20:20:33.509018 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:20:33.509266 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:20:45.136986 [debug] [Thread-1  ]: finished collecting timing info
[0m20:20:45.138842 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107753730>]}
[0m20:20:45.139647 [info ] [Thread-1  ]: 18 of 38 OK created model model dbt_anomaly_detection.derived_models_2mon_4hr .. [[32mOK[0m in 11.64s]
[0m20:20:45.140219 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_4hr
[0m20:20:45.140452 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_8hr
[0m20:20:45.140879 [info ] [Thread-1  ]: 19 of 38 START model model dbt_anomaly_detection.derived_models_2mon_8hr ....... [RUN]
[0m20:20:45.141533 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_8hr"
[0m20:20:45.141722 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_8hr
[0m20:20:45.141903 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_8hr
[0m20:20:45.149212 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m20:20:45.149962 [debug] [Thread-1  ]: finished collecting timing info
[0m20:20:45.150129 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_8hr
[0m20:20:45.153356 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m20:20:45.154082 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:20:45.154309 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:20:55.934968 [debug] [Thread-1  ]: finished collecting timing info
[0m20:20:55.937058 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10774a250>]}
[0m20:20:55.937676 [info ] [Thread-1  ]: 19 of 38 OK created model model dbt_anomaly_detection.derived_models_2mon_8hr .. [[32mOK[0m in 10.80s]
[0m20:20:55.938143 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_8hr
[0m20:20:55.938368 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_12hr
[0m20:20:55.938573 [info ] [Thread-1  ]: 20 of 38 START model model dbt_anomaly_detection.derived_models_05mon_12hr ..... [RUN]
[0m20:20:55.939244 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_12hr"
[0m20:20:55.939470 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_12hr
[0m20:20:55.939632 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_12hr
[0m20:20:55.946714 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m20:20:55.947232 [debug] [Thread-1  ]: finished collecting timing info
[0m20:20:55.947369 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_12hr
[0m20:20:55.950390 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m20:20:55.950710 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:20:55.950854 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:21:06.134088 [debug] [Thread-1  ]: finished collecting timing info
[0m20:21:06.135082 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10880e220>]}
[0m20:21:06.135626 [info ] [Thread-1  ]: 20 of 38 OK created model model dbt_anomaly_detection.derived_models_05mon_12hr  [[32mOK[0m in 10.20s]
[0m20:21:06.136190 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_12hr
[0m20:21:06.136447 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_24hr
[0m20:21:06.136946 [info ] [Thread-1  ]: 21 of 38 START model model dbt_anomaly_detection.derived_models_05mon_24hr ..... [RUN]
[0m20:21:06.137588 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_24hr"
[0m20:21:06.137807 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_24hr
[0m20:21:06.138008 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_24hr
[0m20:21:06.144380 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m20:21:06.145186 [debug] [Thread-1  ]: finished collecting timing info
[0m20:21:06.145355 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_24hr
[0m20:21:06.148697 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m20:21:06.149228 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:21:06.149460 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:21:16.708006 [debug] [Thread-1  ]: finished collecting timing info
[0m20:21:16.709151 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108860e50>]}
[0m20:21:16.709851 [info ] [Thread-1  ]: 21 of 38 OK created model model dbt_anomaly_detection.derived_models_05mon_24hr  [[32mOK[0m in 10.57s]
[0m20:21:16.710531 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_24hr
[0m20:21:16.710790 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_12hr
[0m20:21:16.711272 [info ] [Thread-1  ]: 22 of 38 START model model dbt_anomaly_detection.derived_models_1mon_12hr ...... [RUN]
[0m20:21:16.711955 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_12hr"
[0m20:21:16.712183 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_12hr
[0m20:21:16.712380 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_12hr
[0m20:21:16.720350 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m20:21:16.721307 [debug] [Thread-1  ]: finished collecting timing info
[0m20:21:16.721471 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_12hr
[0m20:21:16.724660 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m20:21:16.725066 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:21:16.725244 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:21:26.894396 [debug] [Thread-1  ]: finished collecting timing info
[0m20:21:26.896277 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c66f40>]}
[0m20:21:26.897052 [info ] [Thread-1  ]: 22 of 38 OK created model model dbt_anomaly_detection.derived_models_1mon_12hr . [[32mOK[0m in 10.18s]
[0m20:21:26.897661 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_12hr
[0m20:21:26.897936 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_24hr
[0m20:21:26.898404 [info ] [Thread-1  ]: 23 of 38 START model model dbt_anomaly_detection.derived_models_1mon_24hr ...... [RUN]
[0m20:21:26.899089 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_24hr"
[0m20:21:26.899336 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_24hr
[0m20:21:26.899541 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_24hr
[0m20:21:26.910190 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m20:21:26.912316 [debug] [Thread-1  ]: finished collecting timing info
[0m20:21:26.912495 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_24hr
[0m20:21:26.917094 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m20:21:26.917802 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:21:26.918026 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:21:37.564754 [debug] [Thread-1  ]: finished collecting timing info
[0m20:21:37.565696 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107779dc0>]}
[0m20:21:37.566173 [info ] [Thread-1  ]: 23 of 38 OK created model model dbt_anomaly_detection.derived_models_1mon_24hr . [[32mOK[0m in 10.67s]
[0m20:21:37.566639 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_24hr
[0m20:21:37.566835 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_12hr
[0m20:21:37.567414 [info ] [Thread-1  ]: 24 of 38 START model model dbt_anomaly_detection.derived_models_2mon_12hr ...... [RUN]
[0m20:21:37.568116 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_12hr"
[0m20:21:37.568277 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_12hr
[0m20:21:37.568431 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_12hr
[0m20:21:37.574752 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m20:21:37.575471 [debug] [Thread-1  ]: finished collecting timing info
[0m20:21:37.575609 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_12hr
[0m20:21:37.578569 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m20:21:37.579223 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:21:37.579429 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:21:47.415550 [debug] [Thread-1  ]: finished collecting timing info
[0m20:21:47.416437 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10776ecd0>]}
[0m20:21:47.416897 [info ] [Thread-1  ]: 24 of 38 OK created model model dbt_anomaly_detection.derived_models_2mon_12hr . [[32mOK[0m in 9.85s]
[0m20:21:47.417231 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_12hr
[0m20:21:47.417383 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_24hr
[0m20:21:47.417522 [info ] [Thread-1  ]: 25 of 38 START model model dbt_anomaly_detection.derived_models_2mon_24hr ...... [RUN]
[0m20:21:47.417875 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_24hr"
[0m20:21:47.418047 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_24hr
[0m20:21:47.418188 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_24hr
[0m20:21:47.422439 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m20:21:47.422918 [debug] [Thread-1  ]: finished collecting timing info
[0m20:21:47.423041 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_24hr
[0m20:21:47.425290 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m20:21:47.425667 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:21:47.425880 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:21:58.299948 [debug] [Thread-1  ]: finished collecting timing info
[0m20:21:58.301247 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c4d130>]}
[0m20:21:58.573058 [info ] [Thread-1  ]: 25 of 38 OK created model model dbt_anomaly_detection.derived_models_2mon_24hr . [[32mOK[0m in 10.88s]
[0m20:21:58.573890 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_24hr
[0m20:21:58.574796 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ml_detect
[0m20:21:58.575243 [info ] [Thread-1  ]: 26 of 38 START table model dbt_anomaly_detection.derived_ml_detect ............. [RUN]
[0m20:21:58.575894 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_ml_detect"
[0m20:21:58.576101 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_ml_detect
[0m20:21:58.576311 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_ml_detect
[0m20:21:58.594745 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_ml_detect"
[0m20:21:58.596313 [debug] [Thread-1  ]: finished collecting timing info
[0m20:21:58.596467 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_ml_detect
[0m20:21:58.598694 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:21:58.836555 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_ml_detect"
[0m20:21:58.838402 [debug] [Thread-1  ]: On model.anomaly_detection.derived_ml_detect: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_ml_detect"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_ml_detect`
  
  
  OPTIONS()
  as (
    -- 2 periods of training * 2 probabality threshold * 4 aggregation levels 


  WITH test_set AS (
  SELECT
  time_stamps,
        event_count,
        app_event,
        agg_tag
      FROM
        `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`  
      WHERE (agg_tag = "4hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 10 DAY)) 
      OR (agg_tag = "8hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 10 DAY))
      OR (agg_tag = "12hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 15 DAY))
      OR (agg_tag = "24hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 15 DAY))
  )

  
  

    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
       

  
  UNION ALL
  



    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
       

  
  ORDER BY
  agg_tag,
  time_stamps,
  app_event,
  prob_threshold,
  training_period
  


  );
  
[0m20:22:02.153811 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:02.154785 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10774a850>]}
[0m20:22:02.155307 [info ] [Thread-1  ]: 26 of 38 OK created table model dbt_anomaly_detection.derived_ml_detect ........ [[32mCREATE TABLE (1.5k rows, 422.5 KB processed)[0m in 3.58s]
[0m20:22:02.155789 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ml_detect
[0m20:22:02.156385 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ml_detect_tweaked
[0m20:22:02.156941 [info ] [Thread-1  ]: 27 of 38 START table model dbt_anomaly_detection.ml_detect_tweaked ............. [RUN]
[0m20:22:02.157475 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ml_detect_tweaked"
[0m20:22:02.157653 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ml_detect_tweaked
[0m20:22:02.157816 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ml_detect_tweaked
[0m20:22:02.162154 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ml_detect_tweaked"
[0m20:22:02.163034 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:02.163208 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ml_detect_tweaked
[0m20:22:02.165928 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:22:02.567654 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.ml_detect_tweaked"
[0m20:22:02.568694 [debug] [Thread-1  ]: On model.anomaly_detection.ml_detect_tweaked: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.ml_detect_tweaked"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked`
  
  
  OPTIONS()
  as (
     
  
  with neg_bound_reset as (
SELECT app_event, agg_tag, time_stamps, prob_threshold, training_period, event_count, 
  IF(lower_bound<0, 2, 0.77*lower_bound) AS lower_bound, 1.3*upper_bound AS upper_bound, anomaly_probability, is_anomaly
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ml_detect` )

SELECT app_event, agg_tag, time_stamps, prob_threshold, training_period, event_count, 
  lower_bound, upper_bound, anomaly_probability,
  (upper_bound < event_count OR event_count < lower_bound) AS is_anomaly
FROM neg_bound_reset
  );
  
[0m20:22:04.819063 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:04.819852 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1088b9a00>]}
[0m20:22:04.820273 [info ] [Thread-1  ]: 27 of 38 OK created table model dbt_anomaly_detection.ml_detect_tweaked ........ [[32mCREATE TABLE (1.5k rows, 137.0 KB processed)[0m in 2.66s]
[0m20:22:04.820709 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ml_detect_tweaked
[0m20:22:04.821479 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_model_features_dbt
[0m20:22:04.821938 [info ] [Thread-1  ]: 28 of 38 START table model dbt_anomaly_detection.derived_model_features_dbt .... [RUN]
[0m20:22:04.822390 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_model_features_dbt"
[0m20:22:04.822524 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_model_features_dbt
[0m20:22:04.822649 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_model_features_dbt
[0m20:22:04.826461 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_model_features_dbt"
[0m20:22:04.827065 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:04.827201 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_model_features_dbt
[0m20:22:04.831211 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:22:05.117917 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_model_features_dbt"
[0m20:22:05.118399 [debug] [Thread-1  ]: On model.anomaly_detection.derived_model_features_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_model_features_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_model_features_dbt`
  
  
  OPTIONS()
  as (
    
  
  SELECT
    app_event,
    CONCAT(agg_tag, '_', prob_threshold, "threshold", '_', RTRIM(LTRIM(training_period, "derived_models_"), CONCAT('_', agg_tag))) AS control_config,
    SUM(CASE WHEN is_anomaly = TRUE THEN 1 ELSE 0 END) AS anomalies,
    SQRT(AVG( POWER(upper_bound - lower_bound, 2) ) ) / AVG(lower_bound) AS RMSD_prcnt,
    SUM(CASE WHEN lower_bound < 0 THEN 1 ELSE 0 END) AS neg_lower
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked` AS all_configs
  GROUP BY
    app_event,
    control_config
  ORDER BY
    control_config,
    app_event
  );
  
[0m20:22:07.970983 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:07.972724 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c54eb0>]}
[0m20:22:07.973509 [info ] [Thread-1  ]: 28 of 38 OK created table model dbt_anomaly_detection.derived_model_features_dbt  [[32mCREATE TABLE (48.0 rows, 103.8 KB processed)[0m in 3.15s]
[0m20:22:07.974207 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_model_features_dbt
[0m20:22:07.975473 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_model_features_dbt
[0m20:22:07.975971 [info ] [Thread-1  ]: 29 of 38 START table model dbt_anomaly_detection.filtered_model_features_dbt ... [RUN]
[0m20:22:07.976567 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_model_features_dbt"
[0m20:22:07.976761 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_model_features_dbt
[0m20:22:07.976932 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_model_features_dbt
[0m20:22:07.986343 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_model_features_dbt"
[0m20:22:07.987163 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:07.987326 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_model_features_dbt
[0m20:22:07.989697 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:22:08.213544 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.filtered_model_features_dbt"
[0m20:22:08.214823 [debug] [Thread-1  ]: On model.anomaly_detection.filtered_model_features_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.filtered_model_features_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
  
  
  OPTIONS()
  as (
    
SELECT features.app_event, control_config, anomalies, RMSD_prcnt, neg_lower
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events` AS non_recent
INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`derived_model_features_dbt` AS features
  ON non_recent.app_event = features.app_event
  );
  
[0m20:22:11.115874 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:11.117004 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10883f7c0>]}
[0m20:22:11.117498 [info ] [Thread-1  ]: 29 of 38 OK created table model dbt_anomaly_detection.filtered_model_features_dbt  [[32mCREATE TABLE (48.0 rows, 3.2 KB processed)[0m in 3.14s]
[0m20:22:11.117935 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_model_features_dbt
[0m20:22:11.118660 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ref_distinct_tuples
[0m20:22:11.118881 [info ] [Thread-1  ]: 30 of 38 START table model dbt_anomaly_detection.ref_distinct_tuples ........... [RUN]
[0m20:22:11.119284 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ref_distinct_tuples"
[0m20:22:11.119430 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ref_distinct_tuples
[0m20:22:11.119570 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ref_distinct_tuples
[0m20:22:11.123731 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ref_distinct_tuples"
[0m20:22:11.124579 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:11.124713 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ref_distinct_tuples
[0m20:22:11.127060 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:22:11.474352 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.ref_distinct_tuples"
[0m20:22:11.476574 [debug] [Thread-1  ]: On model.anomaly_detection.ref_distinct_tuples: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.ref_distinct_tuples"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`ref_distinct_tuples`
  
  
  OPTIONS()
  as (
    
SELECT DISTINCT app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
WHERE app_event LIKE '%add%'
UNION ALL
SELECT DISTINCT app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
WHERE app_event NOT LIKE '%ad%'
ORDER BY app_event
  );
  
[0m20:22:13.874805 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:13.875847 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1088b8df0>]}
[0m20:22:13.876359 [info ] [Thread-1  ]: 30 of 38 OK created table model dbt_anomaly_detection.ref_distinct_tuples ...... [[32mCREATE TABLE (2.0 rows, 720.0 Bytes processed)[0m in 2.76s]
[0m20:22:13.877005 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ref_distinct_tuples
[0m20:22:13.877476 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_features_null_filtered
[0m20:22:13.878182 [info ] [Thread-1  ]: 31 of 38 START table model dbt_anomaly_detection.remaining_events_features_null_filtered  [RUN]
[0m20:22:13.879002 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_features_null_filtered"
[0m20:22:13.879227 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_features_null_filtered
[0m20:22:13.879418 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_features_null_filtered
[0m20:22:13.883905 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_features_null_filtered"
[0m20:22:13.884553 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:13.884719 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_features_null_filtered
[0m20:22:13.892682 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:22:14.263003 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_features_null_filtered"
[0m20:22:14.264144 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_features_null_filtered: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_features_null_filtered"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
where RMSD_prcnt is not null
  );
  
[0m20:22:16.364573 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:16.366292 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ceb910>]}
[0m20:22:16.367104 [info ] [Thread-1  ]: 31 of 38 OK created table model dbt_anomaly_detection.remaining_events_features_null_filtered  [[32mCREATE TABLE (48.0 rows, 3.1 KB processed)[0m in 2.49s]
[0m20:22:16.367842 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_features_null_filtered
[0m20:22:16.369056 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies
[0m20:22:16.369513 [info ] [Thread-1  ]: 32 of 38 START table model dbt_anomaly_detection.remaining_events_min_anomalies  [RUN]
[0m20:22:16.370055 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies"
[0m20:22:16.370224 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies
[0m20:22:16.370387 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies
[0m20:22:16.376394 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies"
[0m20:22:16.378340 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:16.378629 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies
[0m20:22:16.381728 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:22:16.621126 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_anomalies"
[0m20:22:16.622548 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies`
  
  
  OPTIONS()
  as (
    

SELECT app_event, MIN(anomalies) AS anomalies 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered`
  GROUP BY app_event
  ORDER BY app_event
  );
  
[0m20:22:18.775199 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:18.775967 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108897fd0>]}
[0m20:22:18.776407 [info ] [Thread-1  ]: 32 of 38 OK created table model dbt_anomaly_detection.remaining_events_min_anomalies  [[32mCREATE TABLE (2.0 rows, 1.1 KB processed)[0m in 2.41s]
[0m20:22:18.776852 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies
[0m20:22:18.777577 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m20:22:18.777807 [info ] [Thread-1  ]: 33 of 38 START table model dbt_anomaly_detection.remaining_events_min_anomalies_results  [RUN]
[0m20:22:18.778189 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m20:22:18.778329 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies_results
[0m20:22:18.778472 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies_results
[0m20:22:18.782493 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m20:22:18.784173 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:18.784349 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies_results
[0m20:22:18.786416 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:22:19.118888 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m20:22:19.121321 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_anomalies_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_anomalies_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results`
  
  
  OPTIONS()
  as (
    

SELECT neg_lower_criteria_res.app_event, neg_lower_criteria_res.control_config, 
    neg_lower_criteria_res.anomalies, neg_lower_criteria_res.RMSD_prcnt, neg_lower_criteria_res.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered` AS neg_lower_criteria_res
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies` AS min_neg_lower_min_anomalies
    ON neg_lower_criteria_res.anomalies = min_neg_lower_min_anomalies.anomalies
      AND neg_lower_criteria_res.app_event = min_neg_lower_min_anomalies.app_event
  ORDER BY neg_lower_criteria_res.app_event, RMSD_prcnt DESC
  );
  
[0m20:22:21.595196 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:21.596680 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108855ac0>]}
[0m20:22:21.597413 [info ] [Thread-1  ]: 33 of 38 OK created table model dbt_anomaly_detection.remaining_events_min_anomalies_results  [[32mCREATE TABLE (30.0 rows, 3.2 KB processed)[0m in 2.82s]
[0m20:22:21.598014 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m20:22:21.598710 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD
[0m20:22:21.599165 [info ] [Thread-1  ]: 34 of 38 START table model dbt_anomaly_detection.remaining_events_min_RMSD ..... [RUN]
[0m20:22:21.599723 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD"
[0m20:22:21.599903 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD
[0m20:22:21.600075 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD
[0m20:22:21.607756 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD"
[0m20:22:21.608534 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:21.608700 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD
[0m20:22:21.612776 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:22:21.829437 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_RMSD"
[0m20:22:21.830042 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_RMSD: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_RMSD"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD`
  
  
  OPTIONS()
  as (
    

SELECT app_event, MIN(RMSD_prcnt) AS RMSD_prcnt 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results`
  GROUP BY app_event
  ORDER BY app_event
  );
  
[0m20:22:23.881820 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:23.883427 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108cf6160>]}
[0m20:22:23.884286 [info ] [Thread-1  ]: 34 of 38 OK created table model dbt_anomaly_detection.remaining_events_min_RMSD  [[32mCREATE TABLE (2.0 rows, 690.0 Bytes processed)[0m in 2.28s]
[0m20:22:23.885104 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD
[0m20:22:23.886154 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m20:22:23.886740 [info ] [Thread-1  ]: 35 of 38 START table model dbt_anomaly_detection.remaining_events_min_RMSD_results  [RUN]
[0m20:22:23.887334 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m20:22:23.887506 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD_results
[0m20:22:23.887670 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD_results
[0m20:22:23.894542 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m20:22:23.895279 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:23.895437 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD_results
[0m20:22:23.898353 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:22:24.173410 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m20:22:24.174740 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_RMSD_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_RMSD_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
  
  
  OPTIONS()
  as (
    

SELECT anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.control_config, 
    anomalies_criteria_res_dbt.anomalies, anomalies_criteria_res_dbt.RMSD_prcnt, anomalies_criteria_res_dbt.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results` AS anomalies_criteria_res_dbt
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD` AS min_anomalies_max_RMSD
    ON anomalies_criteria_res_dbt.RMSD_prcnt = min_anomalies_max_RMSD.RMSD_prcnt
      AND anomalies_criteria_res_dbt.app_event = min_anomalies_max_RMSD.app_event
  ORDER BY anomalies_criteria_res_dbt.app_event, control_config
  );
  
[0m20:23:57.483431 [debug] [Thread-1  ]: finished collecting timing info
[0m20:23:57.487965 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c18880>]}
[0m20:23:57.488874 [info ] [Thread-1  ]: 35 of 38 OK created table model dbt_anomaly_detection.remaining_events_min_RMSD_results  [[32mCREATE TABLE (2.0 rows, 2.0 KB processed)[0m in 93.60s]
[0m20:23:57.489590 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m20:23:57.491238 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_all_models
[0m20:23:57.491858 [info ] [Thread-1  ]: 36 of 38 START table model dbt_anomaly_detection.filtered_all_models ........... [RUN]
[0m20:23:57.492569 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_all_models"
[0m20:23:57.492914 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_all_models
[0m20:23:57.493376 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_all_models
[0m20:23:57.504269 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_all_models"
[0m20:23:57.505336 [debug] [Thread-1  ]: finished collecting timing info
[0m20:23:57.505517 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_all_models
[0m20:23:57.508585 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:23:57.891579 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.filtered_all_models"
[0m20:23:57.894928 [debug] [Thread-1  ]: On model.anomaly_detection.filtered_all_models: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.filtered_all_models"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`filtered_all_models`
  
  
  OPTIONS()
  as (
    
SELECT app_event, control_config, anomalies, RMSD_prcnt, neg_lower
FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
WHERE app_event LIKE '%add%'
UNION ALL 
SELECT app_event, control_config, anomalies, RMSD_prcnt, neg_lower
FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
WHERE app_event NOT LIKE '%ad%'
ORDER BY app_event
  );
  
[0m20:24:00.155628 [debug] [Thread-1  ]: finished collecting timing info
[0m20:24:00.157793 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108d23f10>]}
[0m20:24:00.159896 [info ] [Thread-1  ]: 36 of 38 OK created table model dbt_anomaly_detection.filtered_all_models ...... [[32mCREATE TABLE (2.0 rows, 132.0 Bytes processed)[0m in 2.67s]
[0m20:24:00.161055 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_all_models
[0m20:24:00.163560 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m20:24:00.163981 [info ] [Thread-1  ]: 37 of 38 START table model dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [RUN]
[0m20:24:00.164744 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m20:24:00.165141 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m20:24:00.165345 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m20:24:00.180337 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m20:24:00.181462 [debug] [Thread-1  ]: finished collecting timing info
[0m20:24:00.181677 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m20:24:00.183505 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:24:00.443522 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m20:24:00.445502 [debug] [Thread-1  ]: On model.anomaly_detection.derived_alerts_fired_automation_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_alerts_fired_automation_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_alerts_fired_automation_dbt`
  
  
  OPTIONS()
  as (
    
  
  WITH ml_detect_updated AS (
    SELECT app_event, 
      CONCAT(agg_tag, '_', prob_threshold, "threshold", '_', RTRIM(LTRIM(training_period, "derived_models_"), CONCAT('_', agg_tag))) AS control_config,
      time_stamps, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked`
  )
  SELECT time_stamps, all_configs.app_event AS app_event, control_table.control_config AS control_config, 
    anomalies, RMSD_prcnt, neg_lower, event_count, lower_bound, upper_bound, anomaly_probability, is_anomaly
  FROM ml_detect_updated AS all_configs 
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`filtered_all_models` AS control_table 
    ON all_configs.app_event = control_table.app_event
      AND all_configs.control_config = control_table.control_config
  ORDER BY all_configs.app_event, time_stamps
  );
  
[0m20:24:02.534204 [debug] [Thread-1  ]: finished collecting timing info
[0m20:24:02.534660 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10885f5b0>]}
[0m20:24:02.534914 [info ] [Thread-1  ]: 37 of 38 OK created table model dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [[32mCREATE TABLE (55.0 rows, 138.5 KB processed)[0m in 2.37s]
[0m20:24:02.535212 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m20:24:02.535650 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_events_with_anomalies
[0m20:24:02.535909 [info ] [Thread-1  ]: 38 of 38 START table model dbt_anomaly_detection.derived_events_with_anomalies . [RUN]
[0m20:24:02.536215 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_events_with_anomalies"
[0m20:24:02.536320 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_events_with_anomalies
[0m20:24:02.536417 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_events_with_anomalies
[0m20:24:02.538997 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_events_with_anomalies"
[0m20:24:02.539463 [debug] [Thread-1  ]: finished collecting timing info
[0m20:24:02.539572 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_events_with_anomalies
[0m20:24:02.541564 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:24:02.727225 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_events_with_anomalies"
[0m20:24:02.730094 [debug] [Thread-1  ]: On model.anomaly_detection.derived_events_with_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_events_with_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_events_with_anomalies`
  
  
  OPTIONS()
  as (
    
SELECT time_stamps, app_event, event_count AS event_count_anomalous_yesterday
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_alerts_fired_automation_dbt`
WHERE DATE(time_stamps) = CURRENT_DATE() - 1 
  AND is_anomaly
  );
  
[0m20:24:04.735921 [debug] [Thread-1  ]: finished collecting timing info
[0m20:24:04.736847 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74f3e5cd-dede-47ee-85d0-fb1e54b9ace0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c60fa0>]}
[0m20:24:04.737379 [info ] [Thread-1  ]: 38 of 38 OK created table model dbt_anomaly_detection.derived_events_with_anomalies  [[32mCREATE TABLE (0.0 rows, 1.7 KB processed)[0m in 2.20s]
[0m20:24:04.737906 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_events_with_anomalies
[0m20:24:04.739480 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m20:24:04.740219 [info ] [MainThread]: 
[0m20:24:04.740598 [info ] [MainThread]: Finished running 26 table models, 12 model models in 0 hours 4 minutes and 56.08 seconds (296.08s).
[0m20:24:04.740886 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:24:04.741027 [debug] [MainThread]: Connection 'model.anomaly_detection.derived_events_with_anomalies' was properly closed.
[0m20:24:04.760937 [info ] [MainThread]: 
[0m20:24:04.761188 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:24:04.761403 [info ] [MainThread]: 
[0m20:24:04.761621 [info ] [MainThread]: Done. PASS=38 WARN=0 ERROR=0 SKIP=0 TOTAL=38
[0m20:24:04.761882 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10748c370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108cbdeb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108d3fee0>]}
[0m20:24:04.762066 [debug] [MainThread]: Flushing usage events


============================== 2023-02-10 20:28:19.054610 | 43c11122-1d24-43a3-a74e-824fb75edd3c ==============================
[0m20:28:19.054645 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:28:19.054996 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:28:19.055077 [debug] [MainThread]: Tracking: tracking
[0m20:28:19.062479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10594cc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10594cca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10594c730>]}
[0m20:28:19.112050 [debug] [MainThread]: Partial parsing enabled: 39 files deleted, 0 files added, 0 files changed.
[0m20:28:19.120493 [debug] [MainThread]: 1603: static parser failed on example/all_agg_derived.sql
[0m20:28:19.127735 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/all_agg_derived.sql
[0m20:28:19.128500 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_model_features_dbt.sql
[0m20:28:19.129795 [debug] [MainThread]: 1699: static parser successfully parsed example/count_cutoff.sql
[0m20:28:19.131231 [debug] [MainThread]: 1603: static parser failed on example/derived_ml_detect.sql
[0m20:28:19.138548 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_ml_detect.sql
[0m20:28:19.139164 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_12hr.sql
[0m20:28:19.142401 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_12hr.sql
[0m20:28:19.143100 [debug] [MainThread]: 1699: static parser successfully parsed example/all_agg_derived_cutoff_short.sql
[0m20:28:19.144407 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_4hr.sql
[0m20:28:19.147257 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_4hr.sql
[0m20:28:19.147811 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_quartiles_long.sql
[0m20:28:19.149075 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_model_features_dbt.sql
[0m20:28:19.150318 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_8hr.sql
[0m20:28:19.153688 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_8hr.sql
[0m20:28:19.154287 [debug] [MainThread]: 1699: static parser successfully parsed example/reference_derived.sql
[0m20:28:19.155390 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_RMSD_results.sql
[0m20:28:19.156611 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_events_with_anomalies.sql
[0m20:28:19.157931 [debug] [MainThread]: 1699: static parser successfully parsed example/filtered_all_models.sql
[0m20:28:19.159298 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_long.sql
[0m20:28:19.160439 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_24hr.sql
[0m20:28:19.163316 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_24hr.sql
[0m20:28:19.163873 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_bounds_short.sql
[0m20:28:19.165121 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_4hr.sql
[0m20:28:19.167945 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_4hr.sql
[0m20:28:19.168474 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_nonrecent_events.sql
[0m20:28:19.169744 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_12hr.sql
[0m20:28:19.172544 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_12hr.sql
[0m20:28:19.173143 [debug] [MainThread]: 1699: static parser successfully parsed example/ref_distinct_tuples.sql
[0m20:28:19.174566 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies.sql
[0m20:28:19.175667 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_anomalies_results.sql
[0m20:28:19.177017 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_12hr.sql
[0m20:28:19.179917 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_12hr.sql
[0m20:28:19.180492 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_bounds_long.sql
[0m20:28:19.181594 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_24hr.sql
[0m20:28:19.213455 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_24hr.sql
[0m20:28:19.214199 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_min_RMSD.sql
[0m20:28:19.215324 [debug] [MainThread]: 1699: static parser successfully parsed example/all_agg_derived_cutoff.sql
[0m20:28:19.216443 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_8hr.sql
[0m20:28:19.219444 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_8hr.sql
[0m20:28:19.220084 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_24hr.sql
[0m20:28:19.222967 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_24hr.sql
[0m20:28:19.223512 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_quartiles_short.sql
[0m20:28:19.224604 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_4hr.sql
[0m20:28:19.227476 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_4hr.sql
[0m20:28:19.228012 [debug] [MainThread]: 1699: static parser successfully parsed example/ml_detect_tweaked.sql
[0m20:28:19.229350 [debug] [MainThread]: 1699: static parser successfully parsed example/remaining_events_features_null_filtered.sql
[0m20:28:19.230698 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_alerts_fired_automation_dbt.sql
[0m20:28:19.231961 [debug] [MainThread]: 1699: static parser successfully parsed example/all_agg_derived_cutoff_long.sql
[0m20:28:19.233108 [debug] [MainThread]: 1699: static parser successfully parsed example/aggregation_outliers_short.sql
[0m20:28:19.234311 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_8hr.sql
[0m20:28:19.237148 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_8hr.sql
[0m20:28:19.246626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058f5670>]}
[0m20:28:19.254248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c02160>]}
[0m20:28:19.254415 [info ] [MainThread]: Found 38 models, 0 tests, 0 snapshots, 0 analyses, 540 macros, 0 operations, 0 seed files, 2 sources, 0 exposures, 0 metrics
[0m20:28:19.254539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c021f0>]}
[0m20:28:19.255700 [info ] [MainThread]: 
[0m20:28:19.255945 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m20:28:19.257277 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow"
[0m20:28:19.257424 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:28:20.247193 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow_dbt_anomaly_detection"
[0m20:28:20.247879 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:28:20.479076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b70070>]}
[0m20:28:20.479751 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:28:20.479999 [info ] [MainThread]: 
[0m20:28:20.484231 [debug] [Thread-1  ]: Began running node model.anomaly_detection.reference_derived
[0m20:28:20.484510 [info ] [Thread-1  ]: 1 of 38 START table model dbt_anomaly_detection.reference_derived .............. [RUN]
[0m20:28:20.484884 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.reference_derived"
[0m20:28:20.484986 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.reference_derived
[0m20:28:20.485171 [debug] [Thread-1  ]: Compiling model.anomaly_detection.reference_derived
[0m20:28:20.491182 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.reference_derived"
[0m20:28:20.491921 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:20.492138 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.reference_derived
[0m20:28:20.504659 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:28:20.885507 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.reference_derived"
[0m20:28:20.886452 [debug] [Thread-1  ]: On model.anomaly_detection.reference_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.reference_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived`
  
  
  OPTIONS()
  as (
    
SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`web_purchase_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY) AND DATE(collector_tstamp) < CURRENT_DATE()

UNION ALL

SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`pco_web_apply_filter_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY) AND DATE(collector_tstamp) < CURRENT_DATE()
  );
  
[0m20:28:24.773988 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:24.774335 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d08970>]}
[0m20:28:24.774531 [info ] [Thread-1  ]: 1 of 38 OK created table model dbt_anomaly_detection.reference_derived ......... [[32mCREATE TABLE (1.1m rows, 34.8 MB processed)[0m in 4.29s]
[0m20:28:24.774731 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.reference_derived
[0m20:28:24.775010 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived
[0m20:28:24.775531 [info ] [Thread-1  ]: 2 of 38 START table model dbt_anomaly_detection.all_agg_derived ................ [RUN]
[0m20:28:24.776167 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived"
[0m20:28:24.776290 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived
[0m20:28:24.776358 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived
[0m20:28:24.778915 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived"
[0m20:28:24.779270 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:24.779355 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived
[0m20:28:24.780638 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:28:25.039601 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived"
[0m20:28:25.040760 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
  
  
  OPTIONS()
  as (
    


SELECT "4hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_4hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/4)*4 AS _4hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _4hr_trunc,
    app_id,
    event_type)

UNION ALL


SELECT "8hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_8hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/8)*8 AS _8hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _8hr_trunc,
    app_id,
    event_type)

UNION ALL


SELECT "12hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_12hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/12)*12 AS _12hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _12hr_trunc,
    app_id,
    event_type)

UNION ALL


SELECT "24hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_24hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/24)*24 AS _24hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _24hr_trunc,
    app_id,
    event_type)

ORDER BY
  agg_tag,
  time_stamps,
  app_id,
  event_type


  );
  
[0m20:28:29.637512 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:29.641227 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d59c70>]}
[0m20:28:29.642406 [info ] [Thread-1  ]: 2 of 38 OK created table model dbt_anomaly_detection.all_agg_derived ........... [[32mCREATE TABLE (2.1k rows, 34.4 MB processed)[0m in 4.86s]
[0m20:28:29.643611 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived
[0m20:28:29.645859 [debug] [Thread-1  ]: Began running node model.anomaly_detection.count_cutoff
[0m20:28:29.646714 [info ] [Thread-1  ]: 3 of 38 START table model dbt_anomaly_detection.count_cutoff ................... [RUN]
[0m20:28:29.647918 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.count_cutoff"
[0m20:28:29.648421 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.count_cutoff
[0m20:28:29.648849 [debug] [Thread-1  ]: Compiling model.anomaly_detection.count_cutoff
[0m20:28:29.658395 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.count_cutoff"
[0m20:28:29.660168 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:29.660446 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.count_cutoff
[0m20:28:29.667939 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:28:29.909491 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.count_cutoff"
[0m20:28:29.911720 [debug] [Thread-1  ]: On model.anomaly_detection.count_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.count_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select agg_tag, app_event, min(time_stamps) as strt_time
from pairs
where event_count > 50
group by app_event, agg_tag 
order by app_event, agg_tag
  );
  
[0m20:28:32.005055 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:32.005968 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e060d0>]}
[0m20:28:32.006530 [info ] [Thread-1  ]: 3 of 38 OK created table model dbt_anomaly_detection.count_cutoff .............. [[32mCREATE TABLE (8.0 rows, 77.7 KB processed)[0m in 2.36s]
[0m20:28:32.007084 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.count_cutoff
[0m20:28:32.008139 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff
[0m20:28:32.008462 [info ] [Thread-1  ]: 4 of 38 START table model dbt_anomaly_detection.all_agg_derived_cutoff ......... [RUN]
[0m20:28:32.008991 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff"
[0m20:28:32.009156 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff
[0m20:28:32.009316 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff
[0m20:28:32.014481 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m20:28:32.015242 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:32.015393 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff
[0m20:28:32.018432 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:28:32.419562 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m20:28:32.421814 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select time_stamps, app_event, agg_tag, event_count
from(
select time_stamps, strt_time, pairs.app_event, pairs.agg_tag, event_count
from pairs
inner join `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff` as cutoff
on pairs.agg_tag = cutoff.agg_tag
and pairs.app_event = cutoff.app_event
order by pairs.app_event, pairs.agg_tag, time_stamps)
where time_stamps >= strt_time
  );
  
[0m20:28:34.904522 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:34.906373 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d0b310>]}
[0m20:28:34.907368 [info ] [Thread-1  ]: 4 of 38 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff .... [[32mCREATE TABLE (2.1k rows, 77.9 KB processed)[0m in 2.90s]
[0m20:28:34.908157 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff
[0m20:28:34.909626 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m20:28:34.910135 [info ] [Thread-1  ]: 5 of 38 START table model dbt_anomaly_detection.all_agg_derived_cutoff_long .... [RUN]
[0m20:28:34.910840 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m20:28:34.911108 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_long
[0m20:28:34.911567 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_long
[0m20:28:34.924226 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m20:28:34.925366 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:34.925599 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_long
[0m20:28:34.929365 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:28:35.196611 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m20:28:35.198621 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB(CURRENT_DATE(), INTERVAL 10 DAY)
  );
  
[0m20:28:37.320302 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:37.322014 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e14430>]}
[0m20:28:37.322811 [info ] [Thread-1  ]: 5 of 38 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_long  [[32mCREATE TABLE (1.9k rows, 75.6 KB processed)[0m in 2.41s]
[0m20:28:37.323854 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m20:28:37.324478 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m20:28:37.325003 [info ] [Thread-1  ]: 6 of 38 START table model dbt_anomaly_detection.all_agg_derived_cutoff_short ... [RUN]
[0m20:28:37.325926 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m20:28:37.326221 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_short
[0m20:28:37.326564 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_short
[0m20:28:37.342432 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m20:28:37.343629 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:37.343905 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_short
[0m20:28:37.349786 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:28:37.635422 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m20:28:37.638146 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB(CURRENT_DATE(), INTERVAL 15 DAY)
  );
  
[0m20:28:39.955441 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:39.957666 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d43f10>]}
[0m20:28:39.958635 [info ] [Thread-1  ]: 6 of 38 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_short  [[32mCREATE TABLE (1.8k rows, 75.6 KB processed)[0m in 2.63s]
[0m20:28:39.959700 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m20:28:39.960206 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_nonrecent_events
[0m20:28:39.960635 [info ] [Thread-1  ]: 7 of 38 START table model dbt_anomaly_detection.derived_nonrecent_events ....... [RUN]
[0m20:28:39.961686 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_nonrecent_events"
[0m20:28:39.962040 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_nonrecent_events
[0m20:28:39.962365 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_nonrecent_events
[0m20:28:39.972326 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m20:28:39.974008 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:39.974492 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_nonrecent_events
[0m20:28:39.978690 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:28:40.285020 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m20:28:40.286504 [debug] [Thread-1  ]: On model.anomaly_detection.derived_nonrecent_events: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_nonrecent_events"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events`
  
  
  OPTIONS()
  as (
    

SELECT MIN(time_stamps) AS strt_time, app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
GROUP BY app_event
HAVING DATE(MIN(time_stamps)) < DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
ORDER BY strt_time
  );
  
[0m20:28:42.548874 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:42.551386 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d3d880>]}
[0m20:28:42.552375 [info ] [Thread-1  ]: 7 of 38 OK created table model dbt_anomaly_detection.derived_nonrecent_events .. [[32mCREATE TABLE (2.0 rows, 48.0 KB processed)[0m in 2.59s]
[0m20:28:42.553327 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_nonrecent_events
[0m20:28:42.554283 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_long
[0m20:28:42.555253 [info ] [Thread-1  ]: 8 of 38 START table model dbt_anomaly_detection.aggregation_quartiles_long ..... [RUN]
[0m20:28:42.556145 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_long"
[0m20:28:42.556492 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_long
[0m20:28:42.556780 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_long
[0m20:28:42.567562 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m20:28:42.569072 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:42.569402 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_long
[0m20:28:42.573186 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:28:42.880083 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m20:28:42.882532 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m20:28:45.116039 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:45.116919 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105de6e50>]}
[0m20:28:45.117453 [info ] [Thread-1  ]: 8 of 38 OK created table model dbt_anomaly_detection.aggregation_quartiles_long  [[32mCREATE TABLE (8.0 rows, 53.0 KB processed)[0m in 2.56s]
[0m20:28:45.117979 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_long
[0m20:28:45.118241 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_short
[0m20:28:45.118717 [info ] [Thread-1  ]: 9 of 38 START table model dbt_anomaly_detection.aggregation_quartiles_short .... [RUN]
[0m20:28:45.119402 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_short"
[0m20:28:45.119739 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_short
[0m20:28:45.119945 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_short
[0m20:28:45.131346 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m20:28:45.132251 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:45.132464 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_short
[0m20:28:45.136707 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:28:45.359758 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m20:28:45.361985 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m20:28:47.457265 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:47.459332 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e422b0>]}
[0m20:28:47.460500 [info ] [Thread-1  ]: 9 of 38 OK created table model dbt_anomaly_detection.aggregation_quartiles_short  [[32mCREATE TABLE (8.0 rows, 49.7 KB processed)[0m in 2.34s]
[0m20:28:47.461375 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_short
[0m20:28:47.461785 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_long
[0m20:28:47.462955 [info ] [Thread-1  ]: 10 of 38 START table model dbt_anomaly_detection.aggregation_bounds_long ....... [RUN]
[0m20:28:47.463725 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_long"
[0m20:28:47.464061 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_long
[0m20:28:47.465727 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_long
[0m20:28:47.473372 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m20:28:47.474591 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:47.474853 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_long
[0m20:28:47.478761 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:28:47.704025 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m20:28:47.706324 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m20:28:49.921141 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:49.923334 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e19cd0>]}
[0m20:28:49.924330 [info ] [Thread-1  ]: 10 of 38 OK created table model dbt_anomaly_detection.aggregation_bounds_long .. [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.46s]
[0m20:28:49.925279 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_long
[0m20:28:49.925840 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_short
[0m20:28:49.926284 [info ] [Thread-1  ]: 11 of 38 START table model dbt_anomaly_detection.aggregation_bounds_short ...... [RUN]
[0m20:28:49.928748 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_short"
[0m20:28:49.930017 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_short
[0m20:28:49.930523 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_short
[0m20:28:49.937346 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m20:28:49.938536 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:49.938776 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_short
[0m20:28:49.942739 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:28:50.233037 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m20:28:50.234902 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m20:28:52.706276 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:52.707880 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e1c3a0>]}
[0m20:28:52.708695 [info ] [Thread-1  ]: 11 of 38 OK created table model dbt_anomaly_detection.aggregation_bounds_short . [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.78s]
[0m20:28:52.710109 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_short
[0m20:28:52.712936 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_long
[0m20:28:52.713463 [info ] [Thread-1  ]: 12 of 38 START table model dbt_anomaly_detection.aggregation_outliers_long ..... [RUN]
[0m20:28:52.714285 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_long"
[0m20:28:52.714577 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_long
[0m20:28:52.714842 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_long
[0m20:28:52.726684 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m20:28:52.728947 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:52.729188 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_long
[0m20:28:52.732414 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:28:53.022449 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m20:28:53.038375 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m20:28:55.213998 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:55.214343 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d30a00>]}
[0m20:28:55.214525 [info ] [Thread-1  ]: 12 of 38 OK created table model dbt_anomaly_detection.aggregation_outliers_long  [[32mCREATE TABLE (1.9k rows, 68.3 KB processed)[0m in 2.50s]
[0m20:28:55.214721 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_long
[0m20:28:55.214812 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_short
[0m20:28:55.215057 [info ] [Thread-1  ]: 13 of 38 START table model dbt_anomaly_detection.aggregation_outliers_short .... [RUN]
[0m20:28:55.215555 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_short"
[0m20:28:55.215643 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_short
[0m20:28:55.215730 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_short
[0m20:28:55.218594 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m20:28:55.218894 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:55.218967 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_short
[0m20:28:55.220171 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:28:55.652541 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m20:28:55.653681 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m20:28:57.845509 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:57.846912 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e420d0>]}
[0m20:28:57.847565 [info ] [Thread-1  ]: 13 of 38 OK created table model dbt_anomaly_detection.aggregation_outliers_short  [[32mCREATE TABLE (1.8k rows, 64.0 KB processed)[0m in 2.63s]
[0m20:28:57.848163 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_short
[0m20:28:57.848449 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_4hr
[0m20:28:57.849461 [info ] [Thread-1  ]: 14 of 38 START model model dbt_anomaly_detection.derived_models_05mon_4hr ...... [RUN]
[0m20:28:57.850375 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_4hr"
[0m20:28:57.850564 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_4hr
[0m20:28:57.850735 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_4hr
[0m20:28:57.856975 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m20:28:57.857783 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:57.857928 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_4hr
[0m20:28:57.877777 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m20:28:57.878218 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:28:57.878359 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:29:08.682298 [debug] [Thread-1  ]: finished collecting timing info
[0m20:29:08.683683 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e71100>]}
[0m20:29:08.684495 [info ] [Thread-1  ]: 14 of 38 OK created model model dbt_anomaly_detection.derived_models_05mon_4hr . [[32mOK[0m in 10.83s]
[0m20:29:08.685027 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_4hr
[0m20:29:08.685586 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_8hr
[0m20:29:08.688887 [info ] [Thread-1  ]: 15 of 38 START model model dbt_anomaly_detection.derived_models_05mon_8hr ...... [RUN]
[0m20:29:08.690820 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_8hr"
[0m20:29:08.691136 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_8hr
[0m20:29:08.691719 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_8hr
[0m20:29:08.703044 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m20:29:08.704520 [debug] [Thread-1  ]: finished collecting timing info
[0m20:29:08.704653 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_8hr
[0m20:29:08.706723 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m20:29:08.707119 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:29:08.707239 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:29:18.773196 [debug] [Thread-1  ]: finished collecting timing info
[0m20:29:18.775075 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e19c40>]}
[0m20:29:18.775849 [info ] [Thread-1  ]: 15 of 38 OK created model model dbt_anomaly_detection.derived_models_05mon_8hr . [[32mOK[0m in 10.08s]
[0m20:29:18.776921 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_8hr
[0m20:29:18.777336 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_4hr
[0m20:29:18.777740 [info ] [Thread-1  ]: 16 of 38 START model model dbt_anomaly_detection.derived_models_1mon_4hr ....... [RUN]
[0m20:29:18.779279 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_4hr"
[0m20:29:18.780050 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_4hr
[0m20:29:18.780487 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_4hr
[0m20:29:18.790975 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m20:29:18.791638 [debug] [Thread-1  ]: finished collecting timing info
[0m20:29:18.791767 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_4hr
[0m20:29:18.794425 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m20:29:18.795087 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:29:18.795248 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:29:29.516677 [debug] [Thread-1  ]: finished collecting timing info
[0m20:29:29.517436 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105de9f70>]}
[0m20:29:29.517938 [info ] [Thread-1  ]: 16 of 38 OK created model model dbt_anomaly_detection.derived_models_1mon_4hr .. [[32mOK[0m in 10.74s]
[0m20:29:29.518424 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_4hr
[0m20:29:29.518655 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_8hr
[0m20:29:29.519247 [info ] [Thread-1  ]: 17 of 38 START model model dbt_anomaly_detection.derived_models_1mon_8hr ....... [RUN]
[0m20:29:29.520853 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_8hr"
[0m20:29:29.521179 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_8hr
[0m20:29:29.521367 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_8hr
[0m20:29:29.534709 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m20:29:29.535230 [debug] [Thread-1  ]: finished collecting timing info
[0m20:29:29.535326 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_8hr
[0m20:29:29.538042 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m20:29:29.538866 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:29:29.539140 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:29:41.834550 [debug] [Thread-1  ]: finished collecting timing info
[0m20:29:41.836281 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e47b50>]}
[0m20:29:41.837224 [info ] [Thread-1  ]: 17 of 38 OK created model model dbt_anomaly_detection.derived_models_1mon_8hr .. [[32mOK[0m in 12.32s]
[0m20:29:41.837937 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_8hr
[0m20:29:41.838452 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_4hr
[0m20:29:41.838967 [info ] [Thread-1  ]: 18 of 38 START model model dbt_anomaly_detection.derived_models_2mon_4hr ....... [RUN]
[0m20:29:41.839626 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_4hr"
[0m20:29:41.839821 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_4hr
[0m20:29:41.839993 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_4hr
[0m20:29:41.847492 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m20:29:41.848270 [debug] [Thread-1  ]: finished collecting timing info
[0m20:29:41.848438 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_4hr
[0m20:29:41.852007 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m20:29:41.852727 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:29:41.852946 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:29:53.642880 [debug] [Thread-1  ]: finished collecting timing info
[0m20:29:53.643936 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e47610>]}
[0m20:29:53.644499 [info ] [Thread-1  ]: 18 of 38 OK created model model dbt_anomaly_detection.derived_models_2mon_4hr .. [[32mOK[0m in 11.80s]
[0m20:29:53.645078 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_4hr
[0m20:29:53.645332 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_8hr
[0m20:29:53.645827 [info ] [Thread-1  ]: 19 of 38 START model model dbt_anomaly_detection.derived_models_2mon_8hr ....... [RUN]
[0m20:29:53.646807 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_8hr"
[0m20:29:53.647011 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_8hr
[0m20:29:53.647191 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_8hr
[0m20:29:53.652952 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m20:29:53.653765 [debug] [Thread-1  ]: finished collecting timing info
[0m20:29:53.653919 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_8hr
[0m20:29:53.657290 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m20:29:53.657765 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:29:53.657988 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:30:04.248648 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:04.250256 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e47d00>]}
[0m20:30:04.250967 [info ] [Thread-1  ]: 19 of 38 OK created model model dbt_anomaly_detection.derived_models_2mon_8hr .. [[32mOK[0m in 10.60s]
[0m20:30:04.251521 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_8hr
[0m20:30:04.251742 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_12hr
[0m20:30:04.252260 [info ] [Thread-1  ]: 20 of 38 START model model dbt_anomaly_detection.derived_models_05mon_12hr ..... [RUN]
[0m20:30:04.253083 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_12hr"
[0m20:30:04.253277 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_12hr
[0m20:30:04.253449 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_12hr
[0m20:30:04.264491 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m20:30:04.266521 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:04.266721 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_12hr
[0m20:30:04.269965 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m20:30:04.270632 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:30:04.270852 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:30:15.690426 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:15.692556 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ed77f0>]}
[0m20:30:15.693476 [info ] [Thread-1  ]: 20 of 38 OK created model model dbt_anomaly_detection.derived_models_05mon_12hr  [[32mOK[0m in 11.44s]
[0m20:30:15.694393 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_12hr
[0m20:30:15.696661 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_24hr
[0m20:30:15.697638 [info ] [Thread-1  ]: 21 of 38 START model model dbt_anomaly_detection.derived_models_05mon_24hr ..... [RUN]
[0m20:30:15.698355 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_24hr"
[0m20:30:15.698591 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_24hr
[0m20:30:15.698823 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_24hr
[0m20:30:15.708024 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m20:30:15.708845 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:15.708944 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_24hr
[0m20:30:15.710743 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m20:30:15.711528 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:30:15.711694 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:30:25.995166 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:25.997620 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105dc5190>]}
[0m20:30:25.998541 [info ] [Thread-1  ]: 21 of 38 OK created model model dbt_anomaly_detection.derived_models_05mon_24hr  [[32mOK[0m in 10.30s]
[0m20:30:25.999358 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_24hr
[0m20:30:25.999710 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_12hr
[0m20:30:26.000032 [info ] [Thread-1  ]: 22 of 38 START model model dbt_anomaly_detection.derived_models_1mon_12hr ...... [RUN]
[0m20:30:26.000878 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_12hr"
[0m20:30:26.001210 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_12hr
[0m20:30:26.001460 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_12hr
[0m20:30:26.014673 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m20:30:26.015896 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:26.016135 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_12hr
[0m20:30:26.020852 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m20:30:26.021390 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:30:26.021652 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:30:36.638384 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:36.642146 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e8bc40>]}
[0m20:30:36.643306 [info ] [Thread-1  ]: 22 of 38 OK created model model dbt_anomaly_detection.derived_models_1mon_12hr . [[32mOK[0m in 10.64s]
[0m20:30:36.644572 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_12hr
[0m20:30:36.644995 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_24hr
[0m20:30:36.645393 [info ] [Thread-1  ]: 23 of 38 START model model dbt_anomaly_detection.derived_models_1mon_24hr ...... [RUN]
[0m20:30:36.646469 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_24hr"
[0m20:30:36.646867 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_24hr
[0m20:30:36.647184 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_24hr
[0m20:30:36.663216 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m20:30:36.664811 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:36.665101 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_24hr
[0m20:30:36.669670 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m20:30:36.670239 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:30:36.670497 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:30:46.764375 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:46.766861 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d5cbb0>]}
[0m20:30:46.767794 [info ] [Thread-1  ]: 23 of 38 OK created model model dbt_anomaly_detection.derived_models_1mon_24hr . [[32mOK[0m in 10.12s]
[0m20:30:46.768608 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_24hr
[0m20:30:46.768961 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_12hr
[0m20:30:46.769279 [info ] [Thread-1  ]: 24 of 38 START model model dbt_anomaly_detection.derived_models_2mon_12hr ...... [RUN]
[0m20:30:46.769987 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_12hr"
[0m20:30:46.770268 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_12hr
[0m20:30:46.770523 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_12hr
[0m20:30:46.786739 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m20:30:46.789514 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:46.789768 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_12hr
[0m20:30:46.794410 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m20:30:46.795141 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:30:46.795391 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:30:57.444274 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:57.446288 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ecdca0>]}
[0m20:30:57.447215 [info ] [Thread-1  ]: 24 of 38 OK created model model dbt_anomaly_detection.derived_models_2mon_12hr . [[32mOK[0m in 10.68s]
[0m20:30:57.448154 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_12hr
[0m20:30:57.448850 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_24hr
[0m20:30:57.449183 [info ] [Thread-1  ]: 25 of 38 START model model dbt_anomaly_detection.derived_models_2mon_24hr ...... [RUN]
[0m20:30:57.449877 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_24hr"
[0m20:30:57.450143 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_24hr
[0m20:30:57.450393 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_24hr
[0m20:30:57.460827 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m20:30:57.461825 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:57.462059 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_24hr
[0m20:30:57.469608 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m20:30:57.470199 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:30:57.470456 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m20:31:08.279919 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:08.282087 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ed6760>]}
[0m20:31:08.698298 [info ] [Thread-1  ]: 25 of 38 OK created model model dbt_anomaly_detection.derived_models_2mon_24hr . [[32mOK[0m in 10.83s]
[0m20:31:08.700385 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_24hr
[0m20:31:08.702057 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ml_detect
[0m20:31:08.702758 [info ] [Thread-1  ]: 26 of 38 START table model dbt_anomaly_detection.derived_ml_detect ............. [RUN]
[0m20:31:08.704171 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_ml_detect"
[0m20:31:08.704695 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_ml_detect
[0m20:31:08.705027 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_ml_detect
[0m20:31:08.733110 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_ml_detect"
[0m20:31:08.735617 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:08.735794 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_ml_detect
[0m20:31:08.738746 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:31:09.005663 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_ml_detect"
[0m20:31:09.008806 [debug] [Thread-1  ]: On model.anomaly_detection.derived_ml_detect: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_ml_detect"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_ml_detect`
  
  
  OPTIONS()
  as (
    -- 2 periods of training * 2 probabality threshold * 4 aggregation levels 


  WITH test_set AS (
  SELECT
  time_stamps,
        event_count,
        app_event,
        agg_tag
      FROM
        `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`  
      WHERE (agg_tag = "4hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 10 DAY)) 
      OR (agg_tag = "8hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 10 DAY))
      OR (agg_tag = "12hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 15 DAY))
      OR (agg_tag = "24hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 15 DAY))
  )

  
  

    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
       

  
  UNION ALL
  



    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
       

  
  ORDER BY
  agg_tag,
  time_stamps,
  app_event,
  prob_threshold,
  training_period
  


  );
  
[0m20:31:12.124301 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:12.126390 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ed6520>]}
[0m20:31:12.127356 [info ] [Thread-1  ]: 26 of 38 OK created table model dbt_anomaly_detection.derived_ml_detect ........ [[32mCREATE TABLE (1.5k rows, 422.5 KB processed)[0m in 3.42s]
[0m20:31:12.128439 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ml_detect
[0m20:31:12.130354 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ml_detect_tweaked
[0m20:31:12.131056 [info ] [Thread-1  ]: 27 of 38 START table model dbt_anomaly_detection.ml_detect_tweaked ............. [RUN]
[0m20:31:12.132088 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ml_detect_tweaked"
[0m20:31:12.132540 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ml_detect_tweaked
[0m20:31:12.133013 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ml_detect_tweaked
[0m20:31:12.141452 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ml_detect_tweaked"
[0m20:31:12.142592 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:12.142826 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ml_detect_tweaked
[0m20:31:12.147219 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:31:12.362930 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.ml_detect_tweaked"
[0m20:31:12.365195 [debug] [Thread-1  ]: On model.anomaly_detection.ml_detect_tweaked: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.ml_detect_tweaked"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked`
  
  
  OPTIONS()
  as (
     
  
  with neg_bound_reset as (
SELECT app_event, agg_tag, time_stamps, prob_threshold, training_period, event_count, 
  IF(lower_bound<0, 2, 0.77*lower_bound) AS lower_bound, 1.3*upper_bound AS upper_bound, anomaly_probability, is_anomaly
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ml_detect` )

SELECT app_event, agg_tag, time_stamps, prob_threshold, training_period, event_count, 
  lower_bound, upper_bound, anomaly_probability,
  (upper_bound < event_count OR event_count < lower_bound) AS is_anomaly
FROM neg_bound_reset
  );
  
[0m20:31:14.694292 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:14.696396 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d09d90>]}
[0m20:31:14.697546 [info ] [Thread-1  ]: 27 of 38 OK created table model dbt_anomaly_detection.ml_detect_tweaked ........ [[32mCREATE TABLE (1.5k rows, 137.0 KB processed)[0m in 2.56s]
[0m20:31:14.698862 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ml_detect_tweaked
[0m20:31:14.701152 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_model_features_dbt
[0m20:31:14.701898 [info ] [Thread-1  ]: 28 of 38 START table model dbt_anomaly_detection.derived_model_features_dbt .... [RUN]
[0m20:31:14.702857 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_model_features_dbt"
[0m20:31:14.703192 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_model_features_dbt
[0m20:31:14.703503 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_model_features_dbt
[0m20:31:14.715598 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_model_features_dbt"
[0m20:31:14.716621 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:14.716862 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_model_features_dbt
[0m20:31:14.724829 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:31:15.272033 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_model_features_dbt"
[0m20:31:15.274011 [debug] [Thread-1  ]: On model.anomaly_detection.derived_model_features_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_model_features_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_model_features_dbt`
  
  
  OPTIONS()
  as (
    
  
  SELECT
    app_event,
    CONCAT(agg_tag, '_', prob_threshold, "threshold", '_', RTRIM(LTRIM(training_period, "derived_models_"), CONCAT('_', agg_tag))) AS control_config,
    SUM(CASE WHEN is_anomaly = TRUE THEN 1 ELSE 0 END) AS anomalies,
    SQRT(AVG( POWER(upper_bound - lower_bound, 2) ) ) / AVG(lower_bound) AS RMSD_prcnt,
    SUM(CASE WHEN lower_bound < 0 THEN 1 ELSE 0 END) AS neg_lower
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked` AS all_configs
  GROUP BY
    app_event,
    control_config
  ORDER BY
    control_config,
    app_event
  );
  
[0m20:31:17.454049 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:17.456032 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d09c40>]}
[0m20:31:17.456945 [info ] [Thread-1  ]: 28 of 38 OK created table model dbt_anomaly_detection.derived_model_features_dbt  [[32mCREATE TABLE (48.0 rows, 103.8 KB processed)[0m in 2.75s]
[0m20:31:17.457913 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_model_features_dbt
[0m20:31:17.459518 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_model_features_dbt
[0m20:31:17.460034 [info ] [Thread-1  ]: 29 of 38 START table model dbt_anomaly_detection.filtered_model_features_dbt ... [RUN]
[0m20:31:17.460824 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_model_features_dbt"
[0m20:31:17.461335 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_model_features_dbt
[0m20:31:17.461687 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_model_features_dbt
[0m20:31:17.470299 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_model_features_dbt"
[0m20:31:17.471320 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:17.471561 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_model_features_dbt
[0m20:31:17.475680 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:31:17.728095 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.filtered_model_features_dbt"
[0m20:31:17.730643 [debug] [Thread-1  ]: On model.anomaly_detection.filtered_model_features_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.filtered_model_features_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
  
  
  OPTIONS()
  as (
    
SELECT features.app_event, control_config, anomalies, RMSD_prcnt, neg_lower
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events` AS non_recent
INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`derived_model_features_dbt` AS features
  ON non_recent.app_event = features.app_event
  );
  
[0m20:31:20.131321 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:20.133229 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d580a0>]}
[0m20:31:20.134165 [info ] [Thread-1  ]: 29 of 38 OK created table model dbt_anomaly_detection.filtered_model_features_dbt  [[32mCREATE TABLE (48.0 rows, 3.2 KB processed)[0m in 2.67s]
[0m20:31:20.135276 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_model_features_dbt
[0m20:31:20.137250 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ref_distinct_tuples
[0m20:31:20.137932 [info ] [Thread-1  ]: 30 of 38 START table model dbt_anomaly_detection.ref_distinct_tuples ........... [RUN]
[0m20:31:20.138889 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ref_distinct_tuples"
[0m20:31:20.139304 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ref_distinct_tuples
[0m20:31:20.139636 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ref_distinct_tuples
[0m20:31:20.148322 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ref_distinct_tuples"
[0m20:31:20.149392 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:20.149636 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ref_distinct_tuples
[0m20:31:20.153776 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:31:20.388305 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.ref_distinct_tuples"
[0m20:31:20.389804 [debug] [Thread-1  ]: On model.anomaly_detection.ref_distinct_tuples: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.ref_distinct_tuples"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`ref_distinct_tuples`
  
  
  OPTIONS()
  as (
    
SELECT DISTINCT app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
WHERE app_event LIKE '%add%'
UNION ALL
SELECT DISTINCT app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
WHERE app_event NOT LIKE '%ad%'
ORDER BY app_event
  );
  
[0m20:31:22.607327 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:22.609279 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f45f10>]}
[0m20:31:22.610387 [info ] [Thread-1  ]: 30 of 38 OK created table model dbt_anomaly_detection.ref_distinct_tuples ...... [[32mCREATE TABLE (2.0 rows, 720.0 Bytes processed)[0m in 2.47s]
[0m20:31:22.611046 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ref_distinct_tuples
[0m20:31:22.611340 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_features_null_filtered
[0m20:31:22.611649 [info ] [Thread-1  ]: 31 of 38 START table model dbt_anomaly_detection.remaining_events_features_null_filtered  [RUN]
[0m20:31:22.612283 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_features_null_filtered"
[0m20:31:22.612555 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_features_null_filtered
[0m20:31:22.612761 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_features_null_filtered
[0m20:31:22.627093 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_features_null_filtered"
[0m20:31:22.629224 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:22.629489 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_features_null_filtered
[0m20:31:22.635993 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:31:22.881925 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_features_null_filtered"
[0m20:31:22.883330 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_features_null_filtered: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_features_null_filtered"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
where RMSD_prcnt is not null
  );
  
[0m20:31:24.870629 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:24.871275 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f42310>]}
[0m20:31:24.871675 [info ] [Thread-1  ]: 31 of 38 OK created table model dbt_anomaly_detection.remaining_events_features_null_filtered  [[32mCREATE TABLE (48.0 rows, 3.1 KB processed)[0m in 2.26s]
[0m20:31:24.872063 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_features_null_filtered
[0m20:31:24.872691 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies
[0m20:31:24.873007 [info ] [Thread-1  ]: 32 of 38 START table model dbt_anomaly_detection.remaining_events_min_anomalies  [RUN]
[0m20:31:24.873391 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies"
[0m20:31:24.873529 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies
[0m20:31:24.873665 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies
[0m20:31:24.877804 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies"
[0m20:31:24.878395 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:24.878535 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies
[0m20:31:24.880937 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:31:25.128086 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_anomalies"
[0m20:31:25.129374 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies`
  
  
  OPTIONS()
  as (
    

SELECT app_event, MIN(anomalies) AS anomalies 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered`
  GROUP BY app_event
  ORDER BY app_event
  );
  
[0m20:31:27.232036 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:27.233801 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f3da60>]}
[0m20:31:27.234611 [info ] [Thread-1  ]: 32 of 38 OK created table model dbt_anomaly_detection.remaining_events_min_anomalies  [[32mCREATE TABLE (2.0 rows, 1.1 KB processed)[0m in 2.36s]
[0m20:31:27.239113 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies
[0m20:31:27.241014 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m20:31:27.241727 [info ] [Thread-1  ]: 33 of 38 START table model dbt_anomaly_detection.remaining_events_min_anomalies_results  [RUN]
[0m20:31:27.244653 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m20:31:27.246116 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies_results
[0m20:31:27.246356 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies_results
[0m20:31:27.264947 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m20:31:27.266169 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:27.266413 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies_results
[0m20:31:27.270634 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:31:27.710820 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m20:31:27.714319 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_anomalies_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_anomalies_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results`
  
  
  OPTIONS()
  as (
    

SELECT neg_lower_criteria_res.app_event, neg_lower_criteria_res.control_config, 
    neg_lower_criteria_res.anomalies, neg_lower_criteria_res.RMSD_prcnt, neg_lower_criteria_res.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered` AS neg_lower_criteria_res
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies` AS min_neg_lower_min_anomalies
    ON neg_lower_criteria_res.anomalies = min_neg_lower_min_anomalies.anomalies
      AND neg_lower_criteria_res.app_event = min_neg_lower_min_anomalies.app_event
  ORDER BY neg_lower_criteria_res.app_event, RMSD_prcnt DESC
  );
  
[0m20:31:30.115926 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:30.117802 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f85460>]}
[0m20:31:30.120059 [info ] [Thread-1  ]: 33 of 38 OK created table model dbt_anomaly_detection.remaining_events_min_anomalies_results  [[32mCREATE TABLE (30.0 rows, 3.2 KB processed)[0m in 2.88s]
[0m20:31:30.121389 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m20:31:30.123870 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD
[0m20:31:30.125091 [info ] [Thread-1  ]: 34 of 38 START table model dbt_anomaly_detection.remaining_events_min_RMSD ..... [RUN]
[0m20:31:30.127156 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD"
[0m20:31:30.128373 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD
[0m20:31:30.128813 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD
[0m20:31:30.138214 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD"
[0m20:31:30.139450 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:30.139789 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD
[0m20:31:30.143070 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:31:30.351517 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_RMSD"
[0m20:31:30.353926 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_RMSD: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_RMSD"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD`
  
  
  OPTIONS()
  as (
    

SELECT app_event, MIN(RMSD_prcnt) AS RMSD_prcnt 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results`
  GROUP BY app_event
  ORDER BY app_event
  );
  
[0m20:31:32.355935 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:32.358238 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f85070>]}
[0m20:31:32.360008 [info ] [Thread-1  ]: 34 of 38 OK created table model dbt_anomaly_detection.remaining_events_min_RMSD  [[32mCREATE TABLE (2.0 rows, 690.0 Bytes processed)[0m in 2.23s]
[0m20:31:32.361347 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD
[0m20:31:32.362932 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m20:31:32.363764 [info ] [Thread-1  ]: 35 of 38 START table model dbt_anomaly_detection.remaining_events_min_RMSD_results  [RUN]
[0m20:31:32.365267 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m20:31:32.365773 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD_results
[0m20:31:32.366110 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD_results
[0m20:31:32.374799 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m20:31:32.376157 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:32.376531 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD_results
[0m20:31:32.381265 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:31:32.705840 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m20:31:32.708678 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_RMSD_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_RMSD_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
  
  
  OPTIONS()
  as (
    

SELECT anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.control_config, 
    anomalies_criteria_res_dbt.anomalies, anomalies_criteria_res_dbt.RMSD_prcnt, anomalies_criteria_res_dbt.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results` AS anomalies_criteria_res_dbt
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD` AS min_anomalies_max_RMSD
    ON anomalies_criteria_res_dbt.RMSD_prcnt = min_anomalies_max_RMSD.RMSD_prcnt
      AND anomalies_criteria_res_dbt.app_event = min_anomalies_max_RMSD.app_event
  ORDER BY anomalies_criteria_res_dbt.app_event, control_config
  );
  
[0m20:31:34.992490 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:34.993912 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f0daf0>]}
[0m20:31:34.994591 [info ] [Thread-1  ]: 35 of 38 OK created table model dbt_anomaly_detection.remaining_events_min_RMSD_results  [[32mCREATE TABLE (2.0 rows, 2.0 KB processed)[0m in 2.63s]
[0m20:31:34.995386 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m20:31:34.996497 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_all_models
[0m20:31:34.997380 [info ] [Thread-1  ]: 36 of 38 START table model dbt_anomaly_detection.filtered_all_models ........... [RUN]
[0m20:31:34.998601 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_all_models"
[0m20:31:34.999295 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_all_models
[0m20:31:35.000477 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_all_models
[0m20:31:35.007047 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_all_models"
[0m20:31:35.008098 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:35.008407 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_all_models
[0m20:31:35.013201 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:31:35.259347 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.filtered_all_models"
[0m20:31:35.262368 [debug] [Thread-1  ]: On model.anomaly_detection.filtered_all_models: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.filtered_all_models"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`filtered_all_models`
  
  
  OPTIONS()
  as (
    
SELECT app_event, control_config, anomalies, RMSD_prcnt, neg_lower
FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
WHERE app_event LIKE '%add%'
UNION ALL 
SELECT app_event, control_config, anomalies, RMSD_prcnt, neg_lower
FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
WHERE app_event NOT LIKE '%ad%'
ORDER BY app_event
  );
  
[0m20:31:37.452321 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:37.454169 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fa8580>]}
[0m20:31:37.455457 [info ] [Thread-1  ]: 36 of 38 OK created table model dbt_anomaly_detection.filtered_all_models ...... [[32mCREATE TABLE (2.0 rows, 132.0 Bytes processed)[0m in 2.46s]
[0m20:31:37.456229 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_all_models
[0m20:31:37.457606 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m20:31:37.458066 [info ] [Thread-1  ]: 37 of 38 START table model dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [RUN]
[0m20:31:37.458864 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m20:31:37.459253 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m20:31:37.459715 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m20:31:37.470648 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m20:31:37.472554 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:37.472805 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m20:31:37.476945 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:31:37.698366 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m20:31:37.701479 [debug] [Thread-1  ]: On model.anomaly_detection.derived_alerts_fired_automation_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_alerts_fired_automation_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_alerts_fired_automation_dbt`
  
  
  OPTIONS()
  as (
    
  
  WITH ml_detect_updated AS (
    SELECT app_event, 
      CONCAT(agg_tag, '_', prob_threshold, "threshold", '_', RTRIM(LTRIM(training_period, "derived_models_"), CONCAT('_', agg_tag))) AS control_config,
      time_stamps, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked`
  )
  SELECT time_stamps, all_configs.app_event AS app_event, control_table.control_config AS control_config, 
    anomalies, RMSD_prcnt, neg_lower, event_count, lower_bound, upper_bound, anomaly_probability, is_anomaly
  FROM ml_detect_updated AS all_configs 
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`filtered_all_models` AS control_table 
    ON all_configs.app_event = control_table.app_event
      AND all_configs.control_config = control_table.control_config
  ORDER BY all_configs.app_event, time_stamps
  );
  
[0m20:31:39.752956 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:39.754356 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f80df0>]}
[0m20:31:39.754766 [info ] [Thread-1  ]: 37 of 38 OK created table model dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [[32mCREATE TABLE (55.0 rows, 138.5 KB processed)[0m in 2.30s]
[0m20:31:39.755132 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m20:31:39.755884 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_events_with_anomalies
[0m20:31:39.756319 [info ] [Thread-1  ]: 38 of 38 START table model dbt_anomaly_detection.derived_events_with_anomalies . [RUN]
[0m20:31:39.756785 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_events_with_anomalies"
[0m20:31:39.756934 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_events_with_anomalies
[0m20:31:39.757079 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_events_with_anomalies
[0m20:31:39.761546 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_events_with_anomalies"
[0m20:31:39.762019 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:39.762159 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_events_with_anomalies
[0m20:31:39.764445 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:31:40.053566 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_events_with_anomalies"
[0m20:31:40.056231 [debug] [Thread-1  ]: On model.anomaly_detection.derived_events_with_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_events_with_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_events_with_anomalies`
  
  
  OPTIONS()
  as (
    
SELECT time_stamps, app_event, event_count AS event_count_anomalous_yesterday
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_alerts_fired_automation_dbt`
WHERE DATE(time_stamps) = CURRENT_DATE() - 1 
  AND is_anomaly
  );
  
[0m20:31:42.307853 [debug] [Thread-1  ]: finished collecting timing info
[0m20:31:42.308991 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '43c11122-1d24-43a3-a74e-824fb75edd3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f80cd0>]}
[0m20:31:42.309802 [info ] [Thread-1  ]: 38 of 38 OK created table model dbt_anomaly_detection.derived_events_with_anomalies  [[32mCREATE TABLE (0.0 rows, 1.7 KB processed)[0m in 2.55s]
[0m20:31:42.310459 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_events_with_anomalies
[0m20:31:42.314989 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m20:31:42.316602 [info ] [MainThread]: 
[0m20:31:42.317489 [info ] [MainThread]: Finished running 26 table models, 12 model models in 0 hours 3 minutes and 23.06 seconds (203.06s).
[0m20:31:42.318431 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:31:42.318623 [debug] [MainThread]: Connection 'model.anomaly_detection.derived_events_with_anomalies' was properly closed.
[0m20:31:42.340371 [info ] [MainThread]: 
[0m20:31:42.340711 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:31:42.340918 [info ] [MainThread]: 
[0m20:31:42.341061 [info ] [MainThread]: Done. PASS=38 WARN=0 ERROR=0 SKIP=0 TOTAL=38
[0m20:31:42.341339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f4c730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f4c310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f4c190>]}
[0m20:31:42.341505 [debug] [MainThread]: Flushing usage events


============================== 2023-02-10 22:46:21.672898 | 1a84cd5d-f80c-4ccb-bbee-e9a70b38260b ==============================
[0m22:46:21.672926 [info ] [MainThread]: Running with dbt=1.2.1
[0m22:46:21.673956 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:46:21.674059 [debug] [MainThread]: Tracking: tracking
[0m22:46:21.693285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b09d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b1db50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b1d160>]}
[0m22:46:21.750357 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:46:21.750659 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/reference_derived.sql
[0m22:46:21.759419 [debug] [MainThread]: 1699: static parser successfully parsed example/reference_derived.sql
[0m22:46:21.773895 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ef5f40>]}
[0m22:46:21.781389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105cbfb80>]}
[0m22:46:21.781581 [info ] [MainThread]: Found 38 models, 0 tests, 0 snapshots, 0 analyses, 540 macros, 0 operations, 0 seed files, 2 sources, 0 exposures, 0 metrics
[0m22:46:21.781721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ea6850>]}
[0m22:46:21.783117 [info ] [MainThread]: 
[0m22:46:21.783387 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m22:46:21.784748 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow"
[0m22:46:21.784964 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:46:22.948080 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow_dbt_anomaly_detection"
[0m22:46:22.948343 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:46:23.170654 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b1d790>]}
[0m22:46:23.171783 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:46:23.172223 [info ] [MainThread]: 
[0m22:46:23.178456 [debug] [Thread-1  ]: Began running node model.anomaly_detection.reference_derived
[0m22:46:23.178953 [info ] [Thread-1  ]: 1 of 38 START table model dbt_anomaly_detection.reference_derived .............. [RUN]
[0m22:46:23.179742 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.reference_derived"
[0m22:46:23.179928 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.reference_derived
[0m22:46:23.180178 [debug] [Thread-1  ]: Compiling model.anomaly_detection.reference_derived
[0m22:46:23.184753 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.reference_derived"
[0m22:46:23.185887 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:23.186010 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.reference_derived
[0m22:46:23.231061 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:46:23.597221 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.reference_derived"
[0m22:46:23.599161 [debug] [Thread-1  ]: On model.anomaly_detection.reference_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.reference_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived`
  
  
  OPTIONS()
  as (
    
SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`web_purchase_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB("2023-02-09", INTERVAL 90 DAY) AND DATE(collector_tstamp) < "2023-02-09"

UNION ALL

SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`pco_web_apply_filter_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB("2023-02-09", INTERVAL 90 DAY) AND DATE(collector_tstamp) < "2023-02-09"
  );
  
[0m22:46:27.567833 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:27.568681 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fa2130>]}
[0m22:46:27.569036 [info ] [Thread-1  ]: 1 of 38 OK created table model dbt_anomaly_detection.reference_derived ......... [[32mCREATE TABLE (1.1m rows, 34.8 MB processed)[0m in 4.39s]
[0m22:46:27.569418 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.reference_derived
[0m22:46:27.570068 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived
[0m22:46:27.570453 [info ] [Thread-1  ]: 2 of 38 START table model dbt_anomaly_detection.all_agg_derived ................ [RUN]
[0m22:46:27.570824 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived"
[0m22:46:27.570951 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived
[0m22:46:27.571074 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived
[0m22:46:27.575589 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived"
[0m22:46:27.576202 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:27.576317 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived
[0m22:46:27.578373 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:46:27.950524 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived"
[0m22:46:27.950971 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
  
  
  OPTIONS()
  as (
    


SELECT "4hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_4hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/4)*4 AS _4hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _4hr_trunc,
    app_id,
    event_type)

UNION ALL


SELECT "8hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_8hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/8)*8 AS _8hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _8hr_trunc,
    app_id,
    event_type)

UNION ALL


SELECT "12hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_12hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/12)*12 AS _12hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _12hr_trunc,
    app_id,
    event_type)

UNION ALL


SELECT "24hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_24hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/24)*24 AS _24hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _24hr_trunc,
    app_id,
    event_type)

ORDER BY
  agg_tag,
  time_stamps,
  app_id,
  event_type


  );
  
[0m22:46:32.467654 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:32.468130 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062ace80>]}
[0m22:46:32.468358 [info ] [Thread-1  ]: 2 of 38 OK created table model dbt_anomaly_detection.all_agg_derived ........... [[32mCREATE TABLE (2.2k rows, 34.8 MB processed)[0m in 4.90s]
[0m22:46:32.468592 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived
[0m22:46:32.468985 [debug] [Thread-1  ]: Began running node model.anomaly_detection.count_cutoff
[0m22:46:32.469216 [info ] [Thread-1  ]: 3 of 38 START table model dbt_anomaly_detection.count_cutoff ................... [RUN]
[0m22:46:32.469569 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.count_cutoff"
[0m22:46:32.469657 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.count_cutoff
[0m22:46:32.469737 [debug] [Thread-1  ]: Compiling model.anomaly_detection.count_cutoff
[0m22:46:32.471748 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.count_cutoff"
[0m22:46:32.472331 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:32.472506 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.count_cutoff
[0m22:46:32.474103 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:46:32.704987 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.count_cutoff"
[0m22:46:32.705488 [debug] [Thread-1  ]: On model.anomaly_detection.count_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.count_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select agg_tag, app_event, min(time_stamps) as strt_time
from pairs
where event_count > 50
group by app_event, agg_tag 
order by app_event, agg_tag
  );
  
[0m22:46:34.778335 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:34.778792 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062b5a60>]}
[0m22:46:34.779061 [info ] [Thread-1  ]: 3 of 38 OK created table model dbt_anomaly_detection.count_cutoff .............. [[32mCREATE TABLE (8.0 rows, 78.6 KB processed)[0m in 2.31s]
[0m22:46:34.779351 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.count_cutoff
[0m22:46:34.779731 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff
[0m22:46:34.780164 [info ] [Thread-1  ]: 4 of 38 START table model dbt_anomaly_detection.all_agg_derived_cutoff ......... [RUN]
[0m22:46:34.780549 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff"
[0m22:46:34.780677 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff
[0m22:46:34.780786 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff
[0m22:46:34.787546 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m22:46:34.787931 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:34.788040 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff
[0m22:46:34.789842 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:46:35.110688 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m22:46:35.111707 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select time_stamps, app_event, agg_tag, event_count
from(
select time_stamps, strt_time, pairs.app_event, pairs.agg_tag, event_count
from pairs
inner join `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff` as cutoff
on pairs.agg_tag = cutoff.agg_tag
and pairs.app_event = cutoff.app_event
order by pairs.app_event, pairs.agg_tag, time_stamps)
where time_stamps >= strt_time
  );
  
[0m22:46:37.213111 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:37.214046 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062bd6d0>]}
[0m22:46:37.214520 [info ] [Thread-1  ]: 4 of 38 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff .... [[32mCREATE TABLE (2.2k rows, 78.8 KB processed)[0m in 2.43s]
[0m22:46:37.214992 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff
[0m22:46:37.215948 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m22:46:37.216536 [info ] [Thread-1  ]: 5 of 38 START table model dbt_anomaly_detection.all_agg_derived_cutoff_long .... [RUN]
[0m22:46:37.217088 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m22:46:37.217257 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_long
[0m22:46:37.217413 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_long
[0m22:46:37.222183 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m22:46:37.223765 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:37.224045 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_long
[0m22:46:37.226939 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:46:37.464677 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m22:46:37.465534 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB(CURRENT_DATE(), INTERVAL 10 DAY)
  );
  
[0m22:46:39.698701 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:39.699589 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062b6ee0>]}
[0m22:46:39.700027 [info ] [Thread-1  ]: 5 of 38 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_long  [[32mCREATE TABLE (1.9k rows, 76.5 KB processed)[0m in 2.48s]
[0m22:46:39.700447 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m22:46:39.700640 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m22:46:39.700833 [info ] [Thread-1  ]: 6 of 38 START table model dbt_anomaly_detection.all_agg_derived_cutoff_short ... [RUN]
[0m22:46:39.701205 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m22:46:39.701879 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_short
[0m22:46:39.702037 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_short
[0m22:46:39.705068 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m22:46:39.706538 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:39.706686 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_short
[0m22:46:39.710619 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:46:39.981735 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m22:46:39.982938 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB(CURRENT_DATE(), INTERVAL 15 DAY)
  );
  
[0m22:46:42.102061 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:42.103850 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10621ae80>]}
[0m22:46:42.104380 [info ] [Thread-1  ]: 6 of 38 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_short  [[32mCREATE TABLE (1.8k rows, 76.5 KB processed)[0m in 2.40s]
[0m22:46:42.104845 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m22:46:42.105066 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_nonrecent_events
[0m22:46:42.105895 [info ] [Thread-1  ]: 7 of 38 START table model dbt_anomaly_detection.derived_nonrecent_events ....... [RUN]
[0m22:46:42.106638 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_nonrecent_events"
[0m22:46:42.106820 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_nonrecent_events
[0m22:46:42.106980 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_nonrecent_events
[0m22:46:42.115736 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m22:46:42.116895 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:42.117070 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_nonrecent_events
[0m22:46:42.119683 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:46:42.285812 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m22:46:42.289531 [debug] [Thread-1  ]: On model.anomaly_detection.derived_nonrecent_events: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_nonrecent_events"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events`
  
  
  OPTIONS()
  as (
    

SELECT MIN(time_stamps) AS strt_time, app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
GROUP BY app_event
HAVING DATE(MIN(time_stamps)) < DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
ORDER BY strt_time
  );
  
[0m22:46:44.548352 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:44.549459 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10621e0d0>]}
[0m22:46:44.550209 [info ] [Thread-1  ]: 7 of 38 OK created table model dbt_anomaly_detection.derived_nonrecent_events .. [[32mCREATE TABLE (2.0 rows, 48.5 KB processed)[0m in 2.44s]
[0m22:46:44.550726 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_nonrecent_events
[0m22:46:44.550954 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_long
[0m22:46:44.551415 [info ] [Thread-1  ]: 8 of 38 START table model dbt_anomaly_detection.aggregation_quartiles_long ..... [RUN]
[0m22:46:44.552043 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_long"
[0m22:46:44.552227 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_long
[0m22:46:44.552412 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_long
[0m22:46:44.556980 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m22:46:44.557700 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:44.557876 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_long
[0m22:46:44.560526 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:46:44.824712 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m22:46:44.825948 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m22:46:47.156954 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:47.157604 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062993a0>]}
[0m22:46:47.157947 [info ] [Thread-1  ]: 8 of 38 OK created table model dbt_anomaly_detection.aggregation_quartiles_long  [[32mCREATE TABLE (8.0 rows, 53.6 KB processed)[0m in 2.61s]
[0m22:46:47.158351 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_long
[0m22:46:47.158543 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_short
[0m22:46:47.158912 [info ] [Thread-1  ]: 9 of 38 START table model dbt_anomaly_detection.aggregation_quartiles_short .... [RUN]
[0m22:46:47.159485 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_short"
[0m22:46:47.159625 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_short
[0m22:46:47.159744 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_short
[0m22:46:47.162731 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m22:46:47.164515 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:47.164638 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_short
[0m22:46:47.169569 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:46:47.399940 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m22:46:47.402902 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m22:46:49.488303 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:49.493628 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062ace80>]}
[0m22:46:49.494011 [info ] [Thread-1  ]: 9 of 38 OK created table model dbt_anomaly_detection.aggregation_quartiles_short  [[32mCREATE TABLE (8.0 rows, 50.3 KB processed)[0m in 2.33s]
[0m22:46:49.494286 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_short
[0m22:46:49.494511 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_long
[0m22:46:49.494928 [info ] [Thread-1  ]: 10 of 38 START table model dbt_anomaly_detection.aggregation_bounds_long ....... [RUN]
[0m22:46:49.495217 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_long"
[0m22:46:49.495320 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_long
[0m22:46:49.495404 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_long
[0m22:46:49.498320 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m22:46:49.499162 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:49.499320 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_long
[0m22:46:49.500810 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:46:49.794956 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m22:46:49.797774 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m22:46:51.885438 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:51.885825 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106295a90>]}
[0m22:46:51.886050 [info ] [Thread-1  ]: 10 of 38 OK created table model dbt_anomaly_detection.aggregation_bounds_long .. [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.39s]
[0m22:46:51.886277 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_long
[0m22:46:51.886383 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_short
[0m22:46:51.886620 [info ] [Thread-1  ]: 11 of 38 START table model dbt_anomaly_detection.aggregation_bounds_short ...... [RUN]
[0m22:46:51.886860 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_short"
[0m22:46:51.886946 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_short
[0m22:46:51.887025 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_short
[0m22:46:51.889197 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m22:46:51.889619 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:51.889721 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_short
[0m22:46:51.891397 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:46:52.080271 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m22:46:52.081182 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m22:46:54.267632 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:54.268756 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062abd90>]}
[0m22:46:54.269401 [info ] [Thread-1  ]: 11 of 38 OK created table model dbt_anomaly_detection.aggregation_bounds_short . [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.38s]
[0m22:46:54.269928 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_short
[0m22:46:54.270161 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_long
[0m22:46:54.270798 [info ] [Thread-1  ]: 12 of 38 START table model dbt_anomaly_detection.aggregation_outliers_long ..... [RUN]
[0m22:46:54.271438 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_long"
[0m22:46:54.271617 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_long
[0m22:46:54.271792 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_long
[0m22:46:54.280100 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m22:46:54.281770 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:54.281940 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_long
[0m22:46:54.286289 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:46:54.603446 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m22:46:54.604697 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m22:46:57.134084 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:57.136185 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062048e0>]}
[0m22:46:57.136743 [info ] [Thread-1  ]: 12 of 38 OK created table model dbt_anomaly_detection.aggregation_outliers_long  [[32mCREATE TABLE (1.9k rows, 69.1 KB processed)[0m in 2.86s]
[0m22:46:57.137220 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_long
[0m22:46:57.137481 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_short
[0m22:46:57.138486 [info ] [Thread-1  ]: 13 of 38 START table model dbt_anomaly_detection.aggregation_outliers_short .... [RUN]
[0m22:46:57.138952 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_short"
[0m22:46:57.139132 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_short
[0m22:46:57.139293 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_short
[0m22:46:57.143602 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m22:46:57.144383 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:57.144536 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_short
[0m22:46:57.147322 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:46:57.382701 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m22:46:57.384483 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m22:46:59.511318 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:59.511988 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a929a0>]}
[0m22:46:59.512349 [info ] [Thread-1  ]: 13 of 38 OK created table model dbt_anomaly_detection.aggregation_outliers_short  [[32mCREATE TABLE (1.8k rows, 64.9 KB processed)[0m in 2.37s]
[0m22:46:59.512662 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_short
[0m22:46:59.512797 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_4hr
[0m22:46:59.513097 [info ] [Thread-1  ]: 14 of 38 START model model dbt_anomaly_detection.derived_models_05mon_4hr ...... [RUN]
[0m22:46:59.514093 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_4hr"
[0m22:46:59.514255 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_4hr
[0m22:46:59.514379 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_4hr
[0m22:46:59.518075 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m22:46:59.518560 [debug] [Thread-1  ]: finished collecting timing info
[0m22:46:59.518673 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_4hr
[0m22:46:59.532660 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m22:46:59.533302 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:46:59.533491 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:47:10.219334 [debug] [Thread-1  ]: finished collecting timing info
[0m22:47:10.220558 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10623ae20>]}
[0m22:47:10.220972 [info ] [Thread-1  ]: 14 of 38 OK created model model dbt_anomaly_detection.derived_models_05mon_4hr . [[32mOK[0m in 10.71s]
[0m22:47:10.221361 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_4hr
[0m22:47:10.221526 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_8hr
[0m22:47:10.221700 [info ] [Thread-1  ]: 15 of 38 START model model dbt_anomaly_detection.derived_models_05mon_8hr ...... [RUN]
[0m22:47:10.222208 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_8hr"
[0m22:47:10.222371 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_8hr
[0m22:47:10.222504 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_8hr
[0m22:47:10.231128 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m22:47:10.232212 [debug] [Thread-1  ]: finished collecting timing info
[0m22:47:10.232370 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_8hr
[0m22:47:10.235562 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m22:47:10.236508 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:47:10.236802 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:47:24.395813 [debug] [Thread-1  ]: finished collecting timing info
[0m22:47:24.398404 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062b5a60>]}
[0m22:47:24.399001 [info ] [Thread-1  ]: 15 of 38 OK created model model dbt_anomaly_detection.derived_models_05mon_8hr . [[32mOK[0m in 14.18s]
[0m22:47:24.399595 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_8hr
[0m22:47:24.399867 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_4hr
[0m22:47:24.400156 [info ] [Thread-1  ]: 16 of 38 START model model dbt_anomaly_detection.derived_models_1mon_4hr ....... [RUN]
[0m22:47:24.400968 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_4hr"
[0m22:47:24.401226 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_4hr
[0m22:47:24.401440 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_4hr
[0m22:47:24.409999 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m22:47:24.411372 [debug] [Thread-1  ]: finished collecting timing info
[0m22:47:24.411595 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_4hr
[0m22:47:24.415135 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m22:47:24.416083 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:47:24.416307 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:47:34.551807 [debug] [Thread-1  ]: finished collecting timing info
[0m22:47:34.552716 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106276e20>]}
[0m22:47:34.553203 [info ] [Thread-1  ]: 16 of 38 OK created model model dbt_anomaly_detection.derived_models_1mon_4hr .. [[32mOK[0m in 10.15s]
[0m22:47:34.553678 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_4hr
[0m22:47:34.553895 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_8hr
[0m22:47:34.554286 [info ] [Thread-1  ]: 17 of 38 START model model dbt_anomaly_detection.derived_models_1mon_8hr ....... [RUN]
[0m22:47:34.554850 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_8hr"
[0m22:47:34.555035 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_8hr
[0m22:47:34.555204 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_8hr
[0m22:47:34.562712 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m22:47:34.564203 [debug] [Thread-1  ]: finished collecting timing info
[0m22:47:34.564368 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_8hr
[0m22:47:34.569233 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m22:47:34.570490 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:47:34.570719 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:47:44.710980 [debug] [Thread-1  ]: finished collecting timing info
[0m22:47:44.713430 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106423790>]}
[0m22:47:44.714046 [info ] [Thread-1  ]: 17 of 38 OK created model model dbt_anomaly_detection.derived_models_1mon_8hr .. [[32mOK[0m in 10.16s]
[0m22:47:44.714503 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_8hr
[0m22:47:44.714713 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_4hr
[0m22:47:44.715085 [info ] [Thread-1  ]: 18 of 38 START model model dbt_anomaly_detection.derived_models_2mon_4hr ....... [RUN]
[0m22:47:44.715497 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_4hr"
[0m22:47:44.715642 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_4hr
[0m22:47:44.715785 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_4hr
[0m22:47:44.740496 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m22:47:44.741517 [debug] [Thread-1  ]: finished collecting timing info
[0m22:47:44.741671 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_4hr
[0m22:47:44.744749 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m22:47:44.745168 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:47:44.745375 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:47:55.851408 [debug] [Thread-1  ]: finished collecting timing info
[0m22:47:55.853675 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106212a60>]}
[0m22:47:55.854183 [info ] [Thread-1  ]: 18 of 38 OK created model model dbt_anomaly_detection.derived_models_2mon_4hr .. [[32mOK[0m in 11.14s]
[0m22:47:55.854618 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_4hr
[0m22:47:55.854791 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_8hr
[0m22:47:55.854972 [info ] [Thread-1  ]: 19 of 38 START model model dbt_anomaly_detection.derived_models_2mon_8hr ....... [RUN]
[0m22:47:55.855637 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_8hr"
[0m22:47:55.855876 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_8hr
[0m22:47:55.856045 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_8hr
[0m22:47:55.863951 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m22:47:55.865044 [debug] [Thread-1  ]: finished collecting timing info
[0m22:47:55.865231 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_8hr
[0m22:47:55.868144 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m22:47:55.868570 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:47:55.868714 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:48:25.023426 [debug] [Thread-1  ]: finished collecting timing info
[0m22:48:25.024743 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105adb820>]}
[0m22:48:25.025212 [info ] [Thread-1  ]: 19 of 38 OK created model model dbt_anomaly_detection.derived_models_2mon_8hr .. [[32mOK[0m in 29.17s]
[0m22:48:25.025706 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_8hr
[0m22:48:25.025934 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_12hr
[0m22:48:25.026154 [info ] [Thread-1  ]: 20 of 38 START model model dbt_anomaly_detection.derived_models_05mon_12hr ..... [RUN]
[0m22:48:25.026430 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_12hr"
[0m22:48:25.026515 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_12hr
[0m22:48:25.026590 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_12hr
[0m22:48:25.029238 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m22:48:25.038696 [debug] [Thread-1  ]: finished collecting timing info
[0m22:48:25.038879 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_12hr
[0m22:48:25.044282 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m22:48:25.044813 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:48:25.044967 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:48:35.890723 [debug] [Thread-1  ]: finished collecting timing info
[0m22:48:35.893016 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10627b040>]}
[0m22:48:35.893694 [info ] [Thread-1  ]: 20 of 38 OK created model model dbt_anomaly_detection.derived_models_05mon_12hr  [[32mOK[0m in 10.87s]
[0m22:48:35.894296 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_12hr
[0m22:48:35.894628 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_24hr
[0m22:48:35.895199 [info ] [Thread-1  ]: 21 of 38 START model model dbt_anomaly_detection.derived_models_05mon_24hr ..... [RUN]
[0m22:48:35.895943 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_24hr"
[0m22:48:35.896154 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_24hr
[0m22:48:35.896368 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_24hr
[0m22:48:35.902430 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m22:48:35.903767 [debug] [Thread-1  ]: finished collecting timing info
[0m22:48:35.904183 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_24hr
[0m22:48:35.907897 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m22:48:35.909072 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:48:35.909369 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:48:46.183363 [debug] [Thread-1  ]: finished collecting timing info
[0m22:48:46.185563 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062bdc10>]}
[0m22:48:46.186045 [info ] [Thread-1  ]: 21 of 38 OK created model model dbt_anomaly_detection.derived_models_05mon_24hr  [[32mOK[0m in 10.29s]
[0m22:48:46.186533 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_24hr
[0m22:48:46.186780 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_12hr
[0m22:48:46.187143 [info ] [Thread-1  ]: 22 of 38 START model model dbt_anomaly_detection.derived_models_1mon_12hr ...... [RUN]
[0m22:48:46.187728 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_12hr"
[0m22:48:46.187923 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_12hr
[0m22:48:46.188099 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_12hr
[0m22:48:46.196068 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m22:48:46.198275 [debug] [Thread-1  ]: finished collecting timing info
[0m22:48:46.198461 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_12hr
[0m22:48:46.209884 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m22:48:46.210497 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:48:46.210710 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:48:56.667287 [debug] [Thread-1  ]: finished collecting timing info
[0m22:48:56.669988 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106277910>]}
[0m22:48:56.670635 [info ] [Thread-1  ]: 22 of 38 OK created model model dbt_anomaly_detection.derived_models_1mon_12hr . [[32mOK[0m in 10.48s]
[0m22:48:56.671116 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_12hr
[0m22:48:56.671315 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_24hr
[0m22:48:56.671656 [info ] [Thread-1  ]: 23 of 38 START model model dbt_anomaly_detection.derived_models_1mon_24hr ...... [RUN]
[0m22:48:56.672192 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_24hr"
[0m22:48:56.672357 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_24hr
[0m22:48:56.672507 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_24hr
[0m22:48:56.679629 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m22:48:56.680223 [debug] [Thread-1  ]: finished collecting timing info
[0m22:48:56.680366 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_24hr
[0m22:48:56.688438 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m22:48:56.689373 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:48:56.689580 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:49:07.171644 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:07.173290 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106414520>]}
[0m22:49:07.173765 [info ] [Thread-1  ]: 23 of 38 OK created model model dbt_anomaly_detection.derived_models_1mon_24hr . [[32mOK[0m in 10.50s]
[0m22:49:07.174224 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_24hr
[0m22:49:07.174453 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_12hr
[0m22:49:07.174674 [info ] [Thread-1  ]: 24 of 38 START model model dbt_anomaly_detection.derived_models_2mon_12hr ...... [RUN]
[0m22:49:07.175441 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_12hr"
[0m22:49:07.175721 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_12hr
[0m22:49:07.175897 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_12hr
[0m22:49:07.183630 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m22:49:07.184748 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:07.184905 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_12hr
[0m22:49:07.187930 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m22:49:07.188319 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:49:07.188504 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:49:17.729795 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:17.730196 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106295160>]}
[0m22:49:17.730405 [info ] [Thread-1  ]: 24 of 38 OK created model model dbt_anomaly_detection.derived_models_2mon_12hr . [[32mOK[0m in 10.55s]
[0m22:49:17.730610 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_12hr
[0m22:49:17.730706 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_24hr
[0m22:49:17.730964 [info ] [Thread-1  ]: 25 of 38 START model model dbt_anomaly_detection.derived_models_2mon_24hr ...... [RUN]
[0m22:49:17.731235 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_24hr"
[0m22:49:17.731312 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_24hr
[0m22:49:17.731380 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_24hr
[0m22:49:17.734879 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m22:49:17.735797 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:17.735941 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_24hr
[0m22:49:17.737854 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m22:49:17.738645 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:49:17.738823 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
CURRENT_DATE() AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:49:27.694999 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:27.696508 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064762b0>]}
[0m22:49:27.921667 [info ] [Thread-1  ]: 25 of 38 OK created model model dbt_anomaly_detection.derived_models_2mon_24hr . [[32mOK[0m in 9.97s]
[0m22:49:27.923625 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_24hr
[0m22:49:27.924795 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ml_detect
[0m22:49:27.925181 [info ] [Thread-1  ]: 26 of 38 START table model dbt_anomaly_detection.derived_ml_detect ............. [RUN]
[0m22:49:27.925704 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_ml_detect"
[0m22:49:27.925840 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_ml_detect
[0m22:49:27.925957 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_ml_detect
[0m22:49:27.942417 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_ml_detect"
[0m22:49:27.943301 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:27.943425 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_ml_detect
[0m22:49:27.945726 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:49:28.151635 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_ml_detect"
[0m22:49:28.155150 [debug] [Thread-1  ]: On model.anomaly_detection.derived_ml_detect: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_ml_detect"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_ml_detect`
  
  
  OPTIONS()
  as (
    -- 2 periods of training * 2 probabality threshold * 4 aggregation levels 


  WITH test_set AS (
  SELECT
  time_stamps,
        event_count,
        app_event,
        agg_tag
      FROM
        `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`  
      WHERE (agg_tag = "4hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 10 DAY)) 
      OR (agg_tag = "8hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 10 DAY))
      OR (agg_tag = "12hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 15 DAY))
      OR (agg_tag = "24hr" AND DATE(time_stamps) >= DATE_SUB(current_date(), INTERVAL 15 DAY))
  )

  
  

    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
       

  
  UNION ALL
  



    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
       

  
  ORDER BY
  agg_tag,
  time_stamps,
  app_event,
  prob_threshold,
  training_period
  


  );
  
[0m22:49:31.112751 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:31.113619 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10621ab20>]}
[0m22:49:31.114336 [info ] [Thread-1  ]: 26 of 38 OK created table model dbt_anomaly_detection.derived_ml_detect ........ [[32mCREATE TABLE (1.5k rows, 423.3 KB processed)[0m in 3.19s]
[0m22:49:31.114803 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ml_detect
[0m22:49:31.115535 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ml_detect_tweaked
[0m22:49:31.115945 [info ] [Thread-1  ]: 27 of 38 START table model dbt_anomaly_detection.ml_detect_tweaked ............. [RUN]
[0m22:49:31.116469 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ml_detect_tweaked"
[0m22:49:31.116614 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ml_detect_tweaked
[0m22:49:31.116757 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ml_detect_tweaked
[0m22:49:31.120338 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ml_detect_tweaked"
[0m22:49:31.121760 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:31.121940 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ml_detect_tweaked
[0m22:49:31.124175 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:49:31.387958 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.ml_detect_tweaked"
[0m22:49:31.388495 [debug] [Thread-1  ]: On model.anomaly_detection.ml_detect_tweaked: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.ml_detect_tweaked"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked`
  
  
  OPTIONS()
  as (
     
  
  with neg_bound_reset as (
SELECT app_event, agg_tag, time_stamps, prob_threshold, training_period, event_count, 
  IF(lower_bound<0, 2, 0.77*lower_bound) AS lower_bound, 1.3*upper_bound AS upper_bound, anomaly_probability, is_anomaly
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ml_detect` )

SELECT app_event, agg_tag, time_stamps, prob_threshold, training_period, event_count, 
  lower_bound, upper_bound, anomaly_probability,
  (upper_bound < event_count OR event_count < lower_bound) AS is_anomaly
FROM neg_bound_reset
  );
  
[0m22:49:33.673842 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:33.676151 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064b98e0>]}
[0m22:49:33.676711 [info ] [Thread-1  ]: 27 of 38 OK created table model dbt_anomaly_detection.ml_detect_tweaked ........ [[32mCREATE TABLE (1.5k rows, 137.0 KB processed)[0m in 2.56s]
[0m22:49:33.677262 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ml_detect_tweaked
[0m22:49:33.678018 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_model_features_dbt
[0m22:49:33.678264 [info ] [Thread-1  ]: 28 of 38 START table model dbt_anomaly_detection.derived_model_features_dbt .... [RUN]
[0m22:49:33.678776 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_model_features_dbt"
[0m22:49:33.678929 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_model_features_dbt
[0m22:49:33.679065 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_model_features_dbt
[0m22:49:33.683036 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_model_features_dbt"
[0m22:49:33.683914 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:33.684239 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_model_features_dbt
[0m22:49:33.686622 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:49:33.923629 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_model_features_dbt"
[0m22:49:33.924166 [debug] [Thread-1  ]: On model.anomaly_detection.derived_model_features_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_model_features_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_model_features_dbt`
  
  
  OPTIONS()
  as (
    
  
  SELECT
    app_event,
    CONCAT(agg_tag, '_', prob_threshold, "threshold", '_', RTRIM(LTRIM(training_period, "derived_models_"), CONCAT('_', agg_tag))) AS control_config,
    SUM(CASE WHEN is_anomaly = TRUE THEN 1 ELSE 0 END) AS anomalies,
    SQRT(AVG( POWER(upper_bound - lower_bound, 2) ) ) / AVG(lower_bound) AS RMSD_prcnt,
    SUM(CASE WHEN lower_bound < 0 THEN 1 ELSE 0 END) AS neg_lower
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked` AS all_configs
  GROUP BY
    app_event,
    control_config
  ORDER BY
    control_config,
    app_event
  );
  
[0m22:49:35.922853 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:35.923846 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064b0ee0>]}
[0m22:49:35.924371 [info ] [Thread-1  ]: 28 of 38 OK created table model dbt_anomaly_detection.derived_model_features_dbt  [[32mCREATE TABLE (48.0 rows, 103.8 KB processed)[0m in 2.25s]
[0m22:49:35.924899 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_model_features_dbt
[0m22:49:35.925852 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_model_features_dbt
[0m22:49:35.926221 [info ] [Thread-1  ]: 29 of 38 START table model dbt_anomaly_detection.filtered_model_features_dbt ... [RUN]
[0m22:49:35.926939 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_model_features_dbt"
[0m22:49:35.927163 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_model_features_dbt
[0m22:49:35.927346 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_model_features_dbt
[0m22:49:35.934302 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_model_features_dbt"
[0m22:49:35.935100 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:35.935255 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_model_features_dbt
[0m22:49:35.938500 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:49:36.146779 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.filtered_model_features_dbt"
[0m22:49:36.147361 [debug] [Thread-1  ]: On model.anomaly_detection.filtered_model_features_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.filtered_model_features_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
  
  
  OPTIONS()
  as (
    
SELECT features.app_event, control_config, anomalies, RMSD_prcnt, neg_lower
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events` AS non_recent
INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`derived_model_features_dbt` AS features
  ON non_recent.app_event = features.app_event
  );
  
[0m22:49:38.516379 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:38.524374 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064b07f0>]}
[0m22:49:38.524686 [info ] [Thread-1  ]: 29 of 38 OK created table model dbt_anomaly_detection.filtered_model_features_dbt  [[32mCREATE TABLE (48.0 rows, 3.2 KB processed)[0m in 2.60s]
[0m22:49:38.524925 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_model_features_dbt
[0m22:49:38.525320 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ref_distinct_tuples
[0m22:49:38.525498 [info ] [Thread-1  ]: 30 of 38 START table model dbt_anomaly_detection.ref_distinct_tuples ........... [RUN]
[0m22:49:38.525732 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ref_distinct_tuples"
[0m22:49:38.525817 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ref_distinct_tuples
[0m22:49:38.525890 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ref_distinct_tuples
[0m22:49:38.528164 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ref_distinct_tuples"
[0m22:49:38.528524 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:38.528605 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ref_distinct_tuples
[0m22:49:38.530134 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:49:38.772679 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.ref_distinct_tuples"
[0m22:49:38.773553 [debug] [Thread-1  ]: On model.anomaly_detection.ref_distinct_tuples: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.ref_distinct_tuples"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`ref_distinct_tuples`
  
  
  OPTIONS()
  as (
    
SELECT DISTINCT app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
WHERE app_event LIKE '%add%'
UNION ALL
SELECT DISTINCT app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
WHERE app_event NOT LIKE '%ad%'
ORDER BY app_event
  );
  
[0m22:49:40.750892 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:40.751632 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106423550>]}
[0m22:49:40.752001 [info ] [Thread-1  ]: 30 of 38 OK created table model dbt_anomaly_detection.ref_distinct_tuples ...... [[32mCREATE TABLE (2.0 rows, 720.0 Bytes processed)[0m in 2.23s]
[0m22:49:40.752371 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ref_distinct_tuples
[0m22:49:40.752541 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_features_null_filtered
[0m22:49:40.752865 [info ] [Thread-1  ]: 31 of 38 START table model dbt_anomaly_detection.remaining_events_features_null_filtered  [RUN]
[0m22:49:40.753344 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_features_null_filtered"
[0m22:49:40.753485 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_features_null_filtered
[0m22:49:40.753634 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_features_null_filtered
[0m22:49:40.757520 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_features_null_filtered"
[0m22:49:40.759399 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:40.759648 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_features_null_filtered
[0m22:49:40.767020 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:49:40.991076 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_features_null_filtered"
[0m22:49:40.993454 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_features_null_filtered: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_features_null_filtered"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
where RMSD_prcnt is not null
  );
  
[0m22:49:43.120559 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:43.121412 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064a4c70>]}
[0m22:49:43.121855 [info ] [Thread-1  ]: 31 of 38 OK created table model dbt_anomaly_detection.remaining_events_features_null_filtered  [[32mCREATE TABLE (48.0 rows, 3.1 KB processed)[0m in 2.37s]
[0m22:49:43.122242 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_features_null_filtered
[0m22:49:43.122906 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies
[0m22:49:43.123290 [info ] [Thread-1  ]: 32 of 38 START table model dbt_anomaly_detection.remaining_events_min_anomalies  [RUN]
[0m22:49:43.123715 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies"
[0m22:49:43.123857 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies
[0m22:49:43.123977 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies
[0m22:49:43.128743 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies"
[0m22:49:43.129652 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:43.129795 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies
[0m22:49:43.131864 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:49:43.356663 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_anomalies"
[0m22:49:43.357807 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies`
  
  
  OPTIONS()
  as (
    

SELECT app_event, MIN(anomalies) AS anomalies 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered`
  GROUP BY app_event
  ORDER BY app_event
  );
  
[0m22:49:45.420065 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:45.420908 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f93940>]}
[0m22:49:45.421346 [info ] [Thread-1  ]: 32 of 38 OK created table model dbt_anomaly_detection.remaining_events_min_anomalies  [[32mCREATE TABLE (2.0 rows, 1.1 KB processed)[0m in 2.30s]
[0m22:49:45.421885 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies
[0m22:49:45.422883 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m22:49:45.423398 [info ] [Thread-1  ]: 33 of 38 START table model dbt_anomaly_detection.remaining_events_min_anomalies_results  [RUN]
[0m22:49:45.423865 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m22:49:45.424018 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies_results
[0m22:49:45.424182 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies_results
[0m22:49:45.428129 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m22:49:45.428828 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:45.428975 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies_results
[0m22:49:45.431595 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:49:45.655978 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m22:49:45.657138 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_anomalies_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_anomalies_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results`
  
  
  OPTIONS()
  as (
    

SELECT neg_lower_criteria_res.app_event, neg_lower_criteria_res.control_config, 
    neg_lower_criteria_res.anomalies, neg_lower_criteria_res.RMSD_prcnt, neg_lower_criteria_res.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered` AS neg_lower_criteria_res
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies` AS min_neg_lower_min_anomalies
    ON neg_lower_criteria_res.anomalies = min_neg_lower_min_anomalies.anomalies
      AND neg_lower_criteria_res.app_event = min_neg_lower_min_anomalies.app_event
  ORDER BY neg_lower_criteria_res.app_event, RMSD_prcnt DESC
  );
  
[0m22:49:47.759835 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:47.761506 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062bd8e0>]}
[0m22:49:47.762003 [info ] [Thread-1  ]: 33 of 38 OK created table model dbt_anomaly_detection.remaining_events_min_anomalies_results  [[32mCREATE TABLE (28.0 rows, 3.2 KB processed)[0m in 2.34s]
[0m22:49:47.762441 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m22:49:47.763447 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD
[0m22:49:47.763919 [info ] [Thread-1  ]: 34 of 38 START table model dbt_anomaly_detection.remaining_events_min_RMSD ..... [RUN]
[0m22:49:47.764439 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD"
[0m22:49:47.764590 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD
[0m22:49:47.764732 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD
[0m22:49:47.771071 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD"
[0m22:49:47.772131 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:47.772446 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD
[0m22:49:47.777419 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:49:47.996295 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_RMSD"
[0m22:49:47.997611 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_RMSD: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_RMSD"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD`
  
  
  OPTIONS()
  as (
    

SELECT app_event, MIN(RMSD_prcnt) AS RMSD_prcnt 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results`
  GROUP BY app_event
  ORDER BY app_event
  );
  
[0m22:49:50.155379 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:50.156100 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105994a30>]}
[0m22:49:50.156500 [info ] [Thread-1  ]: 34 of 38 OK created table model dbt_anomaly_detection.remaining_events_min_RMSD  [[32mCREATE TABLE (2.0 rows, 644.0 Bytes processed)[0m in 2.39s]
[0m22:49:50.156878 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD
[0m22:49:50.157583 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m22:49:50.158037 [info ] [Thread-1  ]: 35 of 38 START table model dbt_anomaly_detection.remaining_events_min_RMSD_results  [RUN]
[0m22:49:50.158461 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m22:49:50.158622 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD_results
[0m22:49:50.158956 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD_results
[0m22:49:50.164655 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m22:49:50.165862 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:50.166000 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD_results
[0m22:49:50.168086 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:49:50.365910 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m22:49:50.368116 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_RMSD_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_RMSD_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
  
  
  OPTIONS()
  as (
    

SELECT anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.control_config, 
    anomalies_criteria_res_dbt.anomalies, anomalies_criteria_res_dbt.RMSD_prcnt, anomalies_criteria_res_dbt.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results` AS anomalies_criteria_res_dbt
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD` AS min_anomalies_max_RMSD
    ON anomalies_criteria_res_dbt.RMSD_prcnt = min_anomalies_max_RMSD.RMSD_prcnt
      AND anomalies_criteria_res_dbt.app_event = min_anomalies_max_RMSD.app_event
  ORDER BY anomalies_criteria_res_dbt.app_event, control_config
  );
  
[0m22:49:52.401491 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:52.401818 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062ddd30>]}
[0m22:49:52.401995 [info ] [Thread-1  ]: 35 of 38 OK created table model dbt_anomaly_detection.remaining_events_min_RMSD_results  [[32mCREATE TABLE (2.0 rows, 1.9 KB processed)[0m in 2.24s]
[0m22:49:52.402184 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m22:49:52.402536 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_all_models
[0m22:49:52.402732 [info ] [Thread-1  ]: 36 of 38 START table model dbt_anomaly_detection.filtered_all_models ........... [RUN]
[0m22:49:52.402964 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_all_models"
[0m22:49:52.403042 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_all_models
[0m22:49:52.403110 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_all_models
[0m22:49:52.405140 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_all_models"
[0m22:49:52.406409 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:52.406520 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_all_models
[0m22:49:52.407931 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:49:52.601635 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.filtered_all_models"
[0m22:49:52.602096 [debug] [Thread-1  ]: On model.anomaly_detection.filtered_all_models: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.filtered_all_models"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`filtered_all_models`
  
  
  OPTIONS()
  as (
    
SELECT app_event, control_config, anomalies, RMSD_prcnt, neg_lower
FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
WHERE app_event LIKE '%add%'
UNION ALL 
SELECT app_event, control_config, anomalies, RMSD_prcnt, neg_lower
FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
WHERE app_event NOT LIKE '%ad%'
ORDER BY app_event
  );
  
[0m22:49:54.594105 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:54.594926 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064a81c0>]}
[0m22:49:54.595396 [info ] [Thread-1  ]: 36 of 38 OK created table model dbt_anomaly_detection.filtered_all_models ...... [[32mCREATE TABLE (2.0 rows, 132.0 Bytes processed)[0m in 2.19s]
[0m22:49:54.595864 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_all_models
[0m22:49:54.596621 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m22:49:54.597127 [info ] [Thread-1  ]: 37 of 38 START table model dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [RUN]
[0m22:49:54.597617 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m22:49:54.597776 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m22:49:54.597931 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m22:49:54.607535 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m22:49:54.608243 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:54.608389 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m22:49:54.610980 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:49:54.842108 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m22:49:54.845001 [debug] [Thread-1  ]: On model.anomaly_detection.derived_alerts_fired_automation_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_alerts_fired_automation_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_alerts_fired_automation_dbt`
  
  
  OPTIONS()
  as (
    
  
  WITH ml_detect_updated AS (
    SELECT app_event, 
      CONCAT(agg_tag, '_', prob_threshold, "threshold", '_', RTRIM(LTRIM(training_period, "derived_models_"), CONCAT('_', agg_tag))) AS control_config,
      time_stamps, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked`
  )
  SELECT time_stamps, all_configs.app_event AS app_event, control_table.control_config AS control_config, 
    anomalies, RMSD_prcnt, neg_lower, event_count, lower_bound, upper_bound, anomaly_probability, is_anomaly
  FROM ml_detect_updated AS all_configs 
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`filtered_all_models` AS control_table 
    ON all_configs.app_event = control_table.app_event
      AND all_configs.control_config = control_table.control_config
  ORDER BY all_configs.app_event, time_stamps
  );
  
[0m22:49:56.804067 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:56.804817 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062a5c10>]}
[0m22:49:56.805157 [info ] [Thread-1  ]: 37 of 38 OK created table model dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [[32mCREATE TABLE (55.0 rows, 138.5 KB processed)[0m in 2.21s]
[0m22:49:56.805502 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m22:49:56.806126 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_events_with_anomalies
[0m22:49:56.806363 [info ] [Thread-1  ]: 38 of 38 START table model dbt_anomaly_detection.derived_events_with_anomalies . [RUN]
[0m22:49:56.806697 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_events_with_anomalies"
[0m22:49:56.806810 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_events_with_anomalies
[0m22:49:56.806912 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_events_with_anomalies
[0m22:49:56.809535 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_events_with_anomalies"
[0m22:49:56.809968 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:56.810092 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_events_with_anomalies
[0m22:49:56.811885 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:49:56.991659 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_events_with_anomalies"
[0m22:49:56.992443 [debug] [Thread-1  ]: On model.anomaly_detection.derived_events_with_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_events_with_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_events_with_anomalies`
  
  
  OPTIONS()
  as (
    
SELECT time_stamps, app_event, event_count AS event_count_anomalous_yesterday
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_alerts_fired_automation_dbt`
WHERE DATE(time_stamps) = CURRENT_DATE() - 1 
  AND is_anomaly
  );
  
[0m22:49:59.009112 [debug] [Thread-1  ]: finished collecting timing info
[0m22:49:59.009505 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a84cd5d-f80c-4ccb-bbee-e9a70b38260b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062e47f0>]}
[0m22:49:59.009740 [info ] [Thread-1  ]: 38 of 38 OK created table model dbt_anomaly_detection.derived_events_with_anomalies  [[32mCREATE TABLE (0.0 rows, 1.7 KB processed)[0m in 2.20s]
[0m22:49:59.009981 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_events_with_anomalies
[0m22:49:59.010778 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m22:49:59.011122 [info ] [MainThread]: 
[0m22:49:59.011280 [info ] [MainThread]: Finished running 26 table models, 12 model models in 0 hours 3 minutes and 37.23 seconds (217.23s).
[0m22:49:59.011417 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:49:59.011487 [debug] [MainThread]: Connection 'model.anomaly_detection.derived_events_with_anomalies' was properly closed.
[0m22:49:59.022944 [info ] [MainThread]: 
[0m22:49:59.023170 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:49:59.023359 [info ] [MainThread]: 
[0m22:49:59.023494 [info ] [MainThread]: Done. PASS=38 WARN=0 ERROR=0 SKIP=0 TOTAL=38
[0m22:49:59.023694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062ddd30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064a81c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105af5250>]}
[0m22:49:59.023834 [debug] [MainThread]: Flushing usage events


============================== 2023-02-10 22:50:58.863320 | 32753e3c-f6a4-45d1-be9a-d1ef903c313f ==============================
[0m22:50:58.863349 [info ] [MainThread]: Running with dbt=1.2.1
[0m22:50:58.864123 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:50:58.864239 [debug] [MainThread]: Tracking: tracking
[0m22:50:58.882373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109eb1b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109eb1f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109eb1880>]}
[0m22:50:58.939581 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 17 files changed.
[0m22:50:58.939880 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_05mon_4hr.sql
[0m22:50:58.940000 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_2mon_8hr.sql
[0m22:50:58.940106 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_05mon_12hr.sql
[0m22:50:58.940203 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_1mon_4hr.sql
[0m22:50:58.940301 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_events_with_anomalies.sql
[0m22:50:58.940396 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_ml_detect.sql
[0m22:50:58.940491 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_1mon_12hr.sql
[0m22:50:58.940592 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_05mon_8hr.sql
[0m22:50:58.940686 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_2mon_4hr.sql
[0m22:50:58.940779 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_nonrecent_events.sql
[0m22:50:58.940875 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/all_agg_derived_cutoff_short.sql
[0m22:50:58.940970 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_05mon_24hr.sql
[0m22:50:58.941065 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_1mon_24hr.sql
[0m22:50:58.941158 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_2mon_24hr.sql
[0m22:50:58.941253 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_2mon_12hr.sql
[0m22:50:58.941345 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_1mon_8hr.sql
[0m22:50:58.941444 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/all_agg_derived_cutoff_long.sql
[0m22:50:58.950093 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_4hr.sql
[0m22:50:58.957395 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_4hr.sql
[0m22:50:58.958121 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_8hr.sql
[0m22:50:58.961201 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_8hr.sql
[0m22:50:58.961882 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_12hr.sql
[0m22:50:58.964525 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_12hr.sql
[0m22:50:58.965115 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_4hr.sql
[0m22:50:58.967666 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_4hr.sql
[0m22:50:58.968190 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_events_with_anomalies.sql
[0m22:50:58.969923 [debug] [MainThread]: 1603: static parser failed on example/derived_ml_detect.sql
[0m22:50:58.977644 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_ml_detect.sql
[0m22:50:58.978212 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_12hr.sql
[0m22:50:58.981423 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_12hr.sql
[0m22:50:58.982020 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_8hr.sql
[0m22:50:58.984917 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_8hr.sql
[0m22:50:58.985548 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_4hr.sql
[0m22:50:58.988147 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_4hr.sql
[0m22:50:58.988673 [debug] [MainThread]: 1699: static parser successfully parsed example/derived_nonrecent_events.sql
[0m22:50:58.989985 [debug] [MainThread]: 1699: static parser successfully parsed example/all_agg_derived_cutoff_short.sql
[0m22:50:58.991146 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_24hr.sql
[0m22:50:58.993898 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_24hr.sql
[0m22:50:58.994456 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_24hr.sql
[0m22:50:58.996979 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_24hr.sql
[0m22:50:58.997523 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_24hr.sql
[0m22:50:59.000535 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_24hr.sql
[0m22:50:59.001106 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_12hr.sql
[0m22:50:59.004387 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_12hr.sql
[0m22:50:59.004961 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_8hr.sql
[0m22:50:59.038442 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_8hr.sql
[0m22:50:59.039159 [debug] [MainThread]: 1699: static parser successfully parsed example/all_agg_derived_cutoff_long.sql
[0m22:50:59.048738 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a1d10d0>]}
[0m22:50:59.055946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a040130>]}
[0m22:50:59.056094 [info ] [MainThread]: Found 38 models, 0 tests, 0 snapshots, 0 analyses, 540 macros, 0 operations, 0 seed files, 2 sources, 0 exposures, 0 metrics
[0m22:50:59.056230 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a040070>]}
[0m22:50:59.057424 [info ] [MainThread]: 
[0m22:50:59.057686 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m22:50:59.058998 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow"
[0m22:50:59.059183 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:50:59.842144 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow_dbt_anomaly_detection"
[0m22:50:59.842855 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:51:00.036774 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a1d1b50>]}
[0m22:51:00.037681 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:51:00.038070 [info ] [MainThread]: 
[0m22:51:00.044713 [debug] [Thread-1  ]: Began running node model.anomaly_detection.reference_derived
[0m22:51:00.045211 [info ] [Thread-1  ]: 1 of 38 START table model dbt_anomaly_detection.reference_derived .............. [RUN]
[0m22:51:00.045756 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.reference_derived"
[0m22:51:00.045932 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.reference_derived
[0m22:51:00.046215 [debug] [Thread-1  ]: Compiling model.anomaly_detection.reference_derived
[0m22:51:00.051336 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.reference_derived"
[0m22:51:00.052072 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:00.052299 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.reference_derived
[0m22:51:00.065617 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:00.326969 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.reference_derived"
[0m22:51:00.328053 [debug] [Thread-1  ]: On model.anomaly_detection.reference_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.reference_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived`
  
  
  OPTIONS()
  as (
    
SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`web_purchase_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB("2023-02-09", INTERVAL 90 DAY) AND DATE(collector_tstamp) < "2023-02-09"

UNION ALL

SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`pco_web_apply_filter_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB("2023-02-09", INTERVAL 90 DAY) AND DATE(collector_tstamp) < "2023-02-09"
  );
  
[0m22:51:03.908140 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:03.908591 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a122280>]}
[0m22:51:03.908855 [info ] [Thread-1  ]: 1 of 38 OK created table model dbt_anomaly_detection.reference_derived ......... [[32mCREATE TABLE (1.1m rows, 34.8 MB processed)[0m in 3.86s]
[0m22:51:03.909134 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.reference_derived
[0m22:51:03.909833 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived
[0m22:51:03.910005 [info ] [Thread-1  ]: 2 of 38 START table model dbt_anomaly_detection.all_agg_derived ................ [RUN]
[0m22:51:03.910291 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived"
[0m22:51:03.910391 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived
[0m22:51:03.910488 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived
[0m22:51:03.914347 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived"
[0m22:51:03.914745 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:03.914868 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived
[0m22:51:03.916655 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:04.115530 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived"
[0m22:51:04.116406 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
  
  
  OPTIONS()
  as (
    


SELECT "4hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_4hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/4)*4 AS _4hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _4hr_trunc,
    app_id,
    event_type)

UNION ALL


SELECT "8hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_8hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/8)*8 AS _8hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _8hr_trunc,
    app_id,
    event_type)

UNION ALL


SELECT "12hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_12hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/12)*12 AS _12hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _12hr_trunc,
    app_id,
    event_type)

UNION ALL


SELECT "24hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_24hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/24)*24 AS _24hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _24hr_trunc,
    app_id,
    event_type)

ORDER BY
  agg_tag,
  time_stamps,
  app_id,
  event_type


  );
  
[0m22:51:08.226384 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:08.227366 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2bf7c0>]}
[0m22:51:08.227886 [info ] [Thread-1  ]: 2 of 38 OK created table model dbt_anomaly_detection.all_agg_derived ........... [[32mCREATE TABLE (2.2k rows, 34.8 MB processed)[0m in 4.32s]
[0m22:51:08.228417 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived
[0m22:51:08.229598 [debug] [Thread-1  ]: Began running node model.anomaly_detection.count_cutoff
[0m22:51:08.230277 [info ] [Thread-1  ]: 3 of 38 START table model dbt_anomaly_detection.count_cutoff ................... [RUN]
[0m22:51:08.231225 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.count_cutoff"
[0m22:51:08.237263 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.count_cutoff
[0m22:51:08.237748 [debug] [Thread-1  ]: Compiling model.anomaly_detection.count_cutoff
[0m22:51:08.245001 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.count_cutoff"
[0m22:51:08.255436 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:08.255804 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.count_cutoff
[0m22:51:08.261344 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:08.481469 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.count_cutoff"
[0m22:51:08.483373 [debug] [Thread-1  ]: On model.anomaly_detection.count_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.count_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select agg_tag, app_event, min(time_stamps) as strt_time
from pairs
where event_count > 50
group by app_event, agg_tag 
order by app_event, agg_tag
  );
  
[0m22:51:10.573406 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:10.575507 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2476d0>]}
[0m22:51:10.576449 [info ] [Thread-1  ]: 3 of 38 OK created table model dbt_anomaly_detection.count_cutoff .............. [[32mCREATE TABLE (8.0 rows, 78.6 KB processed)[0m in 2.34s]
[0m22:51:10.581769 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.count_cutoff
[0m22:51:10.584098 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff
[0m22:51:10.584746 [info ] [Thread-1  ]: 4 of 38 START table model dbt_anomaly_detection.all_agg_derived_cutoff ......... [RUN]
[0m22:51:10.585883 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff"
[0m22:51:10.586324 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff
[0m22:51:10.586713 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff
[0m22:51:10.597426 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m22:51:10.598497 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:10.598737 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff
[0m22:51:10.602690 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:10.804225 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m22:51:10.806345 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select time_stamps, app_event, agg_tag, event_count
from(
select time_stamps, strt_time, pairs.app_event, pairs.agg_tag, event_count
from pairs
inner join `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff` as cutoff
on pairs.agg_tag = cutoff.agg_tag
and pairs.app_event = cutoff.app_event
order by pairs.app_event, pairs.agg_tag, time_stamps)
where time_stamps >= strt_time
  );
  
[0m22:51:13.139540 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:13.142155 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2d91c0>]}
[0m22:51:13.143258 [info ] [Thread-1  ]: 4 of 38 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff .... [[32mCREATE TABLE (2.2k rows, 78.8 KB processed)[0m in 2.56s]
[0m22:51:13.144054 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff
[0m22:51:13.145706 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m22:51:13.146275 [info ] [Thread-1  ]: 5 of 38 START table model dbt_anomaly_detection.all_agg_derived_cutoff_long .... [RUN]
[0m22:51:13.147074 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m22:51:13.147651 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_long
[0m22:51:13.147994 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_long
[0m22:51:13.155287 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m22:51:13.157513 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:13.157755 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_long
[0m22:51:13.163966 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:13.409229 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m22:51:13.414006 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB("2023-02-09", INTERVAL 10 DAY)
  );
  
[0m22:51:15.431415 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:15.433811 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a309520>]}
[0m22:51:15.434876 [info ] [Thread-1  ]: 5 of 38 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_long  [[32mCREATE TABLE (1.9k rows, 76.5 KB processed)[0m in 2.29s]
[0m22:51:15.436773 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m22:51:15.437984 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m22:51:15.438328 [info ] [Thread-1  ]: 6 of 38 START table model dbt_anomaly_detection.all_agg_derived_cutoff_short ... [RUN]
[0m22:51:15.439027 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m22:51:15.440678 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_short
[0m22:51:15.440955 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_short
[0m22:51:15.447736 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m22:51:15.449082 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:15.449189 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_short
[0m22:51:15.451216 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:15.649502 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m22:51:15.652165 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB("2023-02-09", INTERVAL 15 DAY)
  );
  
[0m22:51:17.767043 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:17.769849 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a24fca0>]}
[0m22:51:17.771047 [info ] [Thread-1  ]: 6 of 38 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_short  [[32mCREATE TABLE (1.8k rows, 76.5 KB processed)[0m in 2.33s]
[0m22:51:17.771918 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m22:51:17.772280 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_nonrecent_events
[0m22:51:17.772681 [info ] [Thread-1  ]: 7 of 38 START table model dbt_anomaly_detection.derived_nonrecent_events ....... [RUN]
[0m22:51:17.774103 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_nonrecent_events"
[0m22:51:17.774550 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_nonrecent_events
[0m22:51:17.774871 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_nonrecent_events
[0m22:51:17.785869 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m22:51:17.791049 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:17.791200 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_nonrecent_events
[0m22:51:17.793300 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:18.012588 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m22:51:18.014082 [debug] [Thread-1  ]: On model.anomaly_detection.derived_nonrecent_events: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_nonrecent_events"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events`
  
  
  OPTIONS()
  as (
    

SELECT MIN(time_stamps) AS strt_time, app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
GROUP BY app_event
HAVING DATE(MIN(time_stamps)) < DATE_SUB("2023-02-09", INTERVAL 30 DAY)
ORDER BY strt_time
  );
  
[0m22:51:20.036613 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:20.037588 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a33a700>]}
[0m22:51:20.038149 [info ] [Thread-1  ]: 7 of 38 OK created table model dbt_anomaly_detection.derived_nonrecent_events .. [[32mCREATE TABLE (2.0 rows, 48.5 KB processed)[0m in 2.26s]
[0m22:51:20.038645 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_nonrecent_events
[0m22:51:20.038859 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_long
[0m22:51:20.039110 [info ] [Thread-1  ]: 8 of 38 START table model dbt_anomaly_detection.aggregation_quartiles_long ..... [RUN]
[0m22:51:20.040114 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_long"
[0m22:51:20.040384 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_long
[0m22:51:20.040568 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_long
[0m22:51:20.048919 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m22:51:20.049696 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:20.049951 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_long
[0m22:51:20.054176 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:20.279400 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m22:51:20.281512 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m22:51:22.280834 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:22.281799 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2f4040>]}
[0m22:51:22.282388 [info ] [Thread-1  ]: 8 of 38 OK created table model dbt_anomaly_detection.aggregation_quartiles_long  [[32mCREATE TABLE (8.0 rows, 53.0 KB processed)[0m in 2.24s]
[0m22:51:22.282992 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_long
[0m22:51:22.283260 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_short
[0m22:51:22.283551 [info ] [Thread-1  ]: 9 of 38 START table model dbt_anomaly_detection.aggregation_quartiles_short .... [RUN]
[0m22:51:22.284382 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_short"
[0m22:51:22.284921 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_short
[0m22:51:22.285162 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_short
[0m22:51:22.290243 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m22:51:22.291130 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:22.291311 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_short
[0m22:51:22.294270 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:22.525340 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m22:51:22.526693 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m22:51:24.549561 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:24.550545 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2cbd00>]}
[0m22:51:24.551109 [info ] [Thread-1  ]: 9 of 38 OK created table model dbt_anomaly_detection.aggregation_quartiles_short  [[32mCREATE TABLE (8.0 rows, 49.7 KB processed)[0m in 2.27s]
[0m22:51:24.551667 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_short
[0m22:51:24.551931 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_long
[0m22:51:24.552445 [info ] [Thread-1  ]: 10 of 38 START table model dbt_anomaly_detection.aggregation_bounds_long ....... [RUN]
[0m22:51:24.553350 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_long"
[0m22:51:24.553574 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_long
[0m22:51:24.553759 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_long
[0m22:51:24.560452 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m22:51:24.562383 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:24.562631 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_long
[0m22:51:24.565630 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:24.757762 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m22:51:24.759194 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m22:51:26.896405 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:26.897383 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2d91c0>]}
[0m22:51:26.897921 [info ] [Thread-1  ]: 10 of 38 OK created table model dbt_anomaly_detection.aggregation_bounds_long .. [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.34s]
[0m22:51:26.898477 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_long
[0m22:51:26.898729 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_short
[0m22:51:26.899121 [info ] [Thread-1  ]: 11 of 38 START table model dbt_anomaly_detection.aggregation_bounds_short ...... [RUN]
[0m22:51:26.900020 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_short"
[0m22:51:26.900219 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_short
[0m22:51:26.900389 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_short
[0m22:51:26.909776 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m22:51:26.911879 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:26.912031 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_short
[0m22:51:26.914257 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:27.097460 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m22:51:27.099772 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m22:51:29.390054 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:29.391413 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a3b1fd0>]}
[0m22:51:29.391893 [info ] [Thread-1  ]: 11 of 38 OK created table model dbt_anomaly_detection.aggregation_bounds_short . [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.49s]
[0m22:51:29.392301 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_short
[0m22:51:29.392477 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_long
[0m22:51:29.392810 [info ] [Thread-1  ]: 12 of 38 START table model dbt_anomaly_detection.aggregation_outliers_long ..... [RUN]
[0m22:51:29.393759 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_long"
[0m22:51:29.393960 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_long
[0m22:51:29.394106 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_long
[0m22:51:29.397978 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m22:51:29.400230 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:29.400494 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_long
[0m22:51:29.402943 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:29.651943 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m22:51:29.653128 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m22:51:31.750316 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:31.751269 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2bf760>]}
[0m22:51:31.751792 [info ] [Thread-1  ]: 12 of 38 OK created table model dbt_anomaly_detection.aggregation_outliers_long  [[32mCREATE TABLE (1.9k rows, 68.3 KB processed)[0m in 2.36s]
[0m22:51:31.752327 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_long
[0m22:51:31.752544 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_short
[0m22:51:31.752775 [info ] [Thread-1  ]: 13 of 38 START table model dbt_anomaly_detection.aggregation_outliers_short .... [RUN]
[0m22:51:31.753982 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_short"
[0m22:51:31.754329 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_short
[0m22:51:31.754515 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_short
[0m22:51:31.759384 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m22:51:31.760137 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:31.760303 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_short
[0m22:51:31.762934 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:31.978255 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m22:51:31.980420 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m22:51:34.014246 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:34.015218 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a317640>]}
[0m22:51:34.015699 [info ] [Thread-1  ]: 13 of 38 OK created table model dbt_anomaly_detection.aggregation_outliers_short  [[32mCREATE TABLE (1.8k rows, 64.0 KB processed)[0m in 2.26s]
[0m22:51:34.016187 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_short
[0m22:51:34.016446 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_4hr
[0m22:51:34.016875 [info ] [Thread-1  ]: 14 of 38 START model model dbt_anomaly_detection.derived_models_05mon_4hr ...... [RUN]
[0m22:51:34.017657 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_4hr"
[0m22:51:34.017826 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_4hr
[0m22:51:34.017996 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_4hr
[0m22:51:34.026299 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m22:51:34.027931 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:34.028169 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_4hr
[0m22:51:34.045646 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m22:51:34.046675 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:34.046840 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:51:34.785842 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m22:51:36.648824 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:36.650295 [debug] [Thread-1  ]: Database Error in model derived_models_05mon_4hr (models/example/derived_models_05mon_4hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_4hr.sql
[0m22:51:36.650810 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2d95e0>]}
[0m22:51:36.651346 [error] [Thread-1  ]: 14 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_05mon_4hr  [[31mERROR[0m in 2.63s]
[0m22:51:36.651918 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_4hr
[0m22:51:36.652175 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_8hr
[0m22:51:36.652455 [info ] [Thread-1  ]: 15 of 38 START model model dbt_anomaly_detection.derived_models_05mon_8hr ...... [RUN]
[0m22:51:36.653374 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_8hr"
[0m22:51:36.653594 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_8hr
[0m22:51:36.653772 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_8hr
[0m22:51:36.660043 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m22:51:36.661904 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:36.662097 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_8hr
[0m22:51:36.665727 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m22:51:36.666299 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:36.666501 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:51:37.749443 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m22:51:39.679442 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:39.681722 [debug] [Thread-1  ]: Database Error in model derived_models_05mon_8hr (models/example/derived_models_05mon_8hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_8hr.sql
[0m22:51:39.682589 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a3ef550>]}
[0m22:51:39.683819 [error] [Thread-1  ]: 15 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_05mon_8hr  [[31mERROR[0m in 3.03s]
[0m22:51:39.684797 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_8hr
[0m22:51:39.685042 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_4hr
[0m22:51:39.685318 [info ] [Thread-1  ]: 16 of 38 START model model dbt_anomaly_detection.derived_models_1mon_4hr ....... [RUN]
[0m22:51:39.685879 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_4hr"
[0m22:51:39.686273 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_4hr
[0m22:51:39.686473 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_4hr
[0m22:51:39.693016 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m22:51:39.694478 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:39.694622 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_4hr
[0m22:51:39.697338 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m22:51:39.697906 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:39.698083 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:51:40.418715 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m22:51:42.447642 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:42.450750 [debug] [Thread-1  ]: Database Error in model derived_models_1mon_4hr (models/example/derived_models_1mon_4hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_4hr.sql
[0m22:51:42.451357 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a318f70>]}
[0m22:51:42.451921 [error] [Thread-1  ]: 16 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_1mon_4hr  [[31mERROR[0m in 2.77s]
[0m22:51:42.452494 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_4hr
[0m22:51:42.452767 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_8hr
[0m22:51:42.453054 [info ] [Thread-1  ]: 17 of 38 START model model dbt_anomaly_detection.derived_models_1mon_8hr ....... [RUN]
[0m22:51:42.453984 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_8hr"
[0m22:51:42.454262 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_8hr
[0m22:51:42.454482 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_8hr
[0m22:51:42.460739 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m22:51:42.461494 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:42.461675 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_8hr
[0m22:51:42.465051 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m22:51:42.465806 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:42.466036 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:51:43.453237 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m22:51:45.308934 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:45.310746 [debug] [Thread-1  ]: Database Error in model derived_models_1mon_8hr (models/example/derived_models_1mon_8hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_8hr.sql
[0m22:51:45.311094 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a45bf10>]}
[0m22:51:45.311431 [error] [Thread-1  ]: 17 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_1mon_8hr  [[31mERROR[0m in 2.86s]
[0m22:51:45.311755 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_8hr
[0m22:51:45.311892 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_4hr
[0m22:51:45.312299 [info ] [Thread-1  ]: 18 of 38 START model model dbt_anomaly_detection.derived_models_2mon_4hr ....... [RUN]
[0m22:51:45.312834 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_4hr"
[0m22:51:45.313017 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_4hr
[0m22:51:45.313184 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_4hr
[0m22:51:45.318359 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m22:51:45.319398 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:45.319539 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_4hr
[0m22:51:45.322476 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m22:51:45.322831 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:45.322971 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:51:46.452822 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m22:51:47.626814 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:47.627826 [debug] [Thread-1  ]: Database Error in model derived_models_2mon_4hr (models/example/derived_models_2mon_4hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_4hr.sql
[0m22:51:47.628177 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a4a7640>]}
[0m22:51:47.628548 [error] [Thread-1  ]: 18 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_2mon_4hr  [[31mERROR[0m in 2.32s]
[0m22:51:47.628870 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_4hr
[0m22:51:47.629033 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_8hr
[0m22:51:47.629550 [info ] [Thread-1  ]: 19 of 38 START model model dbt_anomaly_detection.derived_models_2mon_8hr ....... [RUN]
[0m22:51:47.630237 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_8hr"
[0m22:51:47.630465 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_8hr
[0m22:51:47.630676 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_8hr
[0m22:51:47.636184 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m22:51:47.636663 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:47.636773 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_8hr
[0m22:51:47.639229 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m22:51:47.639681 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:47.639826 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:51:53.790936 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m22:51:55.106496 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:55.108657 [debug] [Thread-1  ]: Database Error in model derived_models_2mon_8hr (models/example/derived_models_2mon_8hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_8hr.sql
[0m22:51:55.109298 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a42be80>]}
[0m22:51:55.109895 [error] [Thread-1  ]: 19 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_2mon_8hr  [[31mERROR[0m in 7.48s]
[0m22:51:55.110494 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_8hr
[0m22:51:55.110773 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_12hr
[0m22:51:55.111342 [info ] [Thread-1  ]: 20 of 38 START model model dbt_anomaly_detection.derived_models_05mon_12hr ..... [RUN]
[0m22:51:55.112067 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_12hr"
[0m22:51:55.112293 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_12hr
[0m22:51:55.112481 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_12hr
[0m22:51:55.118824 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m22:51:55.121425 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:55.121600 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_12hr
[0m22:51:55.124980 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m22:51:55.125860 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:55.126083 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:51:56.077441 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m22:51:57.206169 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:57.207340 [debug] [Thread-1  ]: Database Error in model derived_models_05mon_12hr (models/example/derived_models_05mon_12hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_12hr.sql
[0m22:51:57.207901 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a504fa0>]}
[0m22:51:57.208397 [error] [Thread-1  ]: 20 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_05mon_12hr  [[31mERROR[0m in 2.10s]
[0m22:51:57.208945 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_12hr
[0m22:51:57.209227 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_24hr
[0m22:51:57.209533 [info ] [Thread-1  ]: 21 of 38 START model model dbt_anomaly_detection.derived_models_05mon_24hr ..... [RUN]
[0m22:51:57.210078 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_24hr"
[0m22:51:57.210305 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_24hr
[0m22:51:57.210407 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_24hr
[0m22:51:57.213025 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m22:51:57.213712 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:57.213802 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_24hr
[0m22:51:57.216212 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m22:51:57.216777 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:57.216948 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:51:58.145267 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m22:51:59.887917 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:59.890147 [debug] [Thread-1  ]: Database Error in model derived_models_05mon_24hr (models/example/derived_models_05mon_24hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_24hr.sql
[0m22:51:59.891014 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a4db910>]}
[0m22:51:59.892204 [error] [Thread-1  ]: 21 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_05mon_24hr  [[31mERROR[0m in 2.68s]
[0m22:51:59.892998 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_24hr
[0m22:51:59.893236 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_12hr
[0m22:51:59.893515 [info ] [Thread-1  ]: 22 of 38 START model model dbt_anomaly_detection.derived_models_1mon_12hr ...... [RUN]
[0m22:51:59.894260 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_12hr"
[0m22:51:59.894431 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_12hr
[0m22:51:59.894575 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_12hr
[0m22:51:59.899587 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m22:51:59.901601 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:59.901742 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_12hr
[0m22:51:59.904185 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m22:51:59.904924 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:51:59.905087 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:52:00.994528 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m22:52:07.339331 [debug] [Thread-1  ]: finished collecting timing info
[0m22:52:07.341568 [debug] [Thread-1  ]: Database Error in model derived_models_1mon_12hr (models/example/derived_models_1mon_12hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_12hr.sql
[0m22:52:07.341987 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a56af10>]}
[0m22:52:07.342518 [error] [Thread-1  ]: 22 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_1mon_12hr  [[31mERROR[0m in 7.45s]
[0m22:52:07.343044 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_12hr
[0m22:52:07.343255 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_24hr
[0m22:52:07.343680 [info ] [Thread-1  ]: 23 of 38 START model model dbt_anomaly_detection.derived_models_1mon_24hr ...... [RUN]
[0m22:52:07.344244 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_24hr"
[0m22:52:07.344431 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_24hr
[0m22:52:07.344604 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_24hr
[0m22:52:07.350344 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m22:52:07.351626 [debug] [Thread-1  ]: finished collecting timing info
[0m22:52:07.351786 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_24hr
[0m22:52:07.354951 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m22:52:07.355351 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:52:07.355521 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:52:08.355255 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m22:52:09.660936 [debug] [Thread-1  ]: finished collecting timing info
[0m22:52:09.661982 [debug] [Thread-1  ]: Database Error in model derived_models_1mon_24hr (models/example/derived_models_1mon_24hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_24hr.sql
[0m22:52:09.662540 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a5aca90>]}
[0m22:52:09.663123 [error] [Thread-1  ]: 23 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_1mon_24hr  [[31mERROR[0m in 2.32s]
[0m22:52:09.663669 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_24hr
[0m22:52:09.663916 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_12hr
[0m22:52:09.664187 [info ] [Thread-1  ]: 24 of 38 START model model dbt_anomaly_detection.derived_models_2mon_12hr ...... [RUN]
[0m22:52:09.664725 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_12hr"
[0m22:52:09.665114 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_12hr
[0m22:52:09.665268 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_12hr
[0m22:52:09.668660 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m22:52:09.669486 [debug] [Thread-1  ]: finished collecting timing info
[0m22:52:09.669645 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_12hr
[0m22:52:09.671457 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m22:52:09.672107 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:52:09.672232 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:52:10.786897 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m22:52:12.675105 [debug] [Thread-1  ]: finished collecting timing info
[0m22:52:12.676886 [debug] [Thread-1  ]: Database Error in model derived_models_2mon_12hr (models/example/derived_models_2mon_12hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_12hr.sql
[0m22:52:12.677531 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a57abe0>]}
[0m22:52:12.678115 [error] [Thread-1  ]: 24 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_2mon_12hr  [[31mERROR[0m in 3.01s]
[0m22:52:12.678652 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_12hr
[0m22:52:12.678888 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_24hr
[0m22:52:12.679336 [info ] [Thread-1  ]: 25 of 38 START model model dbt_anomaly_detection.derived_models_2mon_24hr ...... [RUN]
[0m22:52:12.679813 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_24hr"
[0m22:52:12.679986 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_24hr
[0m22:52:12.680156 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_24hr
[0m22:52:12.686455 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m22:52:12.687272 [debug] [Thread-1  ]: finished collecting timing info
[0m22:52:12.687433 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_24hr
[0m22:52:12.690931 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m22:52:12.691536 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:52:12.691765 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m22:52:13.653611 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m22:52:14.620732 [debug] [Thread-1  ]: finished collecting timing info
[0m22:52:14.621569 [debug] [Thread-1  ]: Database Error in model derived_models_2mon_24hr (models/example/derived_models_2mon_24hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_24hr.sql
[0m22:52:14.621908 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32753e3c-f6a4-45d1-be9a-d1ef903c313f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a60f910>]}
[0m22:52:14.843539 [error] [Thread-1  ]: 25 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_2mon_24hr  [[31mERROR[0m in 1.94s]
[0m22:52:14.844407 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_24hr
[0m22:52:14.845287 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ml_detect
[0m22:52:14.845533 [info ] [Thread-1  ]: 26 of 38 SKIP relation dbt_anomaly_detection.derived_ml_detect ................. [[33mSKIP[0m]
[0m22:52:14.845882 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ml_detect
[0m22:52:14.846285 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ml_detect_tweaked
[0m22:52:14.846610 [info ] [Thread-1  ]: 27 of 38 SKIP relation dbt_anomaly_detection.ml_detect_tweaked ................. [[33mSKIP[0m]
[0m22:52:14.846893 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ml_detect_tweaked
[0m22:52:14.847374 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_model_features_dbt
[0m22:52:14.847499 [info ] [Thread-1  ]: 28 of 38 SKIP relation dbt_anomaly_detection.derived_model_features_dbt ........ [[33mSKIP[0m]
[0m22:52:14.847734 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_model_features_dbt
[0m22:52:14.848046 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_model_features_dbt
[0m22:52:14.848297 [info ] [Thread-1  ]: 29 of 38 SKIP relation dbt_anomaly_detection.filtered_model_features_dbt ....... [[33mSKIP[0m]
[0m22:52:14.848629 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_model_features_dbt
[0m22:52:14.849046 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ref_distinct_tuples
[0m22:52:14.849299 [info ] [Thread-1  ]: 30 of 38 SKIP relation dbt_anomaly_detection.ref_distinct_tuples ............... [[33mSKIP[0m]
[0m22:52:14.849754 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ref_distinct_tuples
[0m22:52:14.849912 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_features_null_filtered
[0m22:52:14.850039 [info ] [Thread-1  ]: 31 of 38 SKIP relation dbt_anomaly_detection.remaining_events_features_null_filtered  [[33mSKIP[0m]
[0m22:52:14.850283 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_features_null_filtered
[0m22:52:14.850783 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies
[0m22:52:14.851128 [info ] [Thread-1  ]: 32 of 38 SKIP relation dbt_anomaly_detection.remaining_events_min_anomalies .... [[33mSKIP[0m]
[0m22:52:14.851607 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies
[0m22:52:14.851950 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m22:52:14.852237 [info ] [Thread-1  ]: 33 of 38 SKIP relation dbt_anomaly_detection.remaining_events_min_anomalies_results  [[33mSKIP[0m]
[0m22:52:14.852503 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m22:52:14.852816 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD
[0m22:52:14.852950 [info ] [Thread-1  ]: 34 of 38 SKIP relation dbt_anomaly_detection.remaining_events_min_RMSD ......... [[33mSKIP[0m]
[0m22:52:14.853455 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD
[0m22:52:14.853905 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m22:52:14.854064 [info ] [Thread-1  ]: 35 of 38 SKIP relation dbt_anomaly_detection.remaining_events_min_RMSD_results . [[33mSKIP[0m]
[0m22:52:14.854398 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m22:52:14.854712 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_all_models
[0m22:52:14.854827 [info ] [Thread-1  ]: 36 of 38 SKIP relation dbt_anomaly_detection.filtered_all_models ............... [[33mSKIP[0m]
[0m22:52:14.855026 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_all_models
[0m22:52:14.855294 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m22:52:14.855511 [info ] [Thread-1  ]: 37 of 38 SKIP relation dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [[33mSKIP[0m]
[0m22:52:14.855824 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m22:52:14.856121 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_events_with_anomalies
[0m22:52:14.856264 [info ] [Thread-1  ]: 38 of 38 SKIP relation dbt_anomaly_detection.derived_events_with_anomalies ..... [[33mSKIP[0m]
[0m22:52:14.856524 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_events_with_anomalies
[0m22:52:14.857469 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m22:52:14.857846 [info ] [MainThread]: 
[0m22:52:14.858030 [info ] [MainThread]: Finished running 26 table models, 12 model models in 0 hours 1 minutes and 15.80 seconds (75.80s).
[0m22:52:14.858188 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:52:14.858276 [debug] [MainThread]: Connection 'model.anomaly_detection.derived_models_2mon_24hr' was properly closed.
[0m22:52:14.871030 [info ] [MainThread]: 
[0m22:52:14.871380 [info ] [MainThread]: [31mCompleted with 12 errors and 0 warnings:[0m
[0m22:52:14.871536 [info ] [MainThread]: 
[0m22:52:14.871739 [error] [MainThread]: [33mDatabase Error in model derived_models_05mon_4hr (models/example/derived_models_05mon_4hr.sql)[0m
[0m22:52:14.871927 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m22:52:14.872039 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_4hr.sql
[0m22:52:14.872152 [info ] [MainThread]: 
[0m22:52:14.872264 [error] [MainThread]: [33mDatabase Error in model derived_models_05mon_8hr (models/example/derived_models_05mon_8hr.sql)[0m
[0m22:52:14.872369 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m22:52:14.872474 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_8hr.sql
[0m22:52:14.872578 [info ] [MainThread]: 
[0m22:52:14.872684 [error] [MainThread]: [33mDatabase Error in model derived_models_1mon_4hr (models/example/derived_models_1mon_4hr.sql)[0m
[0m22:52:14.872788 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m22:52:14.872891 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_4hr.sql
[0m22:52:14.872994 [info ] [MainThread]: 
[0m22:52:14.873099 [error] [MainThread]: [33mDatabase Error in model derived_models_1mon_8hr (models/example/derived_models_1mon_8hr.sql)[0m
[0m22:52:14.873204 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m22:52:14.873307 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_8hr.sql
[0m22:52:14.873412 [info ] [MainThread]: 
[0m22:52:14.873515 [error] [MainThread]: [33mDatabase Error in model derived_models_2mon_4hr (models/example/derived_models_2mon_4hr.sql)[0m
[0m22:52:14.873620 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m22:52:14.873722 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_4hr.sql
[0m22:52:14.873825 [info ] [MainThread]: 
[0m22:52:14.873929 [error] [MainThread]: [33mDatabase Error in model derived_models_2mon_8hr (models/example/derived_models_2mon_8hr.sql)[0m
[0m22:52:14.874033 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m22:52:14.874139 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_8hr.sql
[0m22:52:14.874245 [info ] [MainThread]: 
[0m22:52:14.874349 [error] [MainThread]: [33mDatabase Error in model derived_models_05mon_12hr (models/example/derived_models_05mon_12hr.sql)[0m
[0m22:52:14.874452 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m22:52:14.874550 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_12hr.sql
[0m22:52:14.874649 [info ] [MainThread]: 
[0m22:52:14.874749 [error] [MainThread]: [33mDatabase Error in model derived_models_05mon_24hr (models/example/derived_models_05mon_24hr.sql)[0m
[0m22:52:14.874848 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m22:52:14.874950 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_24hr.sql
[0m22:52:14.875052 [info ] [MainThread]: 
[0m22:52:14.875155 [error] [MainThread]: [33mDatabase Error in model derived_models_1mon_12hr (models/example/derived_models_1mon_12hr.sql)[0m
[0m22:52:14.875258 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m22:52:14.875364 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_12hr.sql
[0m22:52:14.875468 [info ] [MainThread]: 
[0m22:52:14.875572 [error] [MainThread]: [33mDatabase Error in model derived_models_1mon_24hr (models/example/derived_models_1mon_24hr.sql)[0m
[0m22:52:14.875675 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m22:52:14.875782 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_24hr.sql
[0m22:52:14.875890 [info ] [MainThread]: 
[0m22:52:14.875995 [error] [MainThread]: [33mDatabase Error in model derived_models_2mon_12hr (models/example/derived_models_2mon_12hr.sql)[0m
[0m22:52:14.876098 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m22:52:14.876199 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_12hr.sql
[0m22:52:14.876300 [info ] [MainThread]: 
[0m22:52:14.876405 [error] [MainThread]: [33mDatabase Error in model derived_models_2mon_24hr (models/example/derived_models_2mon_24hr.sql)[0m
[0m22:52:14.876511 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m22:52:14.876613 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_24hr.sql
[0m22:52:14.876751 [info ] [MainThread]: 
[0m22:52:14.876862 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=12 SKIP=13 TOTAL=38
[0m22:52:14.877127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a11fb80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a31e190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a582b80>]}
[0m22:52:14.877392 [debug] [MainThread]: Flushing usage events


============================== 2023-02-13 14:25:27.932834 | 09834606-1cd6-4633-9e74-faebdfbbcaa7 ==============================
[0m14:25:27.932854 [info ] [MainThread]: Running with dbt=1.2.1
[0m14:25:27.933523 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:25:27.933613 [debug] [MainThread]: Tracking: tracking
[0m14:25:27.943039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10764d460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10764d340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10764d2b0>]}
[0m14:25:27.998755 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:25:27.998910 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:25:28.002814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a060d0>]}
[0m14:25:28.010498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078d0640>]}
[0m14:25:28.010648 [info ] [MainThread]: Found 38 models, 0 tests, 0 snapshots, 0 analyses, 540 macros, 0 operations, 0 seed files, 2 sources, 0 exposures, 0 metrics
[0m14:25:28.010779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078d0820>]}
[0m14:25:28.012096 [info ] [MainThread]: 
[0m14:25:28.012326 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m14:25:28.013744 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow"
[0m14:25:28.013950 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:29.488744 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow_dbt_anomaly_detection"
[0m14:25:29.489194 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:25:29.862196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107629df0>]}
[0m14:25:29.863051 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:25:29.863362 [info ] [MainThread]: 
[0m14:25:29.867739 [debug] [Thread-1  ]: Began running node model.anomaly_detection.reference_derived
[0m14:25:29.868113 [info ] [Thread-1  ]: 1 of 38 START table model dbt_anomaly_detection.reference_derived .............. [RUN]
[0m14:25:29.868846 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.reference_derived"
[0m14:25:29.869210 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.reference_derived
[0m14:25:29.869523 [debug] [Thread-1  ]: Compiling model.anomaly_detection.reference_derived
[0m14:25:29.872597 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.reference_derived"
[0m14:25:29.873392 [debug] [Thread-1  ]: finished collecting timing info
[0m14:25:29.873623 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.reference_derived
[0m14:25:29.886940 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:25:30.261153 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.reference_derived"
[0m14:25:30.262201 [debug] [Thread-1  ]: On model.anomaly_detection.reference_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.reference_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived`
  
  
  OPTIONS()
  as (
    
SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`web_purchase_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB("2023-02-09", INTERVAL 90 DAY) AND DATE(collector_tstamp) < "2023-02-09"

UNION ALL

SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`pco_web_apply_filter_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB("2023-02-09", INTERVAL 90 DAY) AND DATE(collector_tstamp) < "2023-02-09"
  );
  
[0m14:25:34.231106 [debug] [Thread-1  ]: finished collecting timing info
[0m14:25:34.231896 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b67850>]}
[0m14:25:34.232320 [info ] [Thread-1  ]: 1 of 38 OK created table model dbt_anomaly_detection.reference_derived ......... [[32mCREATE TABLE (1.1m rows, 34.8 MB processed)[0m in 4.36s]
[0m14:25:34.232755 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.reference_derived
[0m14:25:34.233430 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived
[0m14:25:34.233706 [info ] [Thread-1  ]: 2 of 38 START table model dbt_anomaly_detection.all_agg_derived ................ [RUN]
[0m14:25:34.234045 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived"
[0m14:25:34.234159 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived
[0m14:25:34.234310 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived
[0m14:25:34.240500 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived"
[0m14:25:34.241016 [debug] [Thread-1  ]: finished collecting timing info
[0m14:25:34.241118 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived
[0m14:25:34.242836 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:25:34.570329 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived"
[0m14:25:34.571279 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
  
  
  OPTIONS()
  as (
    


SELECT "4hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_4hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/4)*4 AS _4hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _4hr_trunc,
    app_id,
    event_type)

UNION ALL


SELECT "8hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_8hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/8)*8 AS _8hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _8hr_trunc,
    app_id,
    event_type)

UNION ALL


SELECT "12hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_12hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/12)*12 AS _12hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _12hr_trunc,
    app_id,
    event_type)

UNION ALL


SELECT "24hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_24hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/24)*24 AS _24hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _24hr_trunc,
    app_id,
    event_type)

ORDER BY
  agg_tag,
  time_stamps,
  app_id,
  event_type


  );
  
[0m14:25:39.169114 [debug] [Thread-1  ]: finished collecting timing info
[0m14:25:39.170051 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107afefa0>]}
[0m14:25:39.170564 [info ] [Thread-1  ]: 2 of 38 OK created table model dbt_anomaly_detection.all_agg_derived ........... [[32mCREATE TABLE (2.2k rows, 34.8 MB processed)[0m in 4.94s]
[0m14:25:39.171106 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived
[0m14:25:39.172061 [debug] [Thread-1  ]: Began running node model.anomaly_detection.count_cutoff
[0m14:25:39.172610 [info ] [Thread-1  ]: 3 of 38 START table model dbt_anomaly_detection.count_cutoff ................... [RUN]
[0m14:25:39.173171 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.count_cutoff"
[0m14:25:39.173347 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.count_cutoff
[0m14:25:39.173512 [debug] [Thread-1  ]: Compiling model.anomaly_detection.count_cutoff
[0m14:25:39.178013 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.count_cutoff"
[0m14:25:39.178740 [debug] [Thread-1  ]: finished collecting timing info
[0m14:25:39.178897 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.count_cutoff
[0m14:25:39.181518 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:25:39.523148 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.count_cutoff"
[0m14:25:39.523961 [debug] [Thread-1  ]: On model.anomaly_detection.count_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.count_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select agg_tag, app_event, min(time_stamps) as strt_time
from pairs
where event_count > 50
group by app_event, agg_tag 
order by app_event, agg_tag
  );
  
[0m14:25:41.995604 [debug] [Thread-1  ]: finished collecting timing info
[0m14:25:41.996736 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b04be0>]}
[0m14:25:41.997284 [info ] [Thread-1  ]: 3 of 38 OK created table model dbt_anomaly_detection.count_cutoff .............. [[32mCREATE TABLE (8.0 rows, 78.6 KB processed)[0m in 2.82s]
[0m14:25:41.997826 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.count_cutoff
[0m14:25:41.998787 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff
[0m14:25:41.999187 [info ] [Thread-1  ]: 4 of 38 START table model dbt_anomaly_detection.all_agg_derived_cutoff ......... [RUN]
[0m14:25:41.999794 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff"
[0m14:25:41.999998 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff
[0m14:25:42.000193 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff
[0m14:25:42.004906 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m14:25:42.005598 [debug] [Thread-1  ]: finished collecting timing info
[0m14:25:42.005767 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff
[0m14:25:42.068472 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:25:42.438974 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m14:25:42.440135 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select time_stamps, app_event, agg_tag, event_count
from(
select time_stamps, strt_time, pairs.app_event, pairs.agg_tag, event_count
from pairs
inner join `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff` as cutoff
on pairs.agg_tag = cutoff.agg_tag
and pairs.app_event = cutoff.app_event
order by pairs.app_event, pairs.agg_tag, time_stamps)
where time_stamps >= strt_time
  );
  
[0m14:25:44.800033 [debug] [Thread-1  ]: finished collecting timing info
[0m14:25:44.801130 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107562940>]}
[0m14:25:44.802327 [info ] [Thread-1  ]: 4 of 38 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff .... [[32mCREATE TABLE (2.2k rows, 78.8 KB processed)[0m in 2.80s]
[0m14:25:44.803217 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff
[0m14:25:44.805527 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m14:25:44.806808 [info ] [Thread-1  ]: 5 of 38 START table model dbt_anomaly_detection.all_agg_derived_cutoff_long .... [RUN]
[0m14:25:44.807735 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m14:25:44.808019 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_long
[0m14:25:44.808223 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_long
[0m14:25:44.821443 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m14:25:44.822605 [debug] [Thread-1  ]: finished collecting timing info
[0m14:25:44.822886 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_long
[0m14:25:44.826735 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:25:45.222945 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m14:25:45.224287 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB("2023-02-09", INTERVAL 10 DAY)
  );
  
[0m14:25:47.721639 [debug] [Thread-1  ]: finished collecting timing info
[0m14:25:47.722379 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075ccb20>]}
[0m14:25:47.723062 [info ] [Thread-1  ]: 5 of 38 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_long  [[32mCREATE TABLE (1.9k rows, 76.5 KB processed)[0m in 2.91s]
[0m14:25:47.723988 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m14:25:47.725395 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m14:25:47.725801 [info ] [Thread-1  ]: 6 of 38 START table model dbt_anomaly_detection.all_agg_derived_cutoff_short ... [RUN]
[0m14:25:47.726356 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m14:25:47.726582 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_short
[0m14:25:47.726911 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_short
[0m14:25:47.735815 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m14:25:47.736785 [debug] [Thread-1  ]: finished collecting timing info
[0m14:25:47.737004 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_short
[0m14:25:47.740506 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:25:48.128861 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m14:25:48.131383 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB("2023-02-09", INTERVAL 15 DAY)
  );
  
[0m14:25:50.159088 [debug] [Thread-1  ]: finished collecting timing info
[0m14:25:50.160787 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a97130>]}
[0m14:25:50.161519 [info ] [Thread-1  ]: 6 of 38 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_short  [[32mCREATE TABLE (1.8k rows, 76.5 KB processed)[0m in 2.43s]
[0m14:25:50.162184 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m14:25:50.162473 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_nonrecent_events
[0m14:25:50.162742 [info ] [Thread-1  ]: 7 of 38 START table model dbt_anomaly_detection.derived_nonrecent_events ....... [RUN]
[0m14:25:50.163345 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_nonrecent_events"
[0m14:25:50.163540 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_nonrecent_events
[0m14:25:50.164244 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_nonrecent_events
[0m14:25:50.177754 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m14:25:50.178725 [debug] [Thread-1  ]: finished collecting timing info
[0m14:25:50.178961 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_nonrecent_events
[0m14:25:50.185208 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:25:50.578216 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m14:25:50.580610 [debug] [Thread-1  ]: On model.anomaly_detection.derived_nonrecent_events: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_nonrecent_events"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events`
  
  
  OPTIONS()
  as (
    

SELECT MIN(time_stamps) AS strt_time, app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
GROUP BY app_event
HAVING DATE(MIN(time_stamps)) < DATE_SUB("2023-02-09", INTERVAL 30 DAY)
ORDER BY strt_time
  );
  
[0m14:25:53.152572 [debug] [Thread-1  ]: finished collecting timing info
[0m14:25:53.155770 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1079d26a0>]}
[0m14:25:53.156792 [info ] [Thread-1  ]: 7 of 38 OK created table model dbt_anomaly_detection.derived_nonrecent_events .. [[32mCREATE TABLE (2.0 rows, 48.5 KB processed)[0m in 2.99s]
[0m14:25:53.157623 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_nonrecent_events
[0m14:25:53.157946 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_long
[0m14:25:53.158227 [info ] [Thread-1  ]: 8 of 38 START table model dbt_anomaly_detection.aggregation_quartiles_long ..... [RUN]
[0m14:25:53.158942 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_long"
[0m14:25:53.159173 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_long
[0m14:25:53.159531 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_long
[0m14:25:53.167527 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m14:25:53.168853 [debug] [Thread-1  ]: finished collecting timing info
[0m14:25:53.169090 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_long
[0m14:25:53.172834 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:25:53.566687 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m14:25:53.568625 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m14:25:56.939692 [debug] [Thread-1  ]: finished collecting timing info
[0m14:25:56.943648 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107aff970>]}
[0m14:25:56.944682 [info ] [Thread-1  ]: 8 of 38 OK created table model dbt_anomaly_detection.aggregation_quartiles_long  [[32mCREATE TABLE (8.0 rows, 53.0 KB processed)[0m in 3.78s]
[0m14:25:56.945411 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_long
[0m14:25:56.945759 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_short
[0m14:25:56.946320 [info ] [Thread-1  ]: 9 of 38 START table model dbt_anomaly_detection.aggregation_quartiles_short .... [RUN]
[0m14:25:56.947110 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_short"
[0m14:25:56.947406 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_short
[0m14:25:56.949358 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_short
[0m14:25:56.967307 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m14:25:56.968132 [debug] [Thread-1  ]: finished collecting timing info
[0m14:25:56.968376 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_short
[0m14:25:56.972123 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:25:57.574863 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m14:25:57.575891 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m14:25:59.888052 [debug] [Thread-1  ]: finished collecting timing info
[0m14:25:59.888999 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b2bfd0>]}
[0m14:25:59.889612 [info ] [Thread-1  ]: 9 of 38 OK created table model dbt_anomaly_detection.aggregation_quartiles_short  [[32mCREATE TABLE (8.0 rows, 49.7 KB processed)[0m in 2.94s]
[0m14:25:59.890805 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_short
[0m14:25:59.892091 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_long
[0m14:25:59.892531 [info ] [Thread-1  ]: 10 of 38 START table model dbt_anomaly_detection.aggregation_bounds_long ....... [RUN]
[0m14:25:59.893188 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_long"
[0m14:25:59.893549 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_long
[0m14:25:59.893835 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_long
[0m14:25:59.908076 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m14:25:59.908451 [debug] [Thread-1  ]: finished collecting timing info
[0m14:25:59.908540 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_long
[0m14:25:59.909879 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:26:00.263121 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m14:26:00.264518 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m14:26:02.305919 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:02.307173 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b19fa0>]}
[0m14:26:02.309154 [info ] [Thread-1  ]: 10 of 38 OK created table model dbt_anomaly_detection.aggregation_bounds_long .. [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.41s]
[0m14:26:02.309739 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_long
[0m14:26:02.309993 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_short
[0m14:26:02.310410 [info ] [Thread-1  ]: 11 of 38 START table model dbt_anomaly_detection.aggregation_bounds_short ...... [RUN]
[0m14:26:02.310991 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_short"
[0m14:26:02.311472 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_short
[0m14:26:02.311873 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_short
[0m14:26:02.320281 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m14:26:02.321235 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:02.321391 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_short
[0m14:26:02.322829 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:26:02.697540 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m14:26:02.698507 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m14:26:05.155971 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:05.156801 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075c3790>]}
[0m14:26:05.157320 [info ] [Thread-1  ]: 11 of 38 OK created table model dbt_anomaly_detection.aggregation_bounds_short . [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.85s]
[0m14:26:05.158279 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_short
[0m14:26:05.158614 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_long
[0m14:26:05.158928 [info ] [Thread-1  ]: 12 of 38 START table model dbt_anomaly_detection.aggregation_outliers_long ..... [RUN]
[0m14:26:05.160447 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_long"
[0m14:26:05.160901 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_long
[0m14:26:05.161107 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_long
[0m14:26:05.169547 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m14:26:05.170595 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:05.170839 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_long
[0m14:26:05.176110 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:26:05.552443 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m14:26:05.553349 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m14:26:07.802762 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:07.804754 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107af5a90>]}
[0m14:26:07.806657 [info ] [Thread-1  ]: 12 of 38 OK created table model dbt_anomaly_detection.aggregation_outliers_long  [[32mCREATE TABLE (1.9k rows, 68.3 KB processed)[0m in 2.65s]
[0m14:26:07.807764 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_long
[0m14:26:07.808490 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_short
[0m14:26:07.809267 [info ] [Thread-1  ]: 13 of 38 START table model dbt_anomaly_detection.aggregation_outliers_short .... [RUN]
[0m14:26:07.812628 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_short"
[0m14:26:07.812994 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_short
[0m14:26:07.813236 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_short
[0m14:26:07.833920 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m14:26:07.835102 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:07.835292 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_short
[0m14:26:07.838278 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:26:08.248359 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m14:26:08.251633 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m14:26:10.743183 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:10.745260 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107af5a60>]}
[0m14:26:10.745916 [info ] [Thread-1  ]: 13 of 38 OK created table model dbt_anomaly_detection.aggregation_outliers_short  [[32mCREATE TABLE (1.8k rows, 64.0 KB processed)[0m in 2.93s]
[0m14:26:10.747043 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_short
[0m14:26:10.748227 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_4hr
[0m14:26:10.748571 [info ] [Thread-1  ]: 14 of 38 START model model dbt_anomaly_detection.derived_models_05mon_4hr ...... [RUN]
[0m14:26:10.749565 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_4hr"
[0m14:26:10.749987 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_4hr
[0m14:26:10.750189 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_4hr
[0m14:26:10.761616 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m14:26:10.762166 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:10.762310 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_4hr
[0m14:26:10.773496 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m14:26:10.773970 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:26:10.774103 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m14:26:11.817306 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m14:26:13.695211 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:13.696534 [debug] [Thread-1  ]: Database Error in model derived_models_05mon_4hr (models/example/derived_models_05mon_4hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_4hr.sql
[0m14:26:13.697196 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bcac40>]}
[0m14:26:13.698128 [error] [Thread-1  ]: 14 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_05mon_4hr  [[31mERROR[0m in 2.95s]
[0m14:26:13.698958 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_4hr
[0m14:26:13.699223 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_8hr
[0m14:26:13.699690 [info ] [Thread-1  ]: 15 of 38 START model model dbt_anomaly_detection.derived_models_05mon_8hr ...... [RUN]
[0m14:26:13.700821 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_8hr"
[0m14:26:13.701467 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_8hr
[0m14:26:13.701680 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_8hr
[0m14:26:13.714519 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m14:26:13.715521 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:13.715825 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_8hr
[0m14:26:13.718882 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m14:26:13.719223 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:26:13.719330 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m14:26:14.895948 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m14:26:16.740188 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:16.741365 [debug] [Thread-1  ]: Database Error in model derived_models_05mon_8hr (models/example/derived_models_05mon_8hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_8hr.sql
[0m14:26:16.741937 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ac0550>]}
[0m14:26:16.742418 [error] [Thread-1  ]: 15 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_05mon_8hr  [[31mERROR[0m in 3.04s]
[0m14:26:16.742910 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_8hr
[0m14:26:16.743197 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_4hr
[0m14:26:16.743477 [info ] [Thread-1  ]: 16 of 38 START model model dbt_anomaly_detection.derived_models_1mon_4hr ....... [RUN]
[0m14:26:16.744375 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_4hr"
[0m14:26:16.744800 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_4hr
[0m14:26:16.745724 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_4hr
[0m14:26:16.756118 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m14:26:16.757298 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:16.757572 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_4hr
[0m14:26:16.765596 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m14:26:16.766460 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:26:16.766730 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m14:26:20.905214 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m14:26:22.072068 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:22.079170 [debug] [Thread-1  ]: Database Error in model derived_models_1mon_4hr (models/example/derived_models_1mon_4hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_4hr.sql
[0m14:26:22.079779 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bfdc10>]}
[0m14:26:22.080248 [error] [Thread-1  ]: 16 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_1mon_4hr  [[31mERROR[0m in 5.34s]
[0m14:26:22.080792 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_4hr
[0m14:26:22.081086 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_8hr
[0m14:26:22.081599 [info ] [Thread-1  ]: 17 of 38 START model model dbt_anomaly_detection.derived_models_1mon_8hr ....... [RUN]
[0m14:26:22.082866 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_8hr"
[0m14:26:22.083117 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_8hr
[0m14:26:22.083299 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_8hr
[0m14:26:22.094186 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m14:26:22.095465 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:22.095750 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_8hr
[0m14:26:22.100068 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m14:26:22.100533 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:26:22.100723 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m14:26:23.279467 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m14:26:25.100908 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:25.102151 [debug] [Thread-1  ]: Database Error in model derived_models_1mon_8hr (models/example/derived_models_1mon_8hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_8hr.sql
[0m14:26:25.102906 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d3fa30>]}
[0m14:26:25.103485 [error] [Thread-1  ]: 17 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_1mon_8hr  [[31mERROR[0m in 3.02s]
[0m14:26:25.103973 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_8hr
[0m14:26:25.104276 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_4hr
[0m14:26:25.104663 [info ] [Thread-1  ]: 18 of 38 START model model dbt_anomaly_detection.derived_models_2mon_4hr ....... [RUN]
[0m14:26:25.106194 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_4hr"
[0m14:26:25.106711 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_4hr
[0m14:26:25.106934 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_4hr
[0m14:26:25.117906 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m14:26:25.119691 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:25.119894 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_4hr
[0m14:26:25.121735 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m14:26:25.122145 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:26:25.122271 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m14:26:26.192077 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m14:26:28.033936 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:28.035501 [debug] [Thread-1  ]: Database Error in model derived_models_2mon_4hr (models/example/derived_models_2mon_4hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_4hr.sql
[0m14:26:28.035938 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d5e340>]}
[0m14:26:28.036391 [error] [Thread-1  ]: 18 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_2mon_4hr  [[31mERROR[0m in 2.93s]
[0m14:26:28.036883 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_4hr
[0m14:26:28.037549 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_8hr
[0m14:26:28.038251 [info ] [Thread-1  ]: 19 of 38 START model model dbt_anomaly_detection.derived_models_2mon_8hr ....... [RUN]
[0m14:26:28.039587 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_8hr"
[0m14:26:28.039868 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_8hr
[0m14:26:28.040063 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_8hr
[0m14:26:28.048709 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m14:26:28.049840 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:28.050128 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_8hr
[0m14:26:28.055212 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m14:26:28.055645 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:26:28.055762 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m14:26:29.283818 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m14:26:33.846026 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:33.847142 [debug] [Thread-1  ]: Database Error in model derived_models_2mon_8hr (models/example/derived_models_2mon_8hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_8hr.sql
[0m14:26:33.847824 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d9ea60>]}
[0m14:26:33.848272 [error] [Thread-1  ]: 19 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_2mon_8hr  [[31mERROR[0m in 5.81s]
[0m14:26:33.848811 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_8hr
[0m14:26:33.849266 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_12hr
[0m14:26:33.849561 [info ] [Thread-1  ]: 20 of 38 START model model dbt_anomaly_detection.derived_models_05mon_12hr ..... [RUN]
[0m14:26:33.850143 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_12hr"
[0m14:26:33.850594 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_12hr
[0m14:26:33.850843 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_12hr
[0m14:26:33.861726 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m14:26:33.862720 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:33.862968 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_12hr
[0m14:26:33.870494 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m14:26:33.870903 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:26:33.871012 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m14:26:34.997137 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m14:26:36.779088 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:36.779806 [debug] [Thread-1  ]: Database Error in model derived_models_05mon_12hr (models/example/derived_models_05mon_12hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_12hr.sql
[0m14:26:36.780702 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b67670>]}
[0m14:26:36.781183 [error] [Thread-1  ]: 20 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_05mon_12hr  [[31mERROR[0m in 2.93s]
[0m14:26:36.781731 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_12hr
[0m14:26:36.782166 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_24hr
[0m14:26:36.782526 [info ] [Thread-1  ]: 21 of 38 START model model dbt_anomaly_detection.derived_models_05mon_24hr ..... [RUN]
[0m14:26:36.783740 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_24hr"
[0m14:26:36.784073 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_24hr
[0m14:26:36.784269 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_24hr
[0m14:26:36.797534 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m14:26:36.799253 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:36.799861 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_24hr
[0m14:26:36.805638 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m14:26:36.806061 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:26:36.806187 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m14:26:37.971699 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m14:26:39.597796 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:39.601072 [debug] [Thread-1  ]: Database Error in model derived_models_05mon_24hr (models/example/derived_models_05mon_24hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_24hr.sql
[0m14:26:39.601542 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107de3f40>]}
[0m14:26:39.601972 [error] [Thread-1  ]: 21 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_05mon_24hr  [[31mERROR[0m in 2.82s]
[0m14:26:39.602457 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_24hr
[0m14:26:39.603007 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_12hr
[0m14:26:39.603719 [info ] [Thread-1  ]: 22 of 38 START model model dbt_anomaly_detection.derived_models_1mon_12hr ...... [RUN]
[0m14:26:39.604687 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_12hr"
[0m14:26:39.604964 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_12hr
[0m14:26:39.605259 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_12hr
[0m14:26:39.619599 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m14:26:39.620736 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:39.621169 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_12hr
[0m14:26:39.626966 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m14:26:39.628167 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:26:39.628539 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m14:26:40.831209 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m14:26:42.390235 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:42.392663 [debug] [Thread-1  ]: Database Error in model derived_models_1mon_12hr (models/example/derived_models_1mon_12hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_12hr.sql
[0m14:26:42.393197 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1107de820>]}
[0m14:26:42.394560 [error] [Thread-1  ]: 22 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_1mon_12hr  [[31mERROR[0m in 2.79s]
[0m14:26:42.399265 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_12hr
[0m14:26:42.399645 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_24hr
[0m14:26:42.399937 [info ] [Thread-1  ]: 23 of 38 START model model dbt_anomaly_detection.derived_models_1mon_24hr ...... [RUN]
[0m14:26:42.400696 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_24hr"
[0m14:26:42.400921 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_24hr
[0m14:26:42.401105 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_24hr
[0m14:26:42.413841 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m14:26:42.414351 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:42.414458 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_24hr
[0m14:26:42.417748 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m14:26:42.418289 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:26:42.418434 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m14:26:43.589858 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m14:26:45.528355 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:45.529297 [debug] [Thread-1  ]: Database Error in model derived_models_1mon_24hr (models/example/derived_models_1mon_24hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_24hr.sql
[0m14:26:45.529739 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1107b8640>]}
[0m14:26:45.530196 [error] [Thread-1  ]: 23 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_1mon_24hr  [[31mERROR[0m in 3.13s]
[0m14:26:45.531103 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_24hr
[0m14:26:45.531439 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_12hr
[0m14:26:45.531720 [info ] [Thread-1  ]: 24 of 38 START model model dbt_anomaly_detection.derived_models_2mon_12hr ...... [RUN]
[0m14:26:45.532323 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_12hr"
[0m14:26:45.532526 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_12hr
[0m14:26:45.533658 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_12hr
[0m14:26:45.546150 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m14:26:45.546549 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:45.546632 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_12hr
[0m14:26:45.548243 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m14:26:45.548479 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:26:45.548591 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m14:26:46.704837 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m14:26:47.661874 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:47.662575 [debug] [Thread-1  ]: Database Error in model derived_models_2mon_12hr (models/example/derived_models_2mon_12hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_12hr.sql
[0m14:26:47.662946 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110838340>]}
[0m14:26:47.663394 [error] [Thread-1  ]: 24 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_2mon_12hr  [[31mERROR[0m in 2.13s]
[0m14:26:47.663877 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_12hr
[0m14:26:47.664113 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_24hr
[0m14:26:47.664365 [info ] [Thread-1  ]: 25 of 38 START model model dbt_anomaly_detection.derived_models_2mon_24hr ...... [RUN]
[0m14:26:47.665133 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_24hr"
[0m14:26:47.665394 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_24hr
[0m14:26:47.665600 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_24hr
[0m14:26:47.674591 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m14:26:47.675797 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:47.676010 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_24hr
[0m14:26:47.680808 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m14:26:47.681761 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:26:47.682083 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m14:26:48.883744 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m14:26:54.630919 [debug] [Thread-1  ]: finished collecting timing info
[0m14:26:54.634352 [debug] [Thread-1  ]: Database Error in model derived_models_2mon_24hr (models/example/derived_models_2mon_24hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_24hr.sql
[0m14:26:54.635555 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09834606-1cd6-4633-9e74-faebdfbbcaa7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11085d280>]}
[0m14:26:55.293594 [error] [Thread-1  ]: 25 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_2mon_24hr  [[31mERROR[0m in 6.97s]
[0m14:26:55.295482 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_24hr
[0m14:26:55.298524 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ml_detect
[0m14:26:55.299005 [info ] [Thread-1  ]: 26 of 38 SKIP relation dbt_anomaly_detection.derived_ml_detect ................. [[33mSKIP[0m]
[0m14:26:55.299707 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ml_detect
[0m14:26:55.300755 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ml_detect_tweaked
[0m14:26:55.301271 [info ] [Thread-1  ]: 27 of 38 SKIP relation dbt_anomaly_detection.ml_detect_tweaked ................. [[33mSKIP[0m]
[0m14:26:55.302196 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ml_detect_tweaked
[0m14:26:55.303420 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_model_features_dbt
[0m14:26:55.304025 [info ] [Thread-1  ]: 28 of 38 SKIP relation dbt_anomaly_detection.derived_model_features_dbt ........ [[33mSKIP[0m]
[0m14:26:55.304547 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_model_features_dbt
[0m14:26:55.306039 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_model_features_dbt
[0m14:26:55.306470 [info ] [Thread-1  ]: 29 of 38 SKIP relation dbt_anomaly_detection.filtered_model_features_dbt ....... [[33mSKIP[0m]
[0m14:26:55.306975 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_model_features_dbt
[0m14:26:55.310852 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ref_distinct_tuples
[0m14:26:55.311276 [info ] [Thread-1  ]: 30 of 38 SKIP relation dbt_anomaly_detection.ref_distinct_tuples ............... [[33mSKIP[0m]
[0m14:26:55.311698 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ref_distinct_tuples
[0m14:26:55.312150 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_features_null_filtered
[0m14:26:55.312522 [info ] [Thread-1  ]: 31 of 38 SKIP relation dbt_anomaly_detection.remaining_events_features_null_filtered  [[33mSKIP[0m]
[0m14:26:55.312956 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_features_null_filtered
[0m14:26:55.313536 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies
[0m14:26:55.313905 [info ] [Thread-1  ]: 32 of 38 SKIP relation dbt_anomaly_detection.remaining_events_min_anomalies .... [[33mSKIP[0m]
[0m14:26:55.314320 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies
[0m14:26:55.314857 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m14:26:55.315094 [info ] [Thread-1  ]: 33 of 38 SKIP relation dbt_anomaly_detection.remaining_events_min_anomalies_results  [[33mSKIP[0m]
[0m14:26:55.315437 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m14:26:55.315900 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD
[0m14:26:55.316165 [info ] [Thread-1  ]: 34 of 38 SKIP relation dbt_anomaly_detection.remaining_events_min_RMSD ......... [[33mSKIP[0m]
[0m14:26:55.316602 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD
[0m14:26:55.317026 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m14:26:55.317286 [info ] [Thread-1  ]: 35 of 38 SKIP relation dbt_anomaly_detection.remaining_events_min_RMSD_results . [[33mSKIP[0m]
[0m14:26:55.317607 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m14:26:55.318291 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_all_models
[0m14:26:55.318582 [info ] [Thread-1  ]: 36 of 38 SKIP relation dbt_anomaly_detection.filtered_all_models ............... [[33mSKIP[0m]
[0m14:26:55.319008 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_all_models
[0m14:26:55.319505 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m14:26:55.319768 [info ] [Thread-1  ]: 37 of 38 SKIP relation dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [[33mSKIP[0m]
[0m14:26:55.320094 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m14:26:55.320617 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_events_with_anomalies
[0m14:26:55.320846 [info ] [Thread-1  ]: 38 of 38 SKIP relation dbt_anomaly_detection.derived_events_with_anomalies ..... [[33mSKIP[0m]
[0m14:26:55.321201 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_events_with_anomalies
[0m14:26:55.322448 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m14:26:55.322956 [info ] [MainThread]: 
[0m14:26:55.323201 [info ] [MainThread]: Finished running 26 table models, 12 model models in 0 hours 1 minutes and 27.31 seconds (87.31s).
[0m14:26:55.323426 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:26:55.323548 [debug] [MainThread]: Connection 'model.anomaly_detection.derived_models_2mon_24hr' was properly closed.
[0m14:26:55.338290 [info ] [MainThread]: 
[0m14:26:55.338530 [info ] [MainThread]: [31mCompleted with 12 errors and 0 warnings:[0m
[0m14:26:55.338728 [info ] [MainThread]: 
[0m14:26:55.338886 [error] [MainThread]: [33mDatabase Error in model derived_models_05mon_4hr (models/example/derived_models_05mon_4hr.sql)[0m
[0m14:26:55.339041 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m14:26:55.339190 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_4hr.sql
[0m14:26:55.339338 [info ] [MainThread]: 
[0m14:26:55.339487 [error] [MainThread]: [33mDatabase Error in model derived_models_05mon_8hr (models/example/derived_models_05mon_8hr.sql)[0m
[0m14:26:55.339632 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m14:26:55.339778 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_8hr.sql
[0m14:26:55.339927 [info ] [MainThread]: 
[0m14:26:55.340073 [error] [MainThread]: [33mDatabase Error in model derived_models_1mon_4hr (models/example/derived_models_1mon_4hr.sql)[0m
[0m14:26:55.340216 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m14:26:55.340358 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_4hr.sql
[0m14:26:55.340500 [info ] [MainThread]: 
[0m14:26:55.340643 [error] [MainThread]: [33mDatabase Error in model derived_models_1mon_8hr (models/example/derived_models_1mon_8hr.sql)[0m
[0m14:26:55.340787 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m14:26:55.340928 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_8hr.sql
[0m14:26:55.341074 [info ] [MainThread]: 
[0m14:26:55.341221 [error] [MainThread]: [33mDatabase Error in model derived_models_2mon_4hr (models/example/derived_models_2mon_4hr.sql)[0m
[0m14:26:55.341369 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m14:26:55.341515 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_4hr.sql
[0m14:26:55.341657 [info ] [MainThread]: 
[0m14:26:55.341800 [error] [MainThread]: [33mDatabase Error in model derived_models_2mon_8hr (models/example/derived_models_2mon_8hr.sql)[0m
[0m14:26:55.341945 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m14:26:55.342245 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_8hr.sql
[0m14:26:55.342455 [info ] [MainThread]: 
[0m14:26:55.342620 [error] [MainThread]: [33mDatabase Error in model derived_models_05mon_12hr (models/example/derived_models_05mon_12hr.sql)[0m
[0m14:26:55.342778 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m14:26:55.342927 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_12hr.sql
[0m14:26:55.343074 [info ] [MainThread]: 
[0m14:26:55.343221 [error] [MainThread]: [33mDatabase Error in model derived_models_05mon_24hr (models/example/derived_models_05mon_24hr.sql)[0m
[0m14:26:55.343365 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m14:26:55.343509 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_24hr.sql
[0m14:26:55.343653 [info ] [MainThread]: 
[0m14:26:55.343800 [error] [MainThread]: [33mDatabase Error in model derived_models_1mon_12hr (models/example/derived_models_1mon_12hr.sql)[0m
[0m14:26:55.343944 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m14:26:55.344084 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_12hr.sql
[0m14:26:55.344224 [info ] [MainThread]: 
[0m14:26:55.344368 [error] [MainThread]: [33mDatabase Error in model derived_models_1mon_24hr (models/example/derived_models_1mon_24hr.sql)[0m
[0m14:26:55.344508 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m14:26:55.344650 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_24hr.sql
[0m14:26:55.344797 [info ] [MainThread]: 
[0m14:26:55.344943 [error] [MainThread]: [33mDatabase Error in model derived_models_2mon_12hr (models/example/derived_models_2mon_12hr.sql)[0m
[0m14:26:55.345085 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m14:26:55.345212 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_12hr.sql
[0m14:26:55.345343 [info ] [MainThread]: 
[0m14:26:55.345470 [error] [MainThread]: [33mDatabase Error in model derived_models_2mon_24hr (models/example/derived_models_2mon_24hr.sql)[0m
[0m14:26:55.345595 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m14:26:55.345721 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_24hr.sql
[0m14:26:55.345885 [info ] [MainThread]: 
[0m14:26:55.346016 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=12 SKIP=13 TOTAL=38
[0m14:26:55.346236 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078d0670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078d0190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11083be50>]}
[0m14:26:55.346411 [debug] [MainThread]: Flushing usage events


============================== 2023-02-13 15:11:27.653450 | ddf6ee2e-da6e-4870-ae9a-abc17d34fb87 ==============================
[0m15:11:27.653486 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:11:27.653981 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m15:11:27.654072 [debug] [MainThread]: Tracking: tracking
[0m15:11:27.671154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106dc87f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106dc8700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106dc8640>]}
[0m15:11:27.726564 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:11:27.726877 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_05mon_24hr.sql
[0m15:11:27.735132 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_24hr.sql
[0m15:11:27.742384 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_24hr.sql
[0m15:11:27.750223 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070dd0d0>]}
[0m15:11:27.757107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d9fc10>]}
[0m15:11:27.757261 [info ] [MainThread]: Found 38 models, 0 tests, 0 snapshots, 0 analyses, 540 macros, 0 operations, 0 seed files, 2 sources, 0 exposures, 0 metrics
[0m15:11:27.757402 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107018ca0>]}
[0m15:11:27.758722 [info ] [MainThread]: 
[0m15:11:27.758947 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m15:11:27.760250 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow"
[0m15:11:27.760408 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:11:29.221175 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow_dbt_anomaly_detection"
[0m15:11:29.221812 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:11:29.619688 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e05eb0>]}
[0m15:11:29.620805 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:11:29.621230 [info ] [MainThread]: 
[0m15:11:29.627104 [debug] [Thread-1  ]: Began running node model.anomaly_detection.reference_derived
[0m15:11:29.627589 [info ] [Thread-1  ]: 1 of 38 START table model dbt_anomaly_detection.reference_derived .............. [RUN]
[0m15:11:29.628288 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.reference_derived"
[0m15:11:29.628462 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.reference_derived
[0m15:11:29.628696 [debug] [Thread-1  ]: Compiling model.anomaly_detection.reference_derived
[0m15:11:29.636846 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.reference_derived"
[0m15:11:29.637662 [debug] [Thread-1  ]: finished collecting timing info
[0m15:11:29.637829 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.reference_derived
[0m15:11:29.682060 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:11:29.990900 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.reference_derived"
[0m15:11:29.993351 [debug] [Thread-1  ]: On model.anomaly_detection.reference_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.reference_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived`
  
  
  OPTIONS()
  as (
    
SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`web_purchase_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB("2023-02-09", INTERVAL 90 DAY) AND DATE(collector_tstamp) < "2023-02-09"

UNION ALL

SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`pco_web_apply_filter_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB("2023-02-09", INTERVAL 90 DAY) AND DATE(collector_tstamp) < "2023-02-09"
  );
  
[0m15:11:33.899742 [debug] [Thread-1  ]: finished collecting timing info
[0m15:11:33.900126 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070740a0>]}
[0m15:11:33.900388 [info ] [Thread-1  ]: 1 of 38 OK created table model dbt_anomaly_detection.reference_derived ......... [[32mCREATE TABLE (1.1m rows, 34.8 MB processed)[0m in 4.27s]
[0m15:11:33.900668 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.reference_derived
[0m15:11:33.901102 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived
[0m15:11:33.901270 [info ] [Thread-1  ]: 2 of 38 START table model dbt_anomaly_detection.all_agg_derived ................ [RUN]
[0m15:11:33.901644 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived"
[0m15:11:33.901773 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived
[0m15:11:33.901881 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived
[0m15:11:33.905502 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived"
[0m15:11:33.905888 [debug] [Thread-1  ]: finished collecting timing info
[0m15:11:33.905993 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived
[0m15:11:33.907741 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:11:34.281392 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived"
[0m15:11:34.282588 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
  
  
  OPTIONS()
  as (
    


SELECT "4hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_4hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/4)*4 AS _4hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _4hr_trunc,
    app_id,
    event_type)

UNION ALL


SELECT "8hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_8hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/8)*8 AS _8hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _8hr_trunc,
    app_id,
    event_type)

UNION ALL


SELECT "12hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_12hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/12)*12 AS _12hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _12hr_trunc,
    app_id,
    event_type)

UNION ALL


SELECT "24hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_24hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/24)*24 AS _24hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _24hr_trunc,
    app_id,
    event_type)

ORDER BY
  agg_tag,
  time_stamps,
  app_id,
  event_type


  );
  
[0m15:11:39.724435 [debug] [Thread-1  ]: finished collecting timing info
[0m15:11:39.726965 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10708ea60>]}
[0m15:11:39.727745 [info ] [Thread-1  ]: 2 of 38 OK created table model dbt_anomaly_detection.all_agg_derived ........... [[32mCREATE TABLE (2.2k rows, 34.8 MB processed)[0m in 5.83s]
[0m15:11:39.728502 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived
[0m15:11:39.729969 [debug] [Thread-1  ]: Began running node model.anomaly_detection.count_cutoff
[0m15:11:39.730549 [info ] [Thread-1  ]: 3 of 38 START table model dbt_anomaly_detection.count_cutoff ................... [RUN]
[0m15:11:39.731375 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.count_cutoff"
[0m15:11:39.732313 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.count_cutoff
[0m15:11:39.732629 [debug] [Thread-1  ]: Compiling model.anomaly_detection.count_cutoff
[0m15:11:39.742338 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.count_cutoff"
[0m15:11:39.743204 [debug] [Thread-1  ]: finished collecting timing info
[0m15:11:39.743403 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.count_cutoff
[0m15:11:39.749503 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:11:40.186081 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.count_cutoff"
[0m15:11:40.187483 [debug] [Thread-1  ]: On model.anomaly_detection.count_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.count_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select agg_tag, app_event, min(time_stamps) as strt_time
from pairs
where event_count > 50
group by app_event, agg_tag 
order by app_event, agg_tag
  );
  
[0m15:11:42.298294 [debug] [Thread-1  ]: finished collecting timing info
[0m15:11:42.299488 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d60280>]}
[0m15:11:42.300205 [info ] [Thread-1  ]: 3 of 38 OK created table model dbt_anomaly_detection.count_cutoff .............. [[32mCREATE TABLE (8.0 rows, 78.6 KB processed)[0m in 2.57s]
[0m15:11:42.300938 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.count_cutoff
[0m15:11:42.302034 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff
[0m15:11:42.302702 [info ] [Thread-1  ]: 4 of 38 START table model dbt_anomaly_detection.all_agg_derived_cutoff ......... [RUN]
[0m15:11:42.305179 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff"
[0m15:11:42.305659 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff
[0m15:11:42.305936 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff
[0m15:11:42.312895 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m15:11:42.314090 [debug] [Thread-1  ]: finished collecting timing info
[0m15:11:42.314576 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff
[0m15:11:42.317811 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:11:42.731550 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m15:11:42.732643 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select time_stamps, app_event, agg_tag, event_count
from(
select time_stamps, strt_time, pairs.app_event, pairs.agg_tag, event_count
from pairs
inner join `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff` as cutoff
on pairs.agg_tag = cutoff.agg_tag
and pairs.app_event = cutoff.app_event
order by pairs.app_event, pairs.agg_tag, time_stamps)
where time_stamps >= strt_time
  );
  
[0m15:11:45.238956 [debug] [Thread-1  ]: finished collecting timing info
[0m15:11:45.241382 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107076910>]}
[0m15:11:45.242410 [info ] [Thread-1  ]: 4 of 38 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff .... [[32mCREATE TABLE (2.2k rows, 78.8 KB processed)[0m in 2.94s]
[0m15:11:45.243417 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff
[0m15:11:45.244859 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m15:11:45.245378 [info ] [Thread-1  ]: 5 of 38 START table model dbt_anomaly_detection.all_agg_derived_cutoff_long .... [RUN]
[0m15:11:45.246278 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m15:11:45.246723 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_long
[0m15:11:45.247021 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_long
[0m15:11:45.259151 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m15:11:45.260374 [debug] [Thread-1  ]: finished collecting timing info
[0m15:11:45.260628 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_long
[0m15:11:45.264461 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:11:45.683968 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m15:11:45.685145 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB("2023-02-09", INTERVAL 10 DAY)
  );
  
[0m15:11:47.898516 [debug] [Thread-1  ]: finished collecting timing info
[0m15:11:47.900200 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1073b4d00>]}
[0m15:11:47.901066 [info ] [Thread-1  ]: 5 of 38 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_long  [[32mCREATE TABLE (1.9k rows, 76.5 KB processed)[0m in 2.65s]
[0m15:11:47.901940 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m15:11:47.902455 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m15:11:47.902855 [info ] [Thread-1  ]: 6 of 38 START table model dbt_anomaly_detection.all_agg_derived_cutoff_short ... [RUN]
[0m15:11:47.905318 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m15:11:47.905839 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_short
[0m15:11:47.906099 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_short
[0m15:11:47.916841 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m15:11:47.917529 [debug] [Thread-1  ]: finished collecting timing info
[0m15:11:47.917665 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_short
[0m15:11:47.922659 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:11:48.352797 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m15:11:48.355783 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB("2023-02-09", INTERVAL 15 DAY)
  );
  
[0m15:11:50.412895 [debug] [Thread-1  ]: finished collecting timing info
[0m15:11:50.413651 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072c1eb0>]}
[0m15:11:50.414167 [info ] [Thread-1  ]: 6 of 38 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_short  [[32mCREATE TABLE (1.8k rows, 76.5 KB processed)[0m in 2.51s]
[0m15:11:50.414966 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m15:11:50.415254 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_nonrecent_events
[0m15:11:50.415531 [info ] [Thread-1  ]: 7 of 38 START table model dbt_anomaly_detection.derived_nonrecent_events ....... [RUN]
[0m15:11:50.416089 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_nonrecent_events"
[0m15:11:50.416602 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_nonrecent_events
[0m15:11:50.416810 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_nonrecent_events
[0m15:11:50.430817 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m15:11:50.432100 [debug] [Thread-1  ]: finished collecting timing info
[0m15:11:50.432434 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_nonrecent_events
[0m15:11:50.434297 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:11:50.842806 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m15:11:50.843770 [debug] [Thread-1  ]: On model.anomaly_detection.derived_nonrecent_events: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_nonrecent_events"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events`
  
  
  OPTIONS()
  as (
    

SELECT MIN(time_stamps) AS strt_time, app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
GROUP BY app_event
HAVING DATE(MIN(time_stamps)) < DATE_SUB("2023-02-09", INTERVAL 30 DAY)
ORDER BY strt_time
  );
  
[0m15:11:53.213876 [debug] [Thread-1  ]: finished collecting timing info
[0m15:11:53.214754 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107070f40>]}
[0m15:11:53.215465 [info ] [Thread-1  ]: 7 of 38 OK created table model dbt_anomaly_detection.derived_nonrecent_events .. [[32mCREATE TABLE (2.0 rows, 48.5 KB processed)[0m in 2.80s]
[0m15:11:53.216964 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_nonrecent_events
[0m15:11:53.217336 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_long
[0m15:11:53.217931 [info ] [Thread-1  ]: 8 of 38 START table model dbt_anomaly_detection.aggregation_quartiles_long ..... [RUN]
[0m15:11:53.218772 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_long"
[0m15:11:53.219101 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_long
[0m15:11:53.219370 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_long
[0m15:11:53.228444 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m15:11:53.230834 [debug] [Thread-1  ]: finished collecting timing info
[0m15:11:53.231012 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_long
[0m15:11:53.232715 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:11:54.262688 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m15:11:54.264518 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m15:11:56.488081 [debug] [Thread-1  ]: finished collecting timing info
[0m15:11:56.488476 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107226370>]}
[0m15:11:56.488686 [info ] [Thread-1  ]: 8 of 38 OK created table model dbt_anomaly_detection.aggregation_quartiles_long  [[32mCREATE TABLE (8.0 rows, 53.0 KB processed)[0m in 3.27s]
[0m15:11:56.488912 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_long
[0m15:11:56.489020 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_short
[0m15:11:56.489204 [info ] [Thread-1  ]: 9 of 38 START table model dbt_anomaly_detection.aggregation_quartiles_short .... [RUN]
[0m15:11:56.489446 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_short"
[0m15:11:56.489542 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_short
[0m15:11:56.489633 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_short
[0m15:11:56.497185 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m15:11:56.497580 [debug] [Thread-1  ]: finished collecting timing info
[0m15:11:56.497679 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_short
[0m15:11:56.499316 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:11:56.916716 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m15:11:56.917908 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m15:11:58.969004 [debug] [Thread-1  ]: finished collecting timing info
[0m15:11:58.971053 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107072280>]}
[0m15:11:58.972045 [info ] [Thread-1  ]: 9 of 38 OK created table model dbt_anomaly_detection.aggregation_quartiles_short  [[32mCREATE TABLE (8.0 rows, 49.7 KB processed)[0m in 2.48s]
[0m15:11:58.972990 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_short
[0m15:11:58.973395 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_long
[0m15:11:58.973785 [info ] [Thread-1  ]: 10 of 38 START table model dbt_anomaly_detection.aggregation_bounds_long ....... [RUN]
[0m15:11:58.974521 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_long"
[0m15:11:58.975091 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_long
[0m15:11:58.975364 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_long
[0m15:11:58.984198 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m15:11:58.997042 [debug] [Thread-1  ]: finished collecting timing info
[0m15:11:58.997451 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_long
[0m15:11:59.000801 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:11:59.443137 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m15:11:59.447570 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m15:12:01.506722 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:01.507346 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d6da00>]}
[0m15:12:01.507649 [info ] [Thread-1  ]: 10 of 38 OK created table model dbt_anomaly_detection.aggregation_bounds_long .. [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.53s]
[0m15:12:01.507956 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_long
[0m15:12:01.508093 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_short
[0m15:12:01.508356 [info ] [Thread-1  ]: 11 of 38 START table model dbt_anomaly_detection.aggregation_bounds_short ...... [RUN]
[0m15:12:01.508813 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_short"
[0m15:12:01.508936 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_short
[0m15:12:01.509044 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_short
[0m15:12:01.511962 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m15:12:01.513547 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:01.513678 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_short
[0m15:12:01.516787 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:12:01.932046 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m15:12:01.934337 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m15:12:04.426835 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:04.428963 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107392940>]}
[0m15:12:04.429956 [info ] [Thread-1  ]: 11 of 38 OK created table model dbt_anomaly_detection.aggregation_bounds_short . [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.92s]
[0m15:12:04.430846 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_short
[0m15:12:04.431379 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_long
[0m15:12:04.431711 [info ] [Thread-1  ]: 12 of 38 START table model dbt_anomaly_detection.aggregation_outliers_long ..... [RUN]
[0m15:12:04.432388 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_long"
[0m15:12:04.432646 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_long
[0m15:12:04.432907 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_long
[0m15:12:04.447217 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m15:12:04.448200 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:04.448400 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_long
[0m15:12:04.451625 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:12:04.888782 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m15:12:04.890867 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m15:12:07.201150 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:07.203086 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d6a910>]}
[0m15:12:07.204021 [info ] [Thread-1  ]: 12 of 38 OK created table model dbt_anomaly_detection.aggregation_outliers_long  [[32mCREATE TABLE (1.9k rows, 68.3 KB processed)[0m in 2.77s]
[0m15:12:07.205313 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_long
[0m15:12:07.205855 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_short
[0m15:12:07.206403 [info ] [Thread-1  ]: 13 of 38 START table model dbt_anomaly_detection.aggregation_outliers_short .... [RUN]
[0m15:12:07.207485 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_short"
[0m15:12:07.207841 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_short
[0m15:12:07.208152 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_short
[0m15:12:07.223056 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m15:12:07.225097 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:07.225418 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_short
[0m15:12:07.229231 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:12:07.617780 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m15:12:07.620870 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m15:12:09.819403 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:09.820491 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107393070>]}
[0m15:12:09.821655 [info ] [Thread-1  ]: 13 of 38 OK created table model dbt_anomaly_detection.aggregation_outliers_short  [[32mCREATE TABLE (1.8k rows, 64.0 KB processed)[0m in 2.61s]
[0m15:12:09.822447 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_short
[0m15:12:09.822876 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_4hr
[0m15:12:09.823583 [info ] [Thread-1  ]: 14 of 38 START model model dbt_anomaly_detection.derived_models_05mon_4hr ...... [RUN]
[0m15:12:09.826292 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_4hr"
[0m15:12:09.826658 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_4hr
[0m15:12:09.826886 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_4hr
[0m15:12:09.835923 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m15:12:09.836857 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:09.837013 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_4hr
[0m15:12:09.859119 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m15:12:09.859895 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:12:09.860079 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m15:12:10.996092 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m15:12:12.825745 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:12.828282 [debug] [Thread-1  ]: Database Error in model derived_models_05mon_4hr (models/example/derived_models_05mon_4hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_4hr.sql
[0m15:12:12.829027 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107391eb0>]}
[0m15:12:12.830092 [error] [Thread-1  ]: 14 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_05mon_4hr  [[31mERROR[0m in 3.00s]
[0m15:12:12.830840 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_4hr
[0m15:12:12.831169 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_8hr
[0m15:12:12.831476 [info ] [Thread-1  ]: 15 of 38 START model model dbt_anomaly_detection.derived_models_05mon_8hr ...... [RUN]
[0m15:12:12.837195 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_8hr"
[0m15:12:12.837857 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_8hr
[0m15:12:12.838671 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_8hr
[0m15:12:12.851841 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m15:12:12.852891 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:12.853132 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_8hr
[0m15:12:12.857647 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m15:12:12.858214 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:12:12.858468 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m15:12:13.846564 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m15:12:14.907441 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:14.909166 [debug] [Thread-1  ]: Database Error in model derived_models_05mon_8hr (models/example/derived_models_05mon_8hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_8hr.sql
[0m15:12:14.909856 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107336340>]}
[0m15:12:14.910661 [error] [Thread-1  ]: 15 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_05mon_8hr  [[31mERROR[0m in 2.07s]
[0m15:12:14.911566 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_8hr
[0m15:12:14.913002 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_4hr
[0m15:12:14.913678 [info ] [Thread-1  ]: 16 of 38 START model model dbt_anomaly_detection.derived_models_1mon_4hr ....... [RUN]
[0m15:12:14.914507 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_4hr"
[0m15:12:14.914871 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_4hr
[0m15:12:14.915143 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_4hr
[0m15:12:14.926218 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m15:12:14.926881 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:14.927065 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_4hr
[0m15:12:14.930798 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m15:12:14.931287 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:12:14.931496 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m15:12:15.974935 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m15:12:17.052311 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:17.054499 [debug] [Thread-1  ]: Database Error in model derived_models_1mon_4hr (models/example/derived_models_1mon_4hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_4hr.sql
[0m15:12:17.055363 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10736fa90>]}
[0m15:12:17.059126 [error] [Thread-1  ]: 16 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_1mon_4hr  [[31mERROR[0m in 2.14s]
[0m15:12:17.063215 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_4hr
[0m15:12:17.063848 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_8hr
[0m15:12:17.064716 [info ] [Thread-1  ]: 17 of 38 START model model dbt_anomaly_detection.derived_models_1mon_8hr ....... [RUN]
[0m15:12:17.066337 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_8hr"
[0m15:12:17.067365 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_8hr
[0m15:12:17.067665 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_8hr
[0m15:12:17.080409 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m15:12:17.082316 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:17.082549 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_8hr
[0m15:12:17.086620 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m15:12:17.087294 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:12:17.087535 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m15:12:18.052409 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m15:12:24.260599 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:24.261547 [debug] [Thread-1  ]: Database Error in model derived_models_1mon_8hr (models/example/derived_models_1mon_8hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_8hr.sql
[0m15:12:24.262160 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072c0370>]}
[0m15:12:24.262933 [error] [Thread-1  ]: 17 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_1mon_8hr  [[31mERROR[0m in 7.20s]
[0m15:12:24.263807 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_8hr
[0m15:12:24.264276 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_4hr
[0m15:12:24.264795 [info ] [Thread-1  ]: 18 of 38 START model model dbt_anomaly_detection.derived_models_2mon_4hr ....... [RUN]
[0m15:12:24.266459 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_4hr"
[0m15:12:24.269483 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_4hr
[0m15:12:24.269802 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_4hr
[0m15:12:24.280492 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m15:12:24.281576 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:24.281888 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_4hr
[0m15:12:24.285721 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m15:12:24.287163 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:12:24.287366 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "4hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m15:12:25.467605 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m15:12:29.872650 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:29.873406 [debug] [Thread-1  ]: Database Error in model derived_models_2mon_4hr (models/example/derived_models_2mon_4hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_4hr.sql
[0m15:12:29.873823 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12000fc40>]}
[0m15:12:29.874410 [error] [Thread-1  ]: 18 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_2mon_4hr  [[31mERROR[0m in 5.61s]
[0m15:12:29.874905 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_4hr
[0m15:12:29.875137 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_8hr
[0m15:12:29.875479 [info ] [Thread-1  ]: 19 of 38 START model model dbt_anomaly_detection.derived_models_2mon_8hr ....... [RUN]
[0m15:12:29.876438 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_8hr"
[0m15:12:29.876729 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_8hr
[0m15:12:29.877043 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_8hr
[0m15:12:29.887449 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m15:12:29.891744 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:29.892127 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_8hr
[0m15:12:29.895628 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m15:12:29.896094 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:12:29.896213 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
10 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "8hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m15:12:31.108296 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m15:12:32.169358 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:32.174104 [debug] [Thread-1  ]: Database Error in model derived_models_2mon_8hr (models/example/derived_models_2mon_8hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_8hr.sql
[0m15:12:32.175592 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1073cacd0>]}
[0m15:12:32.176634 [error] [Thread-1  ]: 19 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_2mon_8hr  [[31mERROR[0m in 2.30s]
[0m15:12:32.177582 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_8hr
[0m15:12:32.178015 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_12hr
[0m15:12:32.178906 [info ] [Thread-1  ]: 20 of 38 START model model dbt_anomaly_detection.derived_models_05mon_12hr ..... [RUN]
[0m15:12:32.180175 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_12hr"
[0m15:12:32.185770 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_12hr
[0m15:12:32.187324 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_12hr
[0m15:12:32.195509 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m15:12:32.197116 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:32.197436 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_12hr
[0m15:12:32.202199 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m15:12:32.202949 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:12:32.203236 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
30 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m15:12:33.639786 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m15:12:39.333146 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:39.334348 [debug] [Thread-1  ]: Database Error in model derived_models_05mon_12hr (models/example/derived_models_05mon_12hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_12hr.sql
[0m15:12:39.334815 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120073fd0>]}
[0m15:12:39.335407 [error] [Thread-1  ]: 20 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_05mon_12hr  [[31mERROR[0m in 7.15s]
[0m15:12:39.336129 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_12hr
[0m15:12:39.336571 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_24hr
[0m15:12:39.337023 [info ] [Thread-1  ]: 21 of 38 START model model dbt_anomaly_detection.derived_models_05mon_24hr ..... [RUN]
[0m15:12:39.340790 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_24hr"
[0m15:12:39.341179 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_24hr
[0m15:12:39.341442 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_24hr
[0m15:12:39.350756 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m15:12:39.351523 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:39.351693 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_24hr
[0m15:12:39.353335 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m15:12:39.353968 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:12:39.354112 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB("2023-02-09", INTERVAL 30 DAY)
  AND DATE(time_stamps) < DATE_SUB("2023-02-09", INTERVAL 15 DAY)
  AND agg_tag = "24hr"
    );
    
[0m15:12:50.322248 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:50.325045 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120073fa0>]}
[0m15:12:50.326065 [info ] [Thread-1  ]: 21 of 38 OK created model model dbt_anomaly_detection.derived_models_05mon_24hr  [[32mOK[0m in 10.98s]
[0m15:12:50.326992 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_24hr
[0m15:12:50.327457 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_12hr
[0m15:12:50.327971 [info ] [Thread-1  ]: 22 of 38 START model model dbt_anomaly_detection.derived_models_1mon_12hr ...... [RUN]
[0m15:12:50.328834 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_12hr"
[0m15:12:50.329180 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_12hr
[0m15:12:50.329644 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_12hr
[0m15:12:50.341160 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m15:12:50.341566 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:50.341661 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_12hr
[0m15:12:50.343913 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m15:12:50.344443 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:12:50.344698 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m15:12:51.296694 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m15:12:53.272549 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:53.274124 [debug] [Thread-1  ]: Database Error in model derived_models_1mon_12hr (models/example/derived_models_1mon_12hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_12hr.sql
[0m15:12:53.274840 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120028790>]}
[0m15:12:53.275763 [error] [Thread-1  ]: 22 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_1mon_12hr  [[31mERROR[0m in 2.95s]
[0m15:12:53.276722 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_12hr
[0m15:12:53.277214 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_24hr
[0m15:12:53.277742 [info ] [Thread-1  ]: 23 of 38 START model model dbt_anomaly_detection.derived_models_1mon_24hr ...... [RUN]
[0m15:12:53.278855 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_24hr"
[0m15:12:53.279313 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_24hr
[0m15:12:53.279650 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_24hr
[0m15:12:53.293746 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m15:12:53.294853 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:53.295110 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_24hr
[0m15:12:53.298761 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m15:12:53.299357 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:12:53.299610 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
60 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m15:12:54.251332 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m15:12:59.088246 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:59.089076 [debug] [Thread-1  ]: Database Error in model derived_models_1mon_24hr (models/example/derived_models_1mon_24hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_24hr.sql
[0m15:12:59.089408 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120088fd0>]}
[0m15:12:59.089855 [error] [Thread-1  ]: 23 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_1mon_24hr  [[31mERROR[0m in 5.81s]
[0m15:12:59.090500 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_24hr
[0m15:12:59.090853 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_12hr
[0m15:12:59.092492 [info ] [Thread-1  ]: 24 of 38 START model model dbt_anomaly_detection.derived_models_2mon_12hr ...... [RUN]
[0m15:12:59.094477 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_12hr"
[0m15:12:59.095087 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_12hr
[0m15:12:59.095381 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_12hr
[0m15:12:59.107142 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m15:12:59.107664 [debug] [Thread-1  ]: finished collecting timing info
[0m15:12:59.107791 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_12hr
[0m15:12:59.109634 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m15:12:59.109985 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:12:59.110095 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "12hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m15:13:00.289848 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m15:13:01.713875 [debug] [Thread-1  ]: finished collecting timing info
[0m15:13:01.715902 [debug] [Thread-1  ]: Database Error in model derived_models_2mon_12hr (models/example/derived_models_2mon_12hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_12hr.sql
[0m15:13:01.716734 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120141d00>]}
[0m15:13:01.717852 [error] [Thread-1  ]: 24 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_2mon_12hr  [[31mERROR[0m in 2.62s]
[0m15:13:01.719066 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_12hr
[0m15:13:01.719478 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_24hr
[0m15:13:01.719877 [info ] [Thread-1  ]: 25 of 38 START model model dbt_anomaly_detection.derived_models_2mon_24hr ...... [RUN]
[0m15:13:01.720856 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_24hr"
[0m15:13:01.721649 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_24hr
[0m15:13:01.722005 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_24hr
[0m15:13:01.735215 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m15:13:01.736382 [debug] [Thread-1  ]: finished collecting timing info
[0m15:13:01.736656 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_24hr
[0m15:13:01.741324 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m15:13:01.742152 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:13:01.742455 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
-- the highest horizon required for ml.detect func not to pass nulls 
-- horizon = 30 would result in 6 nulls for 4hr agg, 3 nulls for 8hr agg, 2 nulls for 12hr agg and 1 null for 24hr agg. 
WITH date_control as (
SELECT 
"2023-02-09" AS cur_date
),

interval_control as (
SELECT 
90 AS training_interval -- SET TRAINING INTERVAL HERE for PROD
),

lookback as (
SELECT 
15 AS lookback_interval -- SET TRAINING INTERVAL HERE for PROD
FROM interval_control
)
--- Code above is for dev/ local only. Code below is for model.
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB(                      --- training on sep 17 to sep 20 if dev 
    (SELECT cur_date FROM date_control)
  , INTERVAL (SELECT training_interval FROM interval_control) 
  DAY) AND DATE(time_stamps) < DATE_SUB(
    (SELECT cur_date FROM date_control)
    , INTERVAL (SELECT lookback_interval from lookback)
    DAY)
  AND agg_tag = "24hr"
  -- AND DATE(time_stamps) NOT BETWEEN "2022-11-23" AND "2022-11-30" 
  -- black friday surges affect the distribution of the training set and therefore, will affect the forecast in an unprecented way. 
  -- thus, excluding the last week of November from the train set
    );
    
[0m15:13:02.706911 [debug] [Thread-1  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]')
[0m15:13:04.580427 [debug] [Thread-1  ]: finished collecting timing info
[0m15:13:04.581297 [debug] [Thread-1  ]: Database Error in model derived_models_2mon_24hr (models/example/derived_models_2mon_24hr.sql)
  No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
  compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_24hr.sql
[0m15:13:04.581964 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf6ee2e-da6e-4870-ae9a-abc17d34fb87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120113910>]}
[0m15:13:05.137026 [error] [Thread-1  ]: 25 of 38 ERROR creating model model dbt_anomaly_detection.derived_models_2mon_24hr  [[31mERROR[0m in 2.86s]
[0m15:13:05.139104 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_24hr
[0m15:13:05.141496 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ml_detect
[0m15:13:05.142080 [info ] [Thread-1  ]: 26 of 38 SKIP relation dbt_anomaly_detection.derived_ml_detect ................. [[33mSKIP[0m]
[0m15:13:05.142996 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ml_detect
[0m15:13:05.144781 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ml_detect_tweaked
[0m15:13:05.145368 [info ] [Thread-1  ]: 27 of 38 SKIP relation dbt_anomaly_detection.ml_detect_tweaked ................. [[33mSKIP[0m]
[0m15:13:05.146045 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ml_detect_tweaked
[0m15:13:05.147233 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_model_features_dbt
[0m15:13:05.147640 [info ] [Thread-1  ]: 28 of 38 SKIP relation dbt_anomaly_detection.derived_model_features_dbt ........ [[33mSKIP[0m]
[0m15:13:05.148168 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_model_features_dbt
[0m15:13:05.150189 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_model_features_dbt
[0m15:13:05.151046 [info ] [Thread-1  ]: 29 of 38 SKIP relation dbt_anomaly_detection.filtered_model_features_dbt ....... [[33mSKIP[0m]
[0m15:13:05.151555 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_model_features_dbt
[0m15:13:05.155854 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ref_distinct_tuples
[0m15:13:05.156519 [info ] [Thread-1  ]: 30 of 38 SKIP relation dbt_anomaly_detection.ref_distinct_tuples ............... [[33mSKIP[0m]
[0m15:13:05.157357 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ref_distinct_tuples
[0m15:13:05.157724 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_features_null_filtered
[0m15:13:05.157941 [info ] [Thread-1  ]: 31 of 38 SKIP relation dbt_anomaly_detection.remaining_events_features_null_filtered  [[33mSKIP[0m]
[0m15:13:05.158778 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_features_null_filtered
[0m15:13:05.159676 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies
[0m15:13:05.160470 [info ] [Thread-1  ]: 32 of 38 SKIP relation dbt_anomaly_detection.remaining_events_min_anomalies .... [[33mSKIP[0m]
[0m15:13:05.161276 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies
[0m15:13:05.162469 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m15:13:05.162808 [info ] [Thread-1  ]: 33 of 38 SKIP relation dbt_anomaly_detection.remaining_events_min_anomalies_results  [[33mSKIP[0m]
[0m15:13:05.163308 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m15:13:05.163972 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD
[0m15:13:05.164316 [info ] [Thread-1  ]: 34 of 38 SKIP relation dbt_anomaly_detection.remaining_events_min_RMSD ......... [[33mSKIP[0m]
[0m15:13:05.164696 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD
[0m15:13:05.165097 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m15:13:05.165555 [info ] [Thread-1  ]: 35 of 38 SKIP relation dbt_anomaly_detection.remaining_events_min_RMSD_results . [[33mSKIP[0m]
[0m15:13:05.165893 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m15:13:05.166606 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_all_models
[0m15:13:05.166868 [info ] [Thread-1  ]: 36 of 38 SKIP relation dbt_anomaly_detection.filtered_all_models ............... [[33mSKIP[0m]
[0m15:13:05.167388 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_all_models
[0m15:13:05.168022 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m15:13:05.168377 [info ] [Thread-1  ]: 37 of 38 SKIP relation dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [[33mSKIP[0m]
[0m15:13:05.168852 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m15:13:05.169313 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_events_with_anomalies
[0m15:13:05.169681 [info ] [Thread-1  ]: 38 of 38 SKIP relation dbt_anomaly_detection.derived_events_with_anomalies ..... [[33mSKIP[0m]
[0m15:13:05.170055 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_events_with_anomalies
[0m15:13:05.171421 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m15:13:05.172303 [info ] [MainThread]: 
[0m15:13:05.172557 [info ] [MainThread]: Finished running 26 table models, 12 model models in 0 hours 1 minutes and 37.41 seconds (97.41s).
[0m15:13:05.172762 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:13:05.172872 [debug] [MainThread]: Connection 'model.anomaly_detection.derived_models_2mon_24hr' was properly closed.
[0m15:13:05.189258 [info ] [MainThread]: 
[0m15:13:05.189539 [info ] [MainThread]: [31mCompleted with 11 errors and 0 warnings:[0m
[0m15:13:05.189734 [info ] [MainThread]: 
[0m15:13:05.189895 [error] [MainThread]: [33mDatabase Error in model derived_models_05mon_4hr (models/example/derived_models_05mon_4hr.sql)[0m
[0m15:13:05.190054 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m15:13:05.190201 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_4hr.sql
[0m15:13:05.190350 [info ] [MainThread]: 
[0m15:13:05.190501 [error] [MainThread]: [33mDatabase Error in model derived_models_05mon_8hr (models/example/derived_models_05mon_8hr.sql)[0m
[0m15:13:05.190650 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m15:13:05.190794 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_8hr.sql
[0m15:13:05.190944 [info ] [MainThread]: 
[0m15:13:05.191092 [error] [MainThread]: [33mDatabase Error in model derived_models_1mon_4hr (models/example/derived_models_1mon_4hr.sql)[0m
[0m15:13:05.191221 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m15:13:05.191348 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_4hr.sql
[0m15:13:05.191475 [info ] [MainThread]: 
[0m15:13:05.191603 [error] [MainThread]: [33mDatabase Error in model derived_models_1mon_8hr (models/example/derived_models_1mon_8hr.sql)[0m
[0m15:13:05.191733 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m15:13:05.191863 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_8hr.sql
[0m15:13:05.191990 [info ] [MainThread]: 
[0m15:13:05.192118 [error] [MainThread]: [33mDatabase Error in model derived_models_2mon_4hr (models/example/derived_models_2mon_4hr.sql)[0m
[0m15:13:05.192244 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m15:13:05.192371 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_4hr.sql
[0m15:13:05.192499 [info ] [MainThread]: 
[0m15:13:05.192626 [error] [MainThread]: [33mDatabase Error in model derived_models_2mon_8hr (models/example/derived_models_2mon_8hr.sql)[0m
[0m15:13:05.192752 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m15:13:05.192878 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_8hr.sql
[0m15:13:05.193005 [info ] [MainThread]: 
[0m15:13:05.193137 [error] [MainThread]: [33mDatabase Error in model derived_models_05mon_12hr (models/example/derived_models_05mon_12hr.sql)[0m
[0m15:13:05.193262 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m15:13:05.193391 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_05mon_12hr.sql
[0m15:13:05.193517 [info ] [MainThread]: 
[0m15:13:05.193644 [error] [MainThread]: [33mDatabase Error in model derived_models_1mon_12hr (models/example/derived_models_1mon_12hr.sql)[0m
[0m15:13:05.193770 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m15:13:05.193897 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_12hr.sql
[0m15:13:05.194021 [info ] [MainThread]: 
[0m15:13:05.194146 [error] [MainThread]: [33mDatabase Error in model derived_models_1mon_24hr (models/example/derived_models_1mon_24hr.sql)[0m
[0m15:13:05.194272 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m15:13:05.194397 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_1mon_24hr.sql
[0m15:13:05.194524 [info ] [MainThread]: 
[0m15:13:05.194651 [error] [MainThread]: [33mDatabase Error in model derived_models_2mon_12hr (models/example/derived_models_2mon_12hr.sql)[0m
[0m15:13:05.194776 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m15:13:05.194901 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_12hr.sql
[0m15:13:05.195028 [info ] [MainThread]: 
[0m15:13:05.195153 [error] [MainThread]: [33mDatabase Error in model derived_models_2mon_24hr (models/example/derived_models_2mon_24hr.sql)[0m
[0m15:13:05.195278 [error] [MainThread]:   No matching signature for function DATE_SUB for argument types: STRING, INTERVAL INT64 DATE_TIME_PART. Supported signatures: DATE_SUB(DATE, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(DATETIME, INTERVAL INT64 DATE_TIME_PART); DATE_SUB(TIMESTAMP, INTERVAL INT64 DATE_TIME_PART) at [33:24]
[0m15:13:05.195400 [error] [MainThread]:   compiled SQL at target/run/anomaly_detection/models/example/derived_models_2mon_24hr.sql
[0m15:13:05.195556 [info ] [MainThread]: 
[0m15:13:05.195685 [info ] [MainThread]: Done. PASS=14 WARN=0 ERROR=11 SKIP=13 TOTAL=38
[0m15:13:05.195917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1201520d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f54b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12016d8b0>]}
[0m15:13:05.196091 [debug] [MainThread]: Flushing usage events


============================== 2023-02-13 15:18:51.804482 | ff82a924-3569-4e7c-98fa-97df52185d08 ==============================
[0m15:18:51.804522 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:18:51.807320 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/rana/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m15:18:51.807446 [debug] [MainThread]: Tracking: tracking
[0m15:18:51.818149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d31b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d31a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d311c0>]}
[0m15:18:51.869241 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 11 files changed.
[0m15:18:51.869553 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_05mon_4hr.sql
[0m15:18:51.869667 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_2mon_4hr.sql
[0m15:18:51.869771 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_1mon_12hr.sql
[0m15:18:51.869872 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_05mon_12hr.sql
[0m15:18:51.869971 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_1mon_24hr.sql
[0m15:18:51.870068 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_05mon_8hr.sql
[0m15:18:51.870164 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_1mon_8hr.sql
[0m15:18:51.870261 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_2mon_8hr.sql
[0m15:18:51.870356 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_2mon_24hr.sql
[0m15:18:51.870452 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_2mon_12hr.sql
[0m15:18:51.870548 [debug] [MainThread]: Partial parsing: updated file: anomaly_detection://models/example/derived_models_1mon_4hr.sql
[0m15:18:51.878645 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_4hr.sql
[0m15:18:51.886078 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_4hr.sql
[0m15:18:51.886695 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_4hr.sql
[0m15:18:51.889288 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_4hr.sql
[0m15:18:51.889849 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_12hr.sql
[0m15:18:51.892450 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_12hr.sql
[0m15:18:51.892983 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_12hr.sql
[0m15:18:51.895469 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_12hr.sql
[0m15:18:51.896068 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_24hr.sql
[0m15:18:51.899203 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_24hr.sql
[0m15:18:51.899888 [debug] [MainThread]: 1603: static parser failed on example/derived_models_05mon_8hr.sql
[0m15:18:51.902412 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_05mon_8hr.sql
[0m15:18:51.902988 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_8hr.sql
[0m15:18:51.905999 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_8hr.sql
[0m15:18:51.906530 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_8hr.sql
[0m15:18:51.908959 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_8hr.sql
[0m15:18:51.909515 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_24hr.sql
[0m15:18:51.911944 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_24hr.sql
[0m15:18:51.912509 [debug] [MainThread]: 1603: static parser failed on example/derived_models_2mon_12hr.sql
[0m15:18:51.915354 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_2mon_12hr.sql
[0m15:18:51.915918 [debug] [MainThread]: 1603: static parser failed on example/derived_models_1mon_4hr.sql
[0m15:18:51.918700 [debug] [MainThread]: 1602: parser fallback to jinja rendering on example/derived_models_1mon_4hr.sql
[0m15:18:51.956471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112aab4c0>]}
[0m15:18:51.963359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d79d00>]}
[0m15:18:51.963502 [info ] [MainThread]: Found 38 models, 0 tests, 0 snapshots, 0 analyses, 540 macros, 0 operations, 0 seed files, 2 sources, 0 exposures, 0 metrics
[0m15:18:51.963632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129a6b20>]}
[0m15:18:51.964789 [info ] [MainThread]: 
[0m15:18:51.965008 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m15:18:51.966252 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow"
[0m15:18:51.966381 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:18:53.100101 [debug] [ThreadPool]: Acquiring new bigquery connection "list_ld-snowplow_dbt_anomaly_detection"
[0m15:18:53.100355 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:18:53.495457 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129a2880>]}
[0m15:18:53.496812 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:18:53.497198 [info ] [MainThread]: 
[0m15:18:53.503867 [debug] [Thread-1  ]: Began running node model.anomaly_detection.reference_derived
[0m15:18:53.504432 [info ] [Thread-1  ]: 1 of 38 START table model dbt_anomaly_detection.reference_derived .............. [RUN]
[0m15:18:53.504980 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.reference_derived"
[0m15:18:53.505147 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.reference_derived
[0m15:18:53.505401 [debug] [Thread-1  ]: Compiling model.anomaly_detection.reference_derived
[0m15:18:53.510494 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.reference_derived"
[0m15:18:53.510999 [debug] [Thread-1  ]: finished collecting timing info
[0m15:18:53.511149 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.reference_derived
[0m15:18:53.524513 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:18:53.940675 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.reference_derived"
[0m15:18:53.941084 [debug] [Thread-1  ]: On model.anomaly_detection.reference_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.reference_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived`
  
  
  OPTIONS()
  as (
    
SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`web_purchase_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB("2023-02-09", INTERVAL 90 DAY) AND DATE(collector_tstamp) < "2023-02-09"

UNION ALL

SELECT collector_tstamp, event, user_event_name, app_id
FROM `ld-snowplow`.`dbt_rhashemi`.`pco_web_apply_filter_exported`
WHERE DATE(collector_tstamp) >= DATE_SUB("2023-02-09", INTERVAL 90 DAY) AND DATE(collector_tstamp) < "2023-02-09"
  );
  
[0m15:18:57.965064 [debug] [Thread-1  ]: finished collecting timing info
[0m15:18:57.965815 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112aa0f70>]}
[0m15:18:57.966239 [info ] [Thread-1  ]: 1 of 38 OK created table model dbt_anomaly_detection.reference_derived ......... [[32mCREATE TABLE (1.1m rows, 34.8 MB processed)[0m in 4.46s]
[0m15:18:57.966700 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.reference_derived
[0m15:18:57.967322 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived
[0m15:18:57.967689 [info ] [Thread-1  ]: 2 of 38 START table model dbt_anomaly_detection.all_agg_derived ................ [RUN]
[0m15:18:57.968300 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived"
[0m15:18:57.968447 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived
[0m15:18:57.968583 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived
[0m15:18:57.973283 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived"
[0m15:18:57.975386 [debug] [Thread-1  ]: finished collecting timing info
[0m15:18:57.975565 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived
[0m15:18:57.977607 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:18:58.433045 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived"
[0m15:18:58.434436 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
  
  
  OPTIONS()
  as (
    


SELECT "4hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_4hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/4)*4 AS _4hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _4hr_trunc,
    app_id,
    event_type)

UNION ALL


SELECT "8hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_8hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/8)*8 AS _8hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _8hr_trunc,
    app_id,
    event_type)

UNION ALL


SELECT "12hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_12hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/12)*12 AS _12hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _12hr_trunc,
    app_id,
    event_type)

UNION ALL


SELECT "24hr" AS agg_tag,
  PARSE_TIMESTAMP("%F %H", CONCAT(date_trunc, ' ', CAST(_24hr_trunc AS STRING))) AS time_stamps,
  app_id,
  event_type,
  event_count
FROM (
  SELECT FORMAT_TIMESTAMP("%F", collector_tstamp) AS date_trunc,
    FLOOR(CAST(FORMAT_TIMESTAMP("%H", collector_tstamp) AS INT64)/24)*24 AS _24hr_trunc,
    app_id,
    event_type,
    COUNT(event_type) AS event_count
  FROM (
    SELECT collector_tstamp, app_id,
      CASE WHEN event = "unstruct" THEN user_event_name ELSE event
      END AS event_type
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`reference_derived` )
  GROUP BY
    date_trunc,
    _24hr_trunc,
    app_id,
    event_type)

ORDER BY
  agg_tag,
  time_stamps,
  app_id,
  event_type


  );
  
[0m15:19:03.101034 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:03.102096 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112af7f40>]}
[0m15:19:03.102635 [info ] [Thread-1  ]: 2 of 38 OK created table model dbt_anomaly_detection.all_agg_derived ........... [[32mCREATE TABLE (2.2k rows, 34.8 MB processed)[0m in 5.13s]
[0m15:19:03.103192 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived
[0m15:19:03.103898 [debug] [Thread-1  ]: Began running node model.anomaly_detection.count_cutoff
[0m15:19:03.104215 [info ] [Thread-1  ]: 3 of 38 START table model dbt_anomaly_detection.count_cutoff ................... [RUN]
[0m15:19:03.104665 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.count_cutoff"
[0m15:19:03.104824 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.count_cutoff
[0m15:19:03.104981 [debug] [Thread-1  ]: Compiling model.anomaly_detection.count_cutoff
[0m15:19:03.109634 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.count_cutoff"
[0m15:19:03.110454 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:03.110616 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.count_cutoff
[0m15:19:03.113302 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:19:03.506439 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.count_cutoff"
[0m15:19:03.507650 [debug] [Thread-1  ]: On model.anomaly_detection.count_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.count_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select agg_tag, app_event, min(time_stamps) as strt_time
from pairs
where event_count > 50
group by app_event, agg_tag 
order by app_event, agg_tag
  );
  
[0m15:19:05.600535 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:05.602705 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d4a1c0>]}
[0m15:19:05.603857 [info ] [Thread-1  ]: 3 of 38 OK created table model dbt_anomaly_detection.count_cutoff .............. [[32mCREATE TABLE (8.0 rows, 78.6 KB processed)[0m in 2.50s]
[0m15:19:05.605032 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.count_cutoff
[0m15:19:05.607219 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff
[0m15:19:05.607879 [info ] [Thread-1  ]: 4 of 38 START table model dbt_anomaly_detection.all_agg_derived_cutoff ......... [RUN]
[0m15:19:05.608580 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff"
[0m15:19:05.608787 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff
[0m15:19:05.608985 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff
[0m15:19:05.619874 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m15:19:05.620675 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:05.620838 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff
[0m15:19:05.623712 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:19:06.008938 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff"
[0m15:19:06.010374 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
  
  
  OPTIONS()
  as (
    

WITH pairs AS (
  SELECT CONCAT(app_id, '_', event_type) AS app_event, time_stamps, event_count, agg_tag  
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived`
)

select time_stamps, app_event, agg_tag, event_count
from(
select time_stamps, strt_time, pairs.app_event, pairs.agg_tag, event_count
from pairs
inner join `ld-snowplow`.`dbt_anomaly_detection`.`count_cutoff` as cutoff
on pairs.agg_tag = cutoff.agg_tag
and pairs.app_event = cutoff.app_event
order by pairs.app_event, pairs.agg_tag, time_stamps)
where time_stamps >= strt_time
  );
  
[0m15:19:08.072799 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:08.073857 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d6b0d0>]}
[0m15:19:08.074406 [info ] [Thread-1  ]: 4 of 38 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff .... [[32mCREATE TABLE (2.2k rows, 78.8 KB processed)[0m in 2.47s]
[0m15:19:08.074983 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff
[0m15:19:08.076003 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m15:19:08.076552 [info ] [Thread-1  ]: 5 of 38 START table model dbt_anomaly_detection.all_agg_derived_cutoff_long .... [RUN]
[0m15:19:08.077160 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m15:19:08.077340 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_long
[0m15:19:08.077506 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_long
[0m15:19:08.082037 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m15:19:08.082836 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:08.083007 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_long
[0m15:19:08.085926 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:19:08.470833 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_long"
[0m15:19:08.471789 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB("2023-02-09", INTERVAL 10 DAY)
  );
  
[0m15:19:10.642772 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:10.643777 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d5e1f0>]}
[0m15:19:10.644334 [info ] [Thread-1  ]: 5 of 38 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_long  [[32mCREATE TABLE (1.9k rows, 76.5 KB processed)[0m in 2.57s]
[0m15:19:10.644883 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_long
[0m15:19:10.645098 [debug] [Thread-1  ]: Began running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m15:19:10.645802 [info ] [Thread-1  ]: 6 of 38 START table model dbt_anomaly_detection.all_agg_derived_cutoff_short ... [RUN]
[0m15:19:10.646699 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m15:19:10.646885 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.all_agg_derived_cutoff_short
[0m15:19:10.647055 [debug] [Thread-1  ]: Compiling model.anomaly_detection.all_agg_derived_cutoff_short
[0m15:19:10.651379 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m15:19:10.652109 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:10.652251 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.all_agg_derived_cutoff_short
[0m15:19:10.654987 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:19:11.060319 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.all_agg_derived_cutoff_short"
[0m15:19:11.061388 [debug] [Thread-1  ]: On model.anomaly_detection.all_agg_derived_cutoff_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.all_agg_derived_cutoff_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
where DATE(time_stamps) < DATE_SUB("2023-02-09", INTERVAL 15 DAY)
  );
  
[0m15:19:13.229312 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:13.229688 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d4eee0>]}
[0m15:19:13.229945 [info ] [Thread-1  ]: 6 of 38 OK created table model dbt_anomaly_detection.all_agg_derived_cutoff_short  [[32mCREATE TABLE (1.8k rows, 76.5 KB processed)[0m in 2.58s]
[0m15:19:13.230220 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.all_agg_derived_cutoff_short
[0m15:19:13.230345 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_nonrecent_events
[0m15:19:13.230567 [info ] [Thread-1  ]: 7 of 38 START table model dbt_anomaly_detection.derived_nonrecent_events ....... [RUN]
[0m15:19:13.230938 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_nonrecent_events"
[0m15:19:13.231045 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_nonrecent_events
[0m15:19:13.231139 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_nonrecent_events
[0m15:19:13.234973 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m15:19:13.235432 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:13.235539 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_nonrecent_events
[0m15:19:13.237380 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:19:13.659468 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_nonrecent_events"
[0m15:19:13.662110 [debug] [Thread-1  ]: On model.anomaly_detection.derived_nonrecent_events: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_nonrecent_events"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events`
  
  
  OPTIONS()
  as (
    

SELECT MIN(time_stamps) AS strt_time, app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`
GROUP BY app_event
HAVING DATE(MIN(time_stamps)) < DATE_SUB("2023-02-09", INTERVAL 30 DAY)
ORDER BY strt_time
  );
  
[0m15:19:15.812978 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:15.813821 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112af1370>]}
[0m15:19:15.814233 [info ] [Thread-1  ]: 7 of 38 OK created table model dbt_anomaly_detection.derived_nonrecent_events .. [[32mCREATE TABLE (2.0 rows, 48.5 KB processed)[0m in 2.58s]
[0m15:19:15.814613 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_nonrecent_events
[0m15:19:15.814769 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_long
[0m15:19:15.815056 [info ] [Thread-1  ]: 8 of 38 START table model dbt_anomaly_detection.aggregation_quartiles_long ..... [RUN]
[0m15:19:15.815444 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_long"
[0m15:19:15.815577 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_long
[0m15:19:15.815698 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_long
[0m15:19:15.820645 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m15:19:15.821240 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:15.821348 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_long
[0m15:19:15.823502 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:19:16.202320 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_long"
[0m15:19:16.204881 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m15:19:18.423308 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:18.423986 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d4bdc0>]}
[0m15:19:18.424385 [info ] [Thread-1  ]: 8 of 38 OK created table model dbt_anomaly_detection.aggregation_quartiles_long  [[32mCREATE TABLE (8.0 rows, 53.0 KB processed)[0m in 2.61s]
[0m15:19:18.424821 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_long
[0m15:19:18.425021 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_quartiles_short
[0m15:19:18.425241 [info ] [Thread-1  ]: 9 of 38 START table model dbt_anomaly_detection.aggregation_quartiles_short .... [RUN]
[0m15:19:18.426153 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_quartiles_short"
[0m15:19:18.426406 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_quartiles_short
[0m15:19:18.426557 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_quartiles_short
[0m15:19:18.430194 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m15:19:18.430745 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:18.430890 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_quartiles_short
[0m15:19:18.434831 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:19:18.840013 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_quartiles_short"
[0m15:19:18.840902 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_quartiles_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_quartiles_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`
  
  
  OPTIONS()
  as (
    

select   ARRAY(SELECT x FROM UNNEST(output) AS x WITH OFFSET
  WHERE OFFSET BETWEEN 1 AND ARRAY_LENGTH(output) - 2) as output, 
  app_event, agg_tag
  from (
select APPROX_QUANTILES(event_count, 4) AS output, app_event
, agg_tag
from `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short`
group by app_event, agg_tag
order by app_event, agg_tag )
  );
  
[0m15:19:21.428900 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:21.431148 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d682e0>]}
[0m15:19:21.431958 [info ] [Thread-1  ]: 9 of 38 OK created table model dbt_anomaly_detection.aggregation_quartiles_short  [[32mCREATE TABLE (8.0 rows, 49.7 KB processed)[0m in 3.00s]
[0m15:19:21.432483 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_quartiles_short
[0m15:19:21.432693 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_long
[0m15:19:21.433093 [info ] [Thread-1  ]: 10 of 38 START table model dbt_anomaly_detection.aggregation_bounds_long ....... [RUN]
[0m15:19:21.433743 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_long"
[0m15:19:21.433890 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_long
[0m15:19:21.434031 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_long
[0m15:19:21.438165 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m15:19:21.438904 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:21.439054 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_long
[0m15:19:21.441373 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:19:21.858914 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_long"
[0m15:19:21.859896 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_long`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m15:19:23.927016 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:23.928872 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d5d3d0>]}
[0m15:19:23.929643 [info ] [Thread-1  ]: 10 of 38 OK created table model dbt_anomaly_detection.aggregation_bounds_long .. [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.50s]
[0m15:19:23.930646 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_long
[0m15:19:23.931245 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_bounds_short
[0m15:19:23.931544 [info ] [Thread-1  ]: 11 of 38 START table model dbt_anomaly_detection.aggregation_bounds_short ...... [RUN]
[0m15:19:23.932270 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_bounds_short"
[0m15:19:23.932550 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_bounds_short
[0m15:19:23.932826 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_bounds_short
[0m15:19:23.941760 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m15:19:23.942740 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:23.942948 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_bounds_short
[0m15:19:23.946157 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:19:24.316846 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_bounds_short"
[0m15:19:24.317366 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_bounds_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_bounds_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short`
  
  
  OPTIONS()
  as (
    

with temp as (
select quarts, app_event, agg_tag from(
select *
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_quartiles_short`,
unnest(output) as quarts)),

temp_1 as (
select app_event, agg_tag, max(quarts) as q3, min(quarts) as q1
from temp
group by app_event, agg_tag),

temp_2 as (
select app_event, agg_tag, q3, q1, q3-q1 as IQR 
from temp_1)

select app_event, agg_tag, (q1-4.5*IQR) as LB, (q3+4.5*IQR) as UB
from temp_2
order by app_event, agg_tag
  );
  
[0m15:19:26.406861 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:26.409257 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d4d6a0>]}
[0m15:19:26.410346 [info ] [Thread-1  ]: 11 of 38 OK created table model dbt_anomaly_detection.aggregation_bounds_short . [[32mCREATE TABLE (8.0 rows, 356.0 Bytes processed)[0m in 2.48s]
[0m15:19:26.411201 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_bounds_short
[0m15:19:26.411621 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_long
[0m15:19:26.412491 [info ] [Thread-1  ]: 12 of 38 START table model dbt_anomaly_detection.aggregation_outliers_long ..... [RUN]
[0m15:19:26.413223 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_long"
[0m15:19:26.413436 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_long
[0m15:19:26.413642 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_long
[0m15:19:26.422155 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m15:19:26.423171 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:26.423380 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_long
[0m15:19:26.428343 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:19:26.696364 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_long"
[0m15:19:26.699120 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_long: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_long"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_long`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_long` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_long` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m15:19:28.885174 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:28.886579 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112dd1730>]}
[0m15:19:28.887296 [info ] [Thread-1  ]: 12 of 38 OK created table model dbt_anomaly_detection.aggregation_outliers_long  [[32mCREATE TABLE (1.9k rows, 68.3 KB processed)[0m in 2.47s]
[0m15:19:28.887942 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_long
[0m15:19:28.888232 [debug] [Thread-1  ]: Began running node model.anomaly_detection.aggregation_outliers_short
[0m15:19:28.888955 [info ] [Thread-1  ]: 13 of 38 START table model dbt_anomaly_detection.aggregation_outliers_short .... [RUN]
[0m15:19:28.889841 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.aggregation_outliers_short"
[0m15:19:28.890047 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.aggregation_outliers_short
[0m15:19:28.890218 [debug] [Thread-1  ]: Compiling model.anomaly_detection.aggregation_outliers_short
[0m15:19:28.895158 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m15:19:28.895886 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:28.896038 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.aggregation_outliers_short
[0m15:19:28.898733 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:19:29.363684 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.aggregation_outliers_short"
[0m15:19:29.365003 [debug] [Thread-1  ]: On model.anomaly_detection.aggregation_outliers_short: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.aggregation_outliers_short"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
  
  
  OPTIONS()
  as (
    

with bounds_agg as (
select time_stamps, bounds.app_event as app_event, bounds.agg_tag as agg_tag, event_count, LB, UB
from `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_bounds_short` as bounds
inner join `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff_short` as aggs
on bounds.app_event = aggs.app_event
and bounds.agg_tag = aggs.agg_tag
order by bounds.app_event, bounds.agg_tag)

select time_stamps, app_event, agg_tag,
case when event_count > UB then UB
when event_count < LB then LB
else event_count
end as event_count
from bounds_agg
order by app_event, agg_tag, time_stamps
  );
  
[0m15:19:31.967897 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:31.970000 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d9bf10>]}
[0m15:19:31.971030 [info ] [Thread-1  ]: 13 of 38 OK created table model dbt_anomaly_detection.aggregation_outliers_short  [[32mCREATE TABLE (1.8k rows, 64.0 KB processed)[0m in 3.08s]
[0m15:19:31.971558 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.aggregation_outliers_short
[0m15:19:31.972775 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_12hr
[0m15:19:31.972991 [info ] [Thread-1  ]: 14 of 38 START model model dbt_anomaly_detection.derived_models_05mon_12hr ..... [RUN]
[0m15:19:31.973342 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_12hr"
[0m15:19:31.973441 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_12hr
[0m15:19:31.973539 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_12hr
[0m15:19:31.977053 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m15:19:31.977654 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:31.977774 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_12hr
[0m15:19:31.990130 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_12hr"
[0m15:19:31.990503 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:19:31.990619 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB("2023-02-09", INTERVAL 30 DAY)
  AND DATE(time_stamps) < DATE_SUB("2023-02-09", INTERVAL 15 DAY)
  AND agg_tag = "12hr"
    );
    
[0m15:19:42.817350 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:42.819052 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a9f430>]}
[0m15:19:42.819765 [info ] [Thread-1  ]: 14 of 38 OK created model model dbt_anomaly_detection.derived_models_05mon_12hr  [[32mOK[0m in 10.85s]
[0m15:19:42.820449 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_12hr
[0m15:19:42.820653 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_24hr
[0m15:19:42.821320 [info ] [Thread-1  ]: 15 of 38 START model model dbt_anomaly_detection.derived_models_05mon_24hr ..... [RUN]
[0m15:19:42.821882 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_24hr"
[0m15:19:42.822018 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_24hr
[0m15:19:42.822159 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_24hr
[0m15:19:42.829852 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m15:19:42.830524 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:42.830657 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_24hr
[0m15:19:42.833834 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_24hr"
[0m15:19:42.834415 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:19:42.834604 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB("2023-02-09", INTERVAL 30 DAY)
  AND DATE(time_stamps) < DATE_SUB("2023-02-09", INTERVAL 15 DAY)
  AND agg_tag = "24hr"
    );
    
[0m15:19:53.251417 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:53.253849 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129d6670>]}
[0m15:19:53.254961 [info ] [Thread-1  ]: 15 of 38 OK created model model dbt_anomaly_detection.derived_models_05mon_24hr  [[32mOK[0m in 10.43s]
[0m15:19:53.256061 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_24hr
[0m15:19:53.256576 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_4hr
[0m15:19:53.256896 [info ] [Thread-1  ]: 16 of 38 START model model dbt_anomaly_detection.derived_models_05mon_4hr ...... [RUN]
[0m15:19:53.257566 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_4hr"
[0m15:19:53.257900 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_4hr
[0m15:19:53.258675 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_4hr
[0m15:19:53.267859 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m15:19:53.268989 [debug] [Thread-1  ]: finished collecting timing info
[0m15:19:53.269187 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_4hr
[0m15:19:53.272628 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_4hr"
[0m15:19:53.273160 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:19:53.273363 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB("2023-02-09", INTERVAL 30 DAY)
  AND DATE(time_stamps) < DATE_SUB("2023-02-09", INTERVAL 10 DAY)
  AND agg_tag = "4hr"
    );
    
[0m15:20:06.228304 [debug] [Thread-1  ]: finished collecting timing info
[0m15:20:06.230523 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1128ca1f0>]}
[0m15:20:06.231675 [info ] [Thread-1  ]: 16 of 38 OK created model model dbt_anomaly_detection.derived_models_05mon_4hr . [[32mOK[0m in 12.97s]
[0m15:20:06.232290 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_4hr
[0m15:20:06.232477 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_05mon_8hr
[0m15:20:06.232691 [info ] [Thread-1  ]: 17 of 38 START model model dbt_anomaly_detection.derived_models_05mon_8hr ...... [RUN]
[0m15:20:06.233345 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_05mon_8hr"
[0m15:20:06.233502 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_05mon_8hr
[0m15:20:06.233634 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_05mon_8hr
[0m15:20:06.239520 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m15:20:06.240088 [debug] [Thread-1  ]: finished collecting timing info
[0m15:20:06.240213 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_05mon_8hr
[0m15:20:06.243826 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_05mon_8hr"
[0m15:20:06.244248 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:20:06.244407 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_05mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_05mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB("2023-02-09", INTERVAL 30 DAY)
  AND DATE(time_stamps) < DATE_SUB("2023-02-09", INTERVAL 10 DAY)
  AND agg_tag = "8hr"
    );
    
[0m15:20:18.699780 [debug] [Thread-1  ]: finished collecting timing info
[0m15:20:18.702108 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d56d30>]}
[0m15:20:18.703309 [info ] [Thread-1  ]: 17 of 38 OK created model model dbt_anomaly_detection.derived_models_05mon_8hr . [[32mOK[0m in 12.47s]
[0m15:20:18.704130 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_05mon_8hr
[0m15:20:18.704421 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_12hr
[0m15:20:18.704891 [info ] [Thread-1  ]: 18 of 38 START model model dbt_anomaly_detection.derived_models_1mon_12hr ...... [RUN]
[0m15:20:18.705589 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_12hr"
[0m15:20:18.705789 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_12hr
[0m15:20:18.705966 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_12hr
[0m15:20:18.713636 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m15:20:18.714594 [debug] [Thread-1  ]: finished collecting timing info
[0m15:20:18.714769 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_12hr
[0m15:20:18.717962 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_12hr"
[0m15:20:18.718411 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:20:18.718588 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB("2023-02-09", INTERVAL 60 DAY)
  AND DATE(time_stamps) < DATE_SUB("2023-02-09", INTERVAL 15 DAY)
  AND agg_tag = "12hr"
    );
    
[0m15:20:28.603547 [debug] [Thread-1  ]: finished collecting timing info
[0m15:20:28.606024 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d56910>]}
[0m15:20:28.606691 [info ] [Thread-1  ]: 18 of 38 OK created model model dbt_anomaly_detection.derived_models_1mon_12hr . [[32mOK[0m in 9.90s]
[0m15:20:28.607123 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_12hr
[0m15:20:28.607294 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_24hr
[0m15:20:28.607605 [info ] [Thread-1  ]: 19 of 38 START model model dbt_anomaly_detection.derived_models_1mon_24hr ...... [RUN]
[0m15:20:28.608046 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_24hr"
[0m15:20:28.608164 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_24hr
[0m15:20:28.608279 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_24hr
[0m15:20:28.612285 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m15:20:28.613019 [debug] [Thread-1  ]: finished collecting timing info
[0m15:20:28.613147 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_24hr
[0m15:20:28.615505 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_24hr"
[0m15:20:28.615893 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:20:28.616014 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB("2023-02-09", INTERVAL 60 DAY)
  AND DATE(time_stamps) < DATE_SUB("2023-02-09", INTERVAL 15 DAY)
  AND agg_tag = "24hr"
    );
    
[0m15:20:39.031882 [debug] [Thread-1  ]: finished collecting timing info
[0m15:20:39.032741 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114820160>]}
[0m15:20:39.033207 [info ] [Thread-1  ]: 19 of 38 OK created model model dbt_anomaly_detection.derived_models_1mon_24hr . [[32mOK[0m in 10.42s]
[0m15:20:39.033690 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_24hr
[0m15:20:39.033896 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_4hr
[0m15:20:39.034289 [info ] [Thread-1  ]: 20 of 38 START model model dbt_anomaly_detection.derived_models_1mon_4hr ....... [RUN]
[0m15:20:39.034811 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_4hr"
[0m15:20:39.034984 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_4hr
[0m15:20:39.035152 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_4hr
[0m15:20:39.041962 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m15:20:39.042591 [debug] [Thread-1  ]: finished collecting timing info
[0m15:20:39.042743 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_4hr
[0m15:20:39.048175 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_4hr"
[0m15:20:39.048621 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:20:39.048782 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB("2023-02-09", INTERVAL 60 DAY)
  AND DATE(time_stamps) < DATE_SUB("2023-02-09", INTERVAL 10 DAY)
  AND agg_tag = "4hr"
    );
    
[0m15:20:49.950355 [debug] [Thread-1  ]: finished collecting timing info
[0m15:20:49.954258 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114824490>]}
[0m15:20:49.955219 [info ] [Thread-1  ]: 20 of 38 OK created model model dbt_anomaly_detection.derived_models_1mon_4hr .. [[32mOK[0m in 10.92s]
[0m15:20:49.956201 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_4hr
[0m15:20:49.956618 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_1mon_8hr
[0m15:20:49.957281 [info ] [Thread-1  ]: 21 of 38 START model model dbt_anomaly_detection.derived_models_1mon_8hr ....... [RUN]
[0m15:20:49.959085 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_1mon_8hr"
[0m15:20:49.959464 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_1mon_8hr
[0m15:20:49.959743 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_1mon_8hr
[0m15:20:49.973014 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m15:20:49.974081 [debug] [Thread-1  ]: finished collecting timing info
[0m15:20:49.974310 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_1mon_8hr
[0m15:20:49.978551 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_1mon_8hr"
[0m15:20:49.979037 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:20:49.979282 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_1mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_1mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB("2023-02-09", INTERVAL 60 DAY)
  AND DATE(time_stamps) < DATE_SUB("2023-02-09", INTERVAL 10 DAY)
  AND agg_tag = "8hr"
    );
    
[0m15:21:00.814197 [debug] [Thread-1  ]: finished collecting timing info
[0m15:21:00.817609 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d4b070>]}
[0m15:21:00.818457 [info ] [Thread-1  ]: 21 of 38 OK created model model dbt_anomaly_detection.derived_models_1mon_8hr .. [[32mOK[0m in 10.86s]
[0m15:21:00.819285 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_1mon_8hr
[0m15:21:00.819631 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_12hr
[0m15:21:00.819997 [info ] [Thread-1  ]: 22 of 38 START model model dbt_anomaly_detection.derived_models_2mon_12hr ...... [RUN]
[0m15:21:00.820830 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_12hr"
[0m15:21:00.821132 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_12hr
[0m15:21:00.821440 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_12hr
[0m15:21:00.838169 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m15:21:00.838844 [debug] [Thread-1  ]: finished collecting timing info
[0m15:21:00.839007 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_12hr
[0m15:21:00.842201 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_12hr"
[0m15:21:00.842716 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:21:00.842857 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_12hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_12hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB("2023-02-09", INTERVAL 90 DAY)
  AND DATE(time_stamps) < DATE_SUB("2023-02-09", INTERVAL 15 DAY)
  AND agg_tag = "12hr"
    );
    
[0m15:21:11.745387 [debug] [Thread-1  ]: finished collecting timing info
[0m15:21:11.747359 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d5c730>]}
[0m15:21:11.748102 [info ] [Thread-1  ]: 22 of 38 OK created model model dbt_anomaly_detection.derived_models_2mon_12hr . [[32mOK[0m in 10.93s]
[0m15:21:11.748883 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_12hr
[0m15:21:11.749217 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_24hr
[0m15:21:11.749479 [info ] [Thread-1  ]: 23 of 38 START model model dbt_anomaly_detection.derived_models_2mon_24hr ...... [RUN]
[0m15:21:11.750049 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_24hr"
[0m15:21:11.750256 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_24hr
[0m15:21:11.750609 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_24hr
[0m15:21:11.765493 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m15:21:11.766605 [debug] [Thread-1  ]: finished collecting timing info
[0m15:21:11.766855 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_24hr
[0m15:21:11.774208 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_24hr"
[0m15:21:11.775163 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:21:11.775438 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_24hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_24hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB("2023-02-09", INTERVAL 90 DAY)
  AND DATE(time_stamps) < DATE_SUB("2023-02-09", INTERVAL 15 DAY)
  AND agg_tag = "24hr"
    );
    
[0m15:21:22.637847 [debug] [Thread-1  ]: finished collecting timing info
[0m15:21:22.640278 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114850e80>]}
[0m15:21:22.641431 [info ] [Thread-1  ]: 23 of 38 OK created model model dbt_anomaly_detection.derived_models_2mon_24hr . [[32mOK[0m in 10.89s]
[0m15:21:22.642464 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_24hr
[0m15:21:22.643016 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_4hr
[0m15:21:22.643414 [info ] [Thread-1  ]: 24 of 38 START model model dbt_anomaly_detection.derived_models_2mon_4hr ....... [RUN]
[0m15:21:22.644261 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_4hr"
[0m15:21:22.644612 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_4hr
[0m15:21:22.644913 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_4hr
[0m15:21:22.660165 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m15:21:22.661256 [debug] [Thread-1  ]: finished collecting timing info
[0m15:21:22.661506 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_4hr
[0m15:21:22.666299 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_4hr"
[0m15:21:22.667244 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:21:22.667556 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_4hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_4hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB("2023-02-09", INTERVAL 90 DAY)
  AND DATE(time_stamps) < DATE_SUB("2023-02-09", INTERVAL 10 DAY)
  AND agg_tag = "4hr"
    );
    
[0m15:21:35.003329 [debug] [Thread-1  ]: finished collecting timing info
[0m15:21:35.006745 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d510d0>]}
[0m15:21:35.008264 [info ] [Thread-1  ]: 24 of 38 OK created model model dbt_anomaly_detection.derived_models_2mon_4hr .. [[32mOK[0m in 12.36s]
[0m15:21:35.009487 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_4hr
[0m15:21:35.010010 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_models_2mon_8hr
[0m15:21:35.011663 [info ] [Thread-1  ]: 25 of 38 START model model dbt_anomaly_detection.derived_models_2mon_8hr ....... [RUN]
[0m15:21:35.013804 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_models_2mon_8hr"
[0m15:21:35.014549 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_models_2mon_8hr
[0m15:21:35.014889 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_models_2mon_8hr
[0m15:21:35.027528 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m15:21:35.028395 [debug] [Thread-1  ]: finished collecting timing info
[0m15:21:35.028599 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_models_2mon_8hr
[0m15:21:35.033398 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_models_2mon_8hr"
[0m15:21:35.034181 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:21:35.034439 [debug] [Thread-1  ]: On model.anomaly_detection.derived_models_2mon_8hr: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_models_2mon_8hr"} */


    create or replace model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`
    options(MODEL_TYPE="ARIMA_PLUS",TIME_SERIES_TIMESTAMP_COL="time_stamps",TIME_SERIES_DATA_COL="event_count",TIME_SERIES_ID_COL=['app_event', 'agg_tag'],HORIZON=120,HOLIDAY_REGION="CA")as (
        
SELECT
  time_stamps,
  event_count,
  app_event,
  agg_tag
FROM
  `ld-snowplow`.`dbt_anomaly_detection`.`aggregation_outliers_short`
WHERE
  DATE(time_stamps) >= DATE_SUB("2023-02-09", INTERVAL 90 DAY)
  AND DATE(time_stamps) < DATE_SUB("2023-02-09", INTERVAL 10 DAY)
  AND agg_tag = "8hr"
    );
    
[0m15:21:45.993155 [debug] [Thread-1  ]: finished collecting timing info
[0m15:21:45.994789 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114850e80>]}
[0m15:21:46.535087 [info ] [Thread-1  ]: 25 of 38 OK created model model dbt_anomaly_detection.derived_models_2mon_8hr .. [[32mOK[0m in 10.98s]
[0m15:21:46.536853 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_models_2mon_8hr
[0m15:21:46.540945 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_ml_detect
[0m15:21:46.541750 [info ] [Thread-1  ]: 26 of 38 START table model dbt_anomaly_detection.derived_ml_detect ............. [RUN]
[0m15:21:46.543099 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_ml_detect"
[0m15:21:46.543374 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_ml_detect
[0m15:21:46.543633 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_ml_detect
[0m15:21:46.572125 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_ml_detect"
[0m15:21:46.573239 [debug] [Thread-1  ]: finished collecting timing info
[0m15:21:46.573604 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_ml_detect
[0m15:21:46.577403 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:21:47.012264 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_ml_detect"
[0m15:21:47.015336 [debug] [Thread-1  ]: On model.anomaly_detection.derived_ml_detect: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_ml_detect"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_ml_detect`
  
  
  OPTIONS()
  as (
    -- 2 periods of training * 2 probabality threshold * 4 aggregation levels 


  WITH test_set AS (
  SELECT
  time_stamps,
        event_count,
        app_event,
        agg_tag
      FROM
        `ld-snowplow`.`dbt_anomaly_detection`.`all_agg_derived_cutoff`  
      WHERE (agg_tag = "4hr" AND DATE(time_stamps) >= DATE_SUB("2023-02-09", INTERVAL 10 DAY)) 
      OR (agg_tag = "8hr" AND DATE(time_stamps) >= DATE_SUB("2023-02-09", INTERVAL 10 DAY))
      OR (agg_tag = "12hr" AND DATE(time_stamps) >= DATE_SUB("2023-02-09", INTERVAL 15 DAY))
      OR (agg_tag = "24hr" AND DATE(time_stamps) >= DATE_SUB("2023-02-09", INTERVAL 15 DAY))
  )

  
  

    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_1mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_2mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.9999" AS prob_threshold, "derived_models_05mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`,
        struct(0.9999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
       

  
  UNION ALL
  



    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
 

    UNION ALL

    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_4hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_4hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "4hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_8hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_8hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "8hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_12hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_12hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "12hr" ))
    )
       

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_1mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_1mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_2mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_2mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
 

    UNION ALL
    
    SELECT
      app_event, agg_tag, time_stamps, "0.999999" AS prob_threshold, "derived_models_05mon_24hr" AS training_period, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM
      
    ml.detect_anomalies(
        model `ld-snowplow`.`dbt_anomaly_detection`.`derived_models_05mon_24hr`,
        struct(0.999999 as anomaly_prob_threshold),
        (select * from (select * from test_set where agg_tag = "24hr" ))
    )
       

  
  ORDER BY
  agg_tag,
  time_stamps,
  app_event,
  prob_threshold,
  training_period
  


  );
  
[0m15:21:50.437835 [debug] [Thread-1  ]: finished collecting timing info
[0m15:21:50.439386 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a9e520>]}
[0m15:21:50.440104 [info ] [Thread-1  ]: 26 of 38 OK created table model dbt_anomaly_detection.derived_ml_detect ........ [[32mCREATE TABLE (1.6k rows, 409.6 KB processed)[0m in 3.90s]
[0m15:21:50.440821 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_ml_detect
[0m15:21:50.441866 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ml_detect_tweaked
[0m15:21:50.442450 [info ] [Thread-1  ]: 27 of 38 START table model dbt_anomaly_detection.ml_detect_tweaked ............. [RUN]
[0m15:21:50.443661 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ml_detect_tweaked"
[0m15:21:50.444085 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ml_detect_tweaked
[0m15:21:50.444419 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ml_detect_tweaked
[0m15:21:50.451126 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ml_detect_tweaked"
[0m15:21:50.452304 [debug] [Thread-1  ]: finished collecting timing info
[0m15:21:50.452553 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ml_detect_tweaked
[0m15:21:50.454107 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:21:50.917541 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.ml_detect_tweaked"
[0m15:21:50.919471 [debug] [Thread-1  ]: On model.anomaly_detection.ml_detect_tweaked: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.ml_detect_tweaked"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked`
  
  
  OPTIONS()
  as (
     
  
  with neg_bound_reset as (
SELECT app_event, agg_tag, time_stamps, prob_threshold, training_period, event_count, 
  IF(lower_bound<0, 2, 0.77*lower_bound) AS lower_bound, 1.3*upper_bound AS upper_bound, anomaly_probability, is_anomaly
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_ml_detect` )

SELECT app_event, agg_tag, time_stamps, prob_threshold, training_period, event_count, 
  lower_bound, upper_bound, anomaly_probability,
  (upper_bound < event_count OR event_count < lower_bound) AS is_anomaly
FROM neg_bound_reset
  );
  
[0m15:21:52.950356 [debug] [Thread-1  ]: finished collecting timing info
[0m15:21:52.952122 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129fddc0>]}
[0m15:21:52.952917 [info ] [Thread-1  ]: 27 of 38 OK created table model dbt_anomaly_detection.ml_detect_tweaked ........ [[32mCREATE TABLE (1.6k rows, 150.3 KB processed)[0m in 2.51s]
[0m15:21:52.954066 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ml_detect_tweaked
[0m15:21:52.957073 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_model_features_dbt
[0m15:21:52.957802 [info ] [Thread-1  ]: 28 of 38 START table model dbt_anomaly_detection.derived_model_features_dbt .... [RUN]
[0m15:21:52.958521 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_model_features_dbt"
[0m15:21:52.958725 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_model_features_dbt
[0m15:21:52.958945 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_model_features_dbt
[0m15:21:52.972099 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_model_features_dbt"
[0m15:21:52.973038 [debug] [Thread-1  ]: finished collecting timing info
[0m15:21:52.973227 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_model_features_dbt
[0m15:21:52.975497 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:21:53.402151 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_model_features_dbt"
[0m15:21:53.406405 [debug] [Thread-1  ]: On model.anomaly_detection.derived_model_features_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_model_features_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_model_features_dbt`
  
  
  OPTIONS()
  as (
    
  
  SELECT
    app_event,
    CONCAT(agg_tag, '_', prob_threshold, "threshold", '_', RTRIM(LTRIM(training_period, "derived_models_"), CONCAT('_', agg_tag))) AS control_config,
    SUM(CASE WHEN is_anomaly = TRUE THEN 1 ELSE 0 END) AS anomalies,
    SQRT(AVG( POWER(upper_bound - lower_bound, 2) ) ) / AVG(lower_bound) AS RMSD_prcnt,
    SUM(CASE WHEN lower_bound < 0 THEN 1 ELSE 0 END) AS neg_lower
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked` AS all_configs
  GROUP BY
    app_event,
    control_config
  ORDER BY
    control_config,
    app_event
  );
  
[0m15:21:55.684518 [debug] [Thread-1  ]: finished collecting timing info
[0m15:21:55.686479 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1148cd310>]}
[0m15:21:55.687392 [info ] [Thread-1  ]: 28 of 38 OK created table model dbt_anomaly_detection.derived_model_features_dbt  [[32mCREATE TABLE (48.0 rows, 113.9 KB processed)[0m in 2.73s]
[0m15:21:55.688279 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_model_features_dbt
[0m15:21:55.690717 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_model_features_dbt
[0m15:21:55.691447 [info ] [Thread-1  ]: 29 of 38 START table model dbt_anomaly_detection.filtered_model_features_dbt ... [RUN]
[0m15:21:55.692393 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_model_features_dbt"
[0m15:21:55.692711 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_model_features_dbt
[0m15:21:55.692995 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_model_features_dbt
[0m15:21:55.707756 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_model_features_dbt"
[0m15:21:55.708743 [debug] [Thread-1  ]: finished collecting timing info
[0m15:21:55.708989 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_model_features_dbt
[0m15:21:55.712498 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:21:56.152070 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.filtered_model_features_dbt"
[0m15:21:56.154579 [debug] [Thread-1  ]: On model.anomaly_detection.filtered_model_features_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.filtered_model_features_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
  
  
  OPTIONS()
  as (
    
SELECT features.app_event, control_config, anomalies, RMSD_prcnt, neg_lower
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_nonrecent_events` AS non_recent
INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`derived_model_features_dbt` AS features
  ON non_recent.app_event = features.app_event
  );
  
[0m15:21:58.328688 [debug] [Thread-1  ]: finished collecting timing info
[0m15:21:58.330795 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1148cd6d0>]}
[0m15:21:58.331932 [info ] [Thread-1  ]: 29 of 38 OK created table model dbt_anomaly_detection.filtered_model_features_dbt  [[32mCREATE TABLE (48.0 rows, 3.2 KB processed)[0m in 2.64s]
[0m15:21:58.332875 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_model_features_dbt
[0m15:21:58.335333 [debug] [Thread-1  ]: Began running node model.anomaly_detection.ref_distinct_tuples
[0m15:21:58.336180 [info ] [Thread-1  ]: 30 of 38 START table model dbt_anomaly_detection.ref_distinct_tuples ........... [RUN]
[0m15:21:58.337041 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.ref_distinct_tuples"
[0m15:21:58.337384 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.ref_distinct_tuples
[0m15:21:58.337640 [debug] [Thread-1  ]: Compiling model.anomaly_detection.ref_distinct_tuples
[0m15:21:58.346974 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.ref_distinct_tuples"
[0m15:21:58.348018 [debug] [Thread-1  ]: finished collecting timing info
[0m15:21:58.348263 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.ref_distinct_tuples
[0m15:21:58.352413 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:21:58.745527 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.ref_distinct_tuples"
[0m15:21:58.748070 [debug] [Thread-1  ]: On model.anomaly_detection.ref_distinct_tuples: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.ref_distinct_tuples"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`ref_distinct_tuples`
  
  
  OPTIONS()
  as (
    
SELECT DISTINCT app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
WHERE app_event LIKE '%add%'
UNION ALL
SELECT DISTINCT app_event
FROM `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
WHERE app_event NOT LIKE '%ad%'
ORDER BY app_event
  );
  
[0m15:22:01.217626 [debug] [Thread-1  ]: finished collecting timing info
[0m15:22:01.219754 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a1aeb0>]}
[0m15:22:01.220776 [info ] [Thread-1  ]: 30 of 38 OK created table model dbt_anomaly_detection.ref_distinct_tuples ...... [[32mCREATE TABLE (2.0 rows, 720.0 Bytes processed)[0m in 2.88s]
[0m15:22:01.221677 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.ref_distinct_tuples
[0m15:22:01.222213 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_features_null_filtered
[0m15:22:01.222605 [info ] [Thread-1  ]: 31 of 38 START table model dbt_anomaly_detection.remaining_events_features_null_filtered  [RUN]
[0m15:22:01.223627 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_features_null_filtered"
[0m15:22:01.224155 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_features_null_filtered
[0m15:22:01.224494 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_features_null_filtered
[0m15:22:01.235655 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_features_null_filtered"
[0m15:22:01.238569 [debug] [Thread-1  ]: finished collecting timing info
[0m15:22:01.238999 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_features_null_filtered
[0m15:22:01.246193 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:22:01.677057 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_features_null_filtered"
[0m15:22:01.680196 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_features_null_filtered: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_features_null_filtered"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered`
  
  
  OPTIONS()
  as (
    

select *
from `ld-snowplow`.`dbt_anomaly_detection`.`filtered_model_features_dbt`
where RMSD_prcnt is not null
  );
  
[0m15:22:03.791252 [debug] [Thread-1  ]: finished collecting timing info
[0m15:22:03.794004 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1148ae610>]}
[0m15:22:03.795590 [info ] [Thread-1  ]: 31 of 38 OK created table model dbt_anomaly_detection.remaining_events_features_null_filtered  [[32mCREATE TABLE (48.0 rows, 3.1 KB processed)[0m in 2.57s]
[0m15:22:03.796481 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_features_null_filtered
[0m15:22:03.798223 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies
[0m15:22:03.799256 [info ] [Thread-1  ]: 32 of 38 START table model dbt_anomaly_detection.remaining_events_min_anomalies  [RUN]
[0m15:22:03.800140 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies"
[0m15:22:03.800624 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies
[0m15:22:03.800897 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies
[0m15:22:03.807724 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies"
[0m15:22:03.810338 [debug] [Thread-1  ]: finished collecting timing info
[0m15:22:03.810594 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies
[0m15:22:03.814765 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:22:04.255869 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_anomalies"
[0m15:22:04.258560 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies`
  
  
  OPTIONS()
  as (
    

SELECT app_event, MIN(anomalies) AS anomalies 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered`
  GROUP BY app_event
  ORDER BY app_event
  );
  
[0m15:22:06.499501 [debug] [Thread-1  ]: finished collecting timing info
[0m15:22:06.501624 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11487ca60>]}
[0m15:22:06.503186 [info ] [Thread-1  ]: 32 of 38 OK created table model dbt_anomaly_detection.remaining_events_min_anomalies  [[32mCREATE TABLE (2.0 rows, 1.1 KB processed)[0m in 2.70s]
[0m15:22:06.505270 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies
[0m15:22:06.508370 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m15:22:06.509249 [info ] [Thread-1  ]: 33 of 38 START table model dbt_anomaly_detection.remaining_events_min_anomalies_results  [RUN]
[0m15:22:06.510203 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m15:22:06.510682 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_anomalies_results
[0m15:22:06.510932 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_anomalies_results
[0m15:22:06.518786 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m15:22:06.519681 [debug] [Thread-1  ]: finished collecting timing info
[0m15:22:06.519887 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_anomalies_results
[0m15:22:06.523187 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:22:06.905829 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_anomalies_results"
[0m15:22:06.908491 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_anomalies_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_anomalies_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results`
  
  
  OPTIONS()
  as (
    

SELECT neg_lower_criteria_res.app_event, neg_lower_criteria_res.control_config, 
    neg_lower_criteria_res.anomalies, neg_lower_criteria_res.RMSD_prcnt, neg_lower_criteria_res.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_features_null_filtered` AS neg_lower_criteria_res
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies` AS min_neg_lower_min_anomalies
    ON neg_lower_criteria_res.anomalies = min_neg_lower_min_anomalies.anomalies
      AND neg_lower_criteria_res.app_event = min_neg_lower_min_anomalies.app_event
  ORDER BY neg_lower_criteria_res.app_event, RMSD_prcnt DESC
  );
  
[0m15:22:09.119043 [debug] [Thread-1  ]: finished collecting timing info
[0m15:22:09.120964 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d4db20>]}
[0m15:22:09.122177 [info ] [Thread-1  ]: 33 of 38 OK created table model dbt_anomaly_detection.remaining_events_min_anomalies_results  [[32mCREATE TABLE (26.0 rows, 3.2 KB processed)[0m in 2.61s]
[0m15:22:09.123487 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_anomalies_results
[0m15:22:09.125026 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD
[0m15:22:09.125637 [info ] [Thread-1  ]: 34 of 38 START table model dbt_anomaly_detection.remaining_events_min_RMSD ..... [RUN]
[0m15:22:09.126687 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD"
[0m15:22:09.127029 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD
[0m15:22:09.127530 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD
[0m15:22:09.141455 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD"
[0m15:22:09.143687 [debug] [Thread-1  ]: finished collecting timing info
[0m15:22:09.143964 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD
[0m15:22:09.150028 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:22:09.567667 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_RMSD"
[0m15:22:09.570222 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_RMSD: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_RMSD"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD`
  
  
  OPTIONS()
  as (
    

SELECT app_event, MIN(RMSD_prcnt) AS RMSD_prcnt 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results`
  GROUP BY app_event
  ORDER BY app_event
  );
  
[0m15:22:11.674870 [debug] [Thread-1  ]: finished collecting timing info
[0m15:22:11.676870 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1148cff40>]}
[0m15:22:11.677900 [info ] [Thread-1  ]: 34 of 38 OK created table model dbt_anomaly_detection.remaining_events_min_RMSD  [[32mCREATE TABLE (2.0 rows, 598.0 Bytes processed)[0m in 2.55s]
[0m15:22:11.678685 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD
[0m15:22:11.680646 [debug] [Thread-1  ]: Began running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m15:22:11.681975 [info ] [Thread-1  ]: 35 of 38 START table model dbt_anomaly_detection.remaining_events_min_RMSD_results  [RUN]
[0m15:22:11.682888 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m15:22:11.684857 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.remaining_events_min_RMSD_results
[0m15:22:11.685694 [debug] [Thread-1  ]: Compiling model.anomaly_detection.remaining_events_min_RMSD_results
[0m15:22:11.691089 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m15:22:11.691986 [debug] [Thread-1  ]: finished collecting timing info
[0m15:22:11.692195 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.remaining_events_min_RMSD_results
[0m15:22:11.695635 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:22:12.118713 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.remaining_events_min_RMSD_results"
[0m15:22:12.123048 [debug] [Thread-1  ]: On model.anomaly_detection.remaining_events_min_RMSD_results: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.remaining_events_min_RMSD_results"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
  
  
  OPTIONS()
  as (
    

SELECT anomalies_criteria_res_dbt.app_event, anomalies_criteria_res_dbt.control_config, 
    anomalies_criteria_res_dbt.anomalies, anomalies_criteria_res_dbt.RMSD_prcnt, anomalies_criteria_res_dbt.neg_lower 
  FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_anomalies_results` AS anomalies_criteria_res_dbt
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD` AS min_anomalies_max_RMSD
    ON anomalies_criteria_res_dbt.RMSD_prcnt = min_anomalies_max_RMSD.RMSD_prcnt
      AND anomalies_criteria_res_dbt.app_event = min_anomalies_max_RMSD.app_event
  ORDER BY anomalies_criteria_res_dbt.app_event, control_config
  );
  
[0m15:22:14.572467 [debug] [Thread-1  ]: finished collecting timing info
[0m15:22:14.574987 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a93dc0>]}
[0m15:22:14.576165 [info ] [Thread-1  ]: 35 of 38 OK created table model dbt_anomaly_detection.remaining_events_min_RMSD_results  [[32mCREATE TABLE (2.0 rows, 1.7 KB processed)[0m in 2.89s]
[0m15:22:14.577430 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.remaining_events_min_RMSD_results
[0m15:22:14.579277 [debug] [Thread-1  ]: Began running node model.anomaly_detection.filtered_all_models
[0m15:22:14.579999 [info ] [Thread-1  ]: 36 of 38 START table model dbt_anomaly_detection.filtered_all_models ........... [RUN]
[0m15:22:14.581478 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.filtered_all_models"
[0m15:22:14.582523 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.filtered_all_models
[0m15:22:14.582909 [debug] [Thread-1  ]: Compiling model.anomaly_detection.filtered_all_models
[0m15:22:14.597784 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.filtered_all_models"
[0m15:22:14.598823 [debug] [Thread-1  ]: finished collecting timing info
[0m15:22:14.599076 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.filtered_all_models
[0m15:22:14.602939 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:22:15.053525 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.filtered_all_models"
[0m15:22:15.056592 [debug] [Thread-1  ]: On model.anomaly_detection.filtered_all_models: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.filtered_all_models"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`filtered_all_models`
  
  
  OPTIONS()
  as (
    
SELECT app_event, control_config, anomalies, RMSD_prcnt, neg_lower
FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
WHERE app_event LIKE '%add%'
UNION ALL 
SELECT app_event, control_config, anomalies, RMSD_prcnt, neg_lower
FROM `ld-snowplow`.`dbt_anomaly_detection`.`remaining_events_min_RMSD_results`
WHERE app_event NOT LIKE '%ad%'
ORDER BY app_event
  );
  
[0m15:22:17.184845 [debug] [Thread-1  ]: finished collecting timing info
[0m15:22:17.188483 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d7a910>]}
[0m15:22:17.189513 [info ] [Thread-1  ]: 36 of 38 OK created table model dbt_anomaly_detection.filtered_all_models ...... [[32mCREATE TABLE (2.0 rows, 134.0 Bytes processed)[0m in 2.61s]
[0m15:22:17.190346 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.filtered_all_models
[0m15:22:17.192220 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m15:22:17.194150 [info ] [Thread-1  ]: 37 of 38 START table model dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [RUN]
[0m15:22:17.195283 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m15:22:17.195627 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m15:22:17.195876 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m15:22:17.208648 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m15:22:17.209110 [debug] [Thread-1  ]: finished collecting timing info
[0m15:22:17.209210 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m15:22:17.210798 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:22:17.689587 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_alerts_fired_automation_dbt"
[0m15:22:17.690369 [debug] [Thread-1  ]: On model.anomaly_detection.derived_alerts_fired_automation_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_alerts_fired_automation_dbt"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_alerts_fired_automation_dbt`
  
  
  OPTIONS()
  as (
    
  
  WITH ml_detect_updated AS (
    SELECT app_event, 
      CONCAT(agg_tag, '_', prob_threshold, "threshold", '_', RTRIM(LTRIM(training_period, "derived_models_"), CONCAT('_', agg_tag))) AS control_config,
      time_stamps, event_count, is_anomaly, lower_bound, upper_bound, anomaly_probability
    FROM `ld-snowplow`.`dbt_anomaly_detection`.`ml_detect_tweaked`
  )
  SELECT time_stamps, all_configs.app_event AS app_event, control_table.control_config AS control_config, 
    anomalies, RMSD_prcnt, neg_lower, event_count, lower_bound, upper_bound, anomaly_probability, is_anomaly
  FROM ml_detect_updated AS all_configs 
  INNER JOIN `ld-snowplow`.`dbt_anomaly_detection`.`filtered_all_models` AS control_table 
    ON all_configs.app_event = control_table.app_event
      AND all_configs.control_config = control_table.control_config
  ORDER BY all_configs.app_event, time_stamps
  );
  
[0m15:22:19.987288 [debug] [Thread-1  ]: finished collecting timing info
[0m15:22:19.989402 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112da52e0>]}
[0m15:22:19.990358 [info ] [Thread-1  ]: 37 of 38 OK created table model dbt_anomaly_detection.derived_alerts_fired_automation_dbt  [[32mCREATE TABLE (45.0 rows, 152.0 KB processed)[0m in 2.79s]
[0m15:22:19.992030 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_alerts_fired_automation_dbt
[0m15:22:19.993723 [debug] [Thread-1  ]: Began running node model.anomaly_detection.derived_events_with_anomalies
[0m15:22:19.994327 [info ] [Thread-1  ]: 38 of 38 START table model dbt_anomaly_detection.derived_events_with_anomalies . [RUN]
[0m15:22:19.995132 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.anomaly_detection.derived_events_with_anomalies"
[0m15:22:19.995442 [debug] [Thread-1  ]: Began compiling node model.anomaly_detection.derived_events_with_anomalies
[0m15:22:19.995690 [debug] [Thread-1  ]: Compiling model.anomaly_detection.derived_events_with_anomalies
[0m15:22:20.003609 [debug] [Thread-1  ]: Writing injected SQL for node "model.anomaly_detection.derived_events_with_anomalies"
[0m15:22:20.005353 [debug] [Thread-1  ]: finished collecting timing info
[0m15:22:20.005603 [debug] [Thread-1  ]: Began executing node model.anomaly_detection.derived_events_with_anomalies
[0m15:22:20.009604 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:22:20.399087 [debug] [Thread-1  ]: Writing runtime SQL for node "model.anomaly_detection.derived_events_with_anomalies"
[0m15:22:20.401428 [debug] [Thread-1  ]: On model.anomaly_detection.derived_events_with_anomalies: /* {"app": "dbt", "dbt_version": "1.2.1", "profile_name": "anomaly_detection", "target_name": "dev", "node_id": "model.anomaly_detection.derived_events_with_anomalies"} */


  create or replace table `ld-snowplow`.`dbt_anomaly_detection`.`derived_events_with_anomalies`
  
  
  OPTIONS()
  as (
    
SELECT time_stamps, app_event, event_count AS event_count_anomalous_yesterday
FROM `ld-snowplow`.`dbt_anomaly_detection`.`derived_alerts_fired_automation_dbt`
WHERE DATE(time_stamps) = "2023-02-09" - 1 
  AND is_anomaly
  );
  
[0m15:22:22.605916 [debug] [Thread-1  ]: finished collecting timing info
[0m15:22:22.606477 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff82a924-3569-4e7c-98fa-97df52185d08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d58ca0>]}
[0m15:22:22.606777 [info ] [Thread-1  ]: 38 of 38 OK created table model dbt_anomaly_detection.derived_events_with_anomalies  [[32mCREATE TABLE (0.0 rows, 1.4 KB processed)[0m in 2.61s]
[0m15:22:22.607060 [debug] [Thread-1  ]: Finished running node model.anomaly_detection.derived_events_with_anomalies
[0m15:22:22.608074 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m15:22:22.608512 [info ] [MainThread]: 
[0m15:22:22.608720 [info ] [MainThread]: Finished running 26 table models, 12 model models in 0 hours 3 minutes and 30.64 seconds (210.64s).
[0m15:22:22.608890 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:22:22.608976 [debug] [MainThread]: Connection 'model.anomaly_detection.derived_events_with_anomalies' was properly closed.
[0m15:22:22.621782 [info ] [MainThread]: 
[0m15:22:22.622003 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:22:22.622175 [info ] [MainThread]: 
[0m15:22:22.622289 [info ] [MainThread]: Done. PASS=38 WARN=0 ERROR=0 SKIP=0 TOTAL=38
[0m15:22:22.622481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114850e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129a6d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1148a0dc0>]}
[0m15:22:22.622623 [debug] [MainThread]: Flushing usage events
